{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>instance_name</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EI</td>\n",
       "      <td>ATRINS-DONOR-521</td>\n",
       "      <td>CCAGCTGCATCACAGGAGGCCAGCGAGCAGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EI</td>\n",
       "      <td>ATRINS-DONOR-905</td>\n",
       "      <td>AGACCCGCCGGGAGGCGGAGGACCTGCAGGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-30</td>\n",
       "      <td>GAGGTGAAGGACGTCCTTCCCCAGGAGCCGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-867</td>\n",
       "      <td>GGGCTGCGTTGCTGGTCACATTCCTGGCAGGT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-2817</td>\n",
       "      <td>GCTCAGCCCCCAGGTCACCCAGGAACTGACGTG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target           instance_name  \\\n",
       "0     EI        ATRINS-DONOR-521   \n",
       "1     EI        ATRINS-DONOR-905   \n",
       "2     EI        BABAPOE-DONOR-30   \n",
       "3     EI       BABAPOE-DONOR-867   \n",
       "4     EI      BABAPOE-DONOR-2817   \n",
       "\n",
       "                                            sequence  \n",
       "0                 CCAGCTGCATCACAGGAGGCCAGCGAGCAGG...  \n",
       "1                 AGACCCGCCGGGAGGCGGAGGACCTGCAGGG...  \n",
       "2                 GAGGTGAAGGACGTCCTTCCCCAGGAGCCGG...  \n",
       "3                GGGCTGCGTTGCTGGTCACATTCCTGGCAGGT...  \n",
       "4               GCTCAGCCCCCAGGTCACCCAGGAACTGACGTG...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./splice.data') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    df_li = []\n",
    "    for row in csv_reader:\n",
    "        df_li.append(row)\n",
    "    df = pd.DataFrame(df_li, columns=[\"target\", \"instance_name\", \"sequence\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target           object\n",
       "instance_name    object\n",
       "sequence         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(lambda a : a.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3005"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"sequence\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCAGCCCCCGGACTGGG'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.sequence.value_counts().keys()[0]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>instance_name</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>IE</td>\n",
       "      <td>HUMGH-ACCEPTOR-1572</td>\n",
       "      <td>GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>IE</td>\n",
       "      <td>HUMGHCSA-ACCEPTOR-6465</td>\n",
       "      <td>GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>IE</td>\n",
       "      <td>HUMGHCSA-ACCEPTOR-43393</td>\n",
       "      <td>GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>IE</td>\n",
       "      <td>HUMGHN-ACCEPTOR-1797</td>\n",
       "      <td>GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>IE</td>\n",
       "      <td>HUMGHVA-ACCEPTOR-741</td>\n",
       "      <td>GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target            instance_name  \\\n",
       "1096     IE      HUMGH-ACCEPTOR-1572   \n",
       "1100     IE   HUMGHCSA-ACCEPTOR-6465   \n",
       "1112     IE  HUMGHCSA-ACCEPTOR-43393   \n",
       "1120     IE     HUMGHN-ACCEPTOR-1797   \n",
       "1121     IE     HUMGHVA-ACCEPTOR-741   \n",
       "\n",
       "                                               sequence  \n",
       "1096  GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...  \n",
       "1100  GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...  \n",
       "1112  GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...  \n",
       "1120  GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...  \n",
       "1121  GGCCTCTCCTTCTCTTCCTTCACTTTGCAGAGGCTGGAAGATGGCA...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sequence == tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"seq_len\"] = df.sequence.apply(lambda seq: len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.seq_len.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"seq_unique_letters\"] = df.sequence.apply(lambda seq: \"\".join(sorted(list(set(seq)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>instance_name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>seq_unique_letters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EI</td>\n",
       "      <td>ATRINS-DONOR-521</td>\n",
       "      <td>CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCC...</td>\n",
       "      <td>60</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EI</td>\n",
       "      <td>ATRINS-DONOR-905</td>\n",
       "      <td>AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCC...</td>\n",
       "      <td>60</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-30</td>\n",
       "      <td>GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCG...</td>\n",
       "      <td>60</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-867</td>\n",
       "      <td>GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTT...</td>\n",
       "      <td>60</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EI</td>\n",
       "      <td>BABAPOE-DONOR-2817</td>\n",
       "      <td>GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCC...</td>\n",
       "      <td>60</td>\n",
       "      <td>ACGT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target       instance_name  \\\n",
       "0     EI    ATRINS-DONOR-521   \n",
       "1     EI    ATRINS-DONOR-905   \n",
       "2     EI    BABAPOE-DONOR-30   \n",
       "3     EI   BABAPOE-DONOR-867   \n",
       "4     EI  BABAPOE-DONOR-2817   \n",
       "\n",
       "                                            sequence  seq_len  \\\n",
       "0  CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCC...       60   \n",
       "1  AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCC...       60   \n",
       "2  GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCG...       60   \n",
       "3  GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTT...       60   \n",
       "4  GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCC...       60   \n",
       "\n",
       "  seq_unique_letters  \n",
       "0               ACGT  \n",
       "1               ACGT  \n",
       "2               ACGT  \n",
       "3               ACGT  \n",
       "4               ACGT  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocess:\n",
    "    def __init__(self, df, is_test_input=False, is_vectorize=False, vectorizer=None, analyzer=\"char\"):\n",
    "        self.is_vectorize = is_vectorize\n",
    "        self.is_test_input = is_test_input\n",
    "        self.is_tfidf_vect = False\n",
    "        self.label_enc = None\n",
    "        if is_vectorize:\n",
    "            self.X_data = df.sequence\n",
    "            self.y_data = df.target\n",
    "            self.vect = vectorizer(analyzer=analyzer).fit(self.X_data)\n",
    "            if \"CountVectorizer\" not in str(type(vectorizer())):\n",
    "                self.is_tfidf_vect = True\n",
    "        else:\n",
    "            self.X_data, self.y_data = self.__make_new_df(df)\n",
    "        \n",
    "    def __make_new_df(self, df):\n",
    "        y_data = df.target\n",
    "        def convert_to_int(x):\n",
    "            return self.letter_bags.index(x)\n",
    "\n",
    "        tmp = df.sequence.apply(lambda seq: list(seq))\n",
    "        new_df = np.zeros((len(tmp), df.seq_len.max()))\n",
    "\n",
    "        str_tmp = \"\"\n",
    "        for letter in df.seq_unique_letters.unique():\n",
    "            str_tmp += letter\n",
    "        self.letter_bags = list(set(list(str_tmp)))\n",
    "\n",
    "        for idx, _ in enumerate(new_df):\n",
    "            converted = map(convert_to_int, list(tmp.values[idx]))\n",
    "            new_df[idx] = list(converted)\n",
    "        \n",
    "        return new_df, y_data\n",
    "    \n",
    "    def __vectorize(self, X_data):\n",
    "        if self.is_tfidf_vect:\n",
    "            tmp_arr = self.vect.transform(X_data).toarray()\n",
    "            tmp_arr = tmp_arr*100\n",
    "            return tmp_arr.astype(int)\n",
    "        return self.vect.transform(X_data).toarray()\n",
    "    \n",
    "    def get_vectorized_data(self):\n",
    "        return self.__vectorize(self.X_data)\n",
    "    \n",
    "    def get_vectorized_data_to_eval(self, x, is_tfidf_vect):\n",
    "        if is_tfidf_vect:\n",
    "            tmp_arr = self.vect.transform(x).toarray()\n",
    "            tmp_arr = tmp_arr*100\n",
    "            return tmp_arr.astype(int)\n",
    "        return self.vect.transform(x).toarray()\n",
    "\n",
    "    def get_inversed_target_data(self, y):\n",
    "        return self.label_bin.inverse_transform(y)\n",
    "    \n",
    "    def get_preprocessed_data(self, X_data=None, y_data=None, test_size=0.4, random_state=0, is_for_nn=False):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X_data, self.y_data, test_size=test_size, random_state=random_state)\n",
    "        if self.is_vectorize:\n",
    "            self.X_train = self.__vectorize(self.X_train)\n",
    "            self.X_test = self.__vectorize(self.X_test)\n",
    "            \n",
    "        if is_for_nn:\n",
    "            self.label_bin = LabelBinarizer().fit(self.y_data)\n",
    "            self.y_train = self.label_bin.transform(self.y_train)\n",
    "            self.y_test = self.label_bin.transform(self.y_test)\n",
    "            \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def get_original_data(self):\n",
    "        return self.X_data, self.y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicModelTrain:\n",
    "    def __init__(self, preprocessed_data, is_run_logistic = True, is_run_multi_NB = True, \n",
    "                 is_run_random = True, is_run_extra_random = True, is_run_svc = True, is_run_xgboost = True):\n",
    "        self.is_run_logistic = is_run_logistic\n",
    "        self.is_run_multi_NB = is_run_multi_NB\n",
    "        self.is_run_random = is_run_random\n",
    "        self.is_run_extra_random = is_run_extra_random\n",
    "        self.is_run_svc = is_run_svc \n",
    "        self.is_run_xgboost = is_run_xgboost\n",
    "        self.preprocessed_data = preprocessed_data\n",
    "        \n",
    "    def randomly_selected_classification(self, model):\n",
    "        x = self.X_data\n",
    "        y = self.y_data\n",
    "        idx = np.random.choice(x.shape[0], 1000, replace=False)\n",
    "        x_test = x[idx]\n",
    "        y_true = y[idx]\n",
    "        if self.preprocessed_data.is_vectorize:\n",
    "            x_test = self.preprocessed_data.get_vectorized_data_to_eval(x_test, self.preprocessed_data.is_tfidf_vect)\n",
    "        y_pred = model.predict(x_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        def display_(str_):\n",
    "            display(Markdown(str(str_)))\n",
    "        \n",
    "        display_(\"##### Ten-fold cross-validation accuracy score : \")\n",
    "        ac_score = np.zeros(10)\n",
    "        for i in list(range(10)):\n",
    "            idx = np.random.choice(x.shape[0], 1000, replace=False)\n",
    "            x_test = x[idx]\n",
    "            y_true = y[idx]\n",
    "            if self.preprocessed_data.is_vectorize:\n",
    "                x_test = self.preprocessed_data.get_vectorized_data_to_eval(x_test, self.preprocessed_data.is_tfidf_vect)\n",
    "            y_pred = model.predict(x_test)\n",
    "            ac_score[i] = accuracy_score(y_true, y_pred)\n",
    "        display_(\"##### : \" + str(ac_score.mean()*100))\n",
    "    \n",
    "    def print_classification_report(self, model, x, y_true):\n",
    "        y_pred = model.predict(x)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "                \n",
    "    def run_all(self, num_boost_round = 10000, lr = 0.01, max_delta_step = 4):\n",
    "        self.X_data, self.y_data = self.preprocessed_data.get_original_data()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.preprocessed_data.get_preprocessed_data()\n",
    "        \n",
    "        def display_(str_):\n",
    "            display(Markdown(str(str_)))\n",
    "        \n",
    "        display_(\"##### shape of X train and X_test : \")\n",
    "        print(self.X_train.shape, self.X_test.shape)\n",
    "        \n",
    "        if self.is_run_logistic:\n",
    "            display_(\"#### Run logistic\")\n",
    "            self.logis_model = LogisticRegression().fit(self.X_train, self.y_train)\n",
    "            display_(\"##### Logistic Regression classification report : \")\n",
    "            self.print_classification_report(self.logis_model, self.X_test, self.y_test)\n",
    "            display_(\"##### Logistic Regression <span style='color: red'>randomly selected 1000 data</span> classification report : \")\n",
    "            self.randomly_selected_classification(self.logis_model)\n",
    "            \n",
    "        if self.is_run_multi_NB:\n",
    "            display_(\"#### Run multinomialNB\")\n",
    "            self.multi_model = MultinomialNB().fit(self.X_train, self.y_train)\n",
    "            display_(\"##### MultinomialNB classification report : \")\n",
    "            self.print_classification_report(self.multi_model, self.X_test, self.y_test)\n",
    "            display_(\"##### MultinomialNB <span style='color: red'>randomly selected 1000 data</span> classification report : \")\n",
    "            self.randomly_selected_classification(self.multi_model)\n",
    "        \n",
    "        if self.is_run_random:\n",
    "            display_(\"#### Run random forest\")\n",
    "            parameters = {'n_estimators': np.arange(100, 200, 10),'max_depth': np.arange(1, 10, 2)}\n",
    "            kfold = KFold(10)\n",
    "            random_model = RandomForestClassifier(random_state=0)\n",
    "            grid_model = GridSearchCV(random_model, parameters, scoring='accuracy', cv = kfold, n_jobs = -1)\n",
    "            grid_model.fit(self.X_train, self.y_train)\n",
    "            params_ls = grid_model.cv_results_['params']\n",
    "            mean_test_score_ls = grid_model.cv_results_[\"mean_test_score\"]\n",
    "            plt.plot(mean_test_score_ls)\n",
    "            plt.title(\"Random forest test score\")\n",
    "            plt.show()\n",
    "            display_(grid_model.best_score_)\n",
    "            display_(grid_model.best_params_)\n",
    "        \n",
    "        if self.is_run_extra_random:\n",
    "            display_(\"#### Run extra random forest\")\n",
    "            parameters = {'n_estimators': np.arange(100, 200, 10),'max_depth': np.arange(1, 10, 2)}\n",
    "            kfold = KFold(10)\n",
    "            extra_model = ExtraTreesClassifier(random_state=0)\n",
    "            grid_model = GridSearchCV(extra_model, parameters, scoring='accuracy', cv=kfold, n_jobs=-1)\n",
    "            grid_model.fit(self.X_train, self.y_train)\n",
    "            params_ls = grid_model.cv_results_['params']\n",
    "            mean_test_score_ls = grid_model.cv_results_[\"mean_test_score\"]\n",
    "            plt.plot(mean_test_score_ls)\n",
    "            plt.title(\"Extra random forest test score\")\n",
    "            plt.show()\n",
    "            display_(grid_model.best_score_)\n",
    "            display_(grid_model.best_params_)\n",
    "        \n",
    "        if self.is_run_svc:\n",
    "            self.svc_model = SVC(kernel='rbf',random_state=0).fit(self.X_train, self.y_train)\n",
    "            display_(\"##### Kernel Support Vector Machine classification report : \")\n",
    "            self.print_classification_report(self.svc_model, self.X_test, self.y_test)\n",
    "            display_(\"##### Kernel Support Vector Machine <span style='color: red'>randomly selected 1000 data</span> classification report : \")\n",
    "            self.randomly_selected_classification(self.svc_model)\n",
    "        \n",
    "        if self.is_run_xgboost:\n",
    "            display_(\"#### Run xgboost\")\n",
    "            label_enc = LabelEncoder().fit(self.y_data)\n",
    "\n",
    "            def get_encoded_target(y, label_enc):\n",
    "                return label_enc.transform(y)\n",
    "\n",
    "            def xgb_preprocess(x, y, is_need_to_be_encoded=False):\n",
    "                y_label = y\n",
    "                if is_need_to_be_encoded:\n",
    "                    y_label = get_encoded_target(y, label_enc) \n",
    "                \n",
    "                return xgboost.DMatrix(x, label = y_label)\n",
    "\n",
    "            def set_predicted_values(model, x, y):\n",
    "                test_y = get_encoded_target(y, label_enc)\n",
    "                test_X = xgb_preprocess(x, test_y)\n",
    "                y_pred_proba = model.predict(test_X)\n",
    "                y_pred = [np.argmax(line) for line in y_pred_proba]\n",
    "                y_true = label_enc.inverse_transform(test_y)\n",
    "                y_pred = label_enc.inverse_transform(y_pred)\n",
    "                return y_true, y_pred\n",
    "            \n",
    "            def print_classification_report_xgb(model, x, y):\n",
    "                y_true, y_pred = set_predicted_values(model, x, y)\n",
    "                print(classification_report(y_true, y_pred))\n",
    "                \n",
    "            def print_classification_report_xgb_1000(model):\n",
    "                x = self.X_data\n",
    "                y = self.y_data\n",
    "                idx = np.random.choice(x.shape[0], 1000, replace=False)\n",
    "                \n",
    "                x_test = x[idx]\n",
    "                y_true = y[idx]\n",
    "                if self.preprocessed_data.is_vectorize:\n",
    "                    x_test = self.preprocessed_data.get_vectorized_data_to_eval(x_test, self.preprocessed_data.is_tfidf_vect)\n",
    "                print_classification_report_xgb(model, x_test, y_true)\n",
    "                \n",
    "                display_(\"##### Ten-fold cross-validation accuracy score : \")\n",
    "                ac_score = np.zeros(10)\n",
    "                for i in list(range(10)):\n",
    "                    idx = np.random.choice(x.shape[0], 1000, replace=False)\n",
    "                    x_test = x[idx]\n",
    "                    y_true = y[idx]\n",
    "                    if self.preprocessed_data.is_vectorize:\n",
    "                        x_test = self.preprocessed_data.get_vectorized_data_to_eval(x_test, self.preprocessed_data.is_tfidf_vect)\n",
    "                    y_true, y_pred = set_predicted_values(model, x_test, y_true)\n",
    "                    ac_score[i] = accuracy_score(y_true, y_pred)\n",
    "                display_(\"##### : \" + str(ac_score.mean()*100))\n",
    "            \n",
    "            def runXgboost(num_boost_round = num_boost_round, lr = lr, max_delta_step = max_delta_step):\n",
    "                dtrain = xgb_preprocess(self.X_train, self.y_train, True)\n",
    "                dtest = xgb_preprocess(self.X_test, self.y_test, True)\n",
    "\n",
    "                params = {'objective': 'multi:softprob', \n",
    "                          'eval_metric': 'mlogloss',\n",
    "                          'num_class': 3, \n",
    "                          'max_delta_step': max_delta_step, \n",
    "                          'eta': lr}\n",
    "\n",
    "                evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "                xgb = xgboost.train(params=params,  \n",
    "                                dtrain=dtrain, \n",
    "                                num_boost_round=num_boost_round, \n",
    "                                evals=evals,\n",
    "                                verbose_eval=False,\n",
    "                                early_stopping_rounds=10)\n",
    "                return xgb\n",
    "            \n",
    "            self.xgb_model = runXgboost()\n",
    "            display_(\"##### XGBoost classification report : \")\n",
    "            print_classification_report_xgb(self.xgb_model, self.X_test, self.y_test)\n",
    "            display_(\"##### XGBoost <span style='color: red'>randomly selected 1000 data</span> classification report : \")\n",
    "            print_classification_report_xgb_1000(self.xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model_classification_report(dnn_model, preprocessed, is_keras=False):\n",
    "    _, X_test, _, y_test = preprocessed.get_preprocessed_data()\n",
    "    y_pred_proba = dnn_model.predict(X_test) if is_keras else dnn_model.predict()\n",
    "        \n",
    "    y_pred = preprocessed.get_inversed_target_data(y_pred_proba)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing DNNModel.py\n"
     ]
    }
   ],
   "source": [
    "class DNNModel:\n",
    "    def __init__(self, sess, name, preprocessed_data, lr=0.01, nn=0):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.preprocessed_data = preprocessed_data\n",
    "        X, y = preprocessed_data.get_original_data()\n",
    "        if self.preprocessed_data.is_vectorize:\n",
    "            X = preprocessed_data.get_vectorized_data()\n",
    "        self.features_num = X.shape[1]\n",
    "        self.class_num = len(y.unique())\n",
    "        self.neuron_num = nn\n",
    "        if nn == 0:\n",
    "            self.neuron_num = 20 if preprocessed_data.is_vectorize else 120\n",
    "            \n",
    "        self.lr = lr\n",
    "        self.__build_net()\n",
    "        \n",
    "    def __build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, shape=[None, self.features_num])\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, self.class_num])\n",
    "\n",
    "            # layer 1\n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.features_num, self.neuron_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.random_normal([self.neuron_num]))\n",
    "            layer1 = tf.nn.relu(tf.matmul(self.X, W1) + b1)\n",
    "\n",
    "            # layer 2\n",
    "            W2 = tf.get_variable(\"W2\", shape=[self.neuron_num, self.neuron_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.random_normal([self.neuron_num]))\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "            # layer 3\n",
    "            W3 = tf.get_variable(\"W3\", shape=[self.neuron_num, self.neuron_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3 = tf.Variable(tf.random_normal([self.neuron_num]))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "            # layer 4\n",
    "            W4 = tf.get_variable(\"W4\", shape=[self.neuron_num, self.class_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4 = tf.Variable(tf.random_normal([self.class_num]))\n",
    "            self.logits = tf.matmul(layer3, W4) + b4\n",
    "            \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.cost)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "    \n",
    "    def predict(self, X_test=None, keep_prob=0.7, training=False):\n",
    "        if X_test == None:\n",
    "            X_test = self.X_test\n",
    "        return self.sess.run(self.logits, feed_dict={self.X : X_test})\n",
    "        \n",
    "    def get_accuracy(self, keep_prob=0.7, training=False):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X : self.X_test, self.y : self.y_test})\n",
    "    \n",
    "    def train(self, keep_prob=0.7, training=True):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.preprocessed_data.get_preprocessed_data(is_for_nn=True) \n",
    "        return self.sess.run([self.cost, self.optimizer], \n",
    "                             feed_dict={self.X: self.X_train, self.y: self.y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_preprocessed = DataPreprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6801826\n",
      "100 0.23562054\n",
      "200 0.08163957\n",
      "300 0.1799301\n",
      "400 0.5350644\n",
      "500 0.085048646\n",
      "600 0.011947288\n",
      "700 0.0034478495\n",
      "800 0.0019695587\n",
      "900 0.0014206497\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "dnn_model = DNNModel(sess, \"dnn_model\", dnn_preprocessed)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoch = 1000\n",
    "\n",
    "for step in range(epoch):\n",
    "    c, _ = dnn_model.train()\n",
    "    if step % 100 == 0:\n",
    "        print(step, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0010971328\n",
      "100 0.0009613466\n",
      "200 0.00089344446\n",
      "300 0.0008697892\n",
      "400 0.0008477885\n",
      "500 0.00083189347\n",
      "600 0.00081978476\n",
      "700 0.00081135164\n",
      "800 0.00080673845\n",
      "900 0.000800729\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "\n",
    "for step in range(epoch):\n",
    "    c, _ = dnn_model.train()\n",
    "    if step % 100 == 0:\n",
    "        print(step, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.78      0.79      0.78       320\n",
      "         IE       0.81      0.79      0.80       327\n",
      "          N       0.89      0.89      0.89       629\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnn_model_classification_report(dnn_model, dnn_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)  # warning 출력 방지\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1914, 60), (1914, 3))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing KerasDNN.py\n"
     ]
    }
   ],
   "source": [
    "class KerasDNN:\n",
    "    def __init__(self, preprocessed_data):\n",
    "        self.preprocessed_data = preprocessed_data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.preprocessed_data.get_preprocessed_data(is_for_nn=True)\n",
    "\n",
    "    def make_keras_model(self, n_li, activation_li, r_seed=0, lr=0.01,\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]):\n",
    "        input_dim = self.X_train.shape[1]\n",
    "        output_dim = self.y_train.shape[1]\n",
    "\n",
    "        np.random.seed(r_seed)\n",
    "        self.model = Sequential()\n",
    "\n",
    "        n_li.append(output_dim)\n",
    "\n",
    "        for idx, n in enumerate(n_li):\n",
    "            if idx == 0:\n",
    "                self.model.add(Dense(120, input_dim=input_dim, activation=activation_li[idx]))\n",
    "            else:\n",
    "                self.model.add(Dense(n, activation=activation_li[idx]))\n",
    "        self.model.compile(optimizer=SGD(lr=lr), loss=loss, metrics=metrics)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def run_keras_model(self, epochs=1000, batch_size=100):\n",
    "        %%time\n",
    "        self.hist = self.model.fit(self.X_train, self.y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                   validation_data=(self.X_test, self.y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.82 µs\n",
      "Train on 1914 samples, validate on 1276 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 1.1304 - acc: 0.5010 - val_loss: 1.0620 - val_acc: 0.5157\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.9315 - acc: 0.5496 - val_loss: 1.2053 - val_acc: 0.4122\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.9192 - acc: 0.5596 - val_loss: 0.9671 - val_acc: 0.4984\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.9156 - acc: 0.5564 - val_loss: 0.9571 - val_acc: 0.5274\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.8975 - acc: 0.5679 - val_loss: 0.9713 - val_acc: 0.5204\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.8946 - acc: 0.5648 - val_loss: 0.9482 - val_acc: 0.4984\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.8992 - acc: 0.5648 - val_loss: 0.9478 - val_acc: 0.5180\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.8933 - acc: 0.5705 - val_loss: 0.9995 - val_acc: 0.4922\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.9018 - acc: 0.5653 - val_loss: 0.9583 - val_acc: 0.5306\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.8891 - acc: 0.5726 - val_loss: 1.0189 - val_acc: 0.5055\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.8961 - acc: 0.5721 - val_loss: 1.1514 - val_acc: 0.3911\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.9044 - acc: 0.5705 - val_loss: 0.9618 - val_acc: 0.5047\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.8869 - acc: 0.5763 - val_loss: 1.0612 - val_acc: 0.5016\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.8944 - acc: 0.5737 - val_loss: 0.9424 - val_acc: 0.5259\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.8818 - acc: 0.5820 - val_loss: 0.9994 - val_acc: 0.5274\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.8924 - acc: 0.5716 - val_loss: 0.9665 - val_acc: 0.5157\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.8942 - acc: 0.5695 - val_loss: 1.0264 - val_acc: 0.4639\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.8870 - acc: 0.5831 - val_loss: 0.9192 - val_acc: 0.5439\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.8846 - acc: 0.5737 - val_loss: 1.1643 - val_acc: 0.4498\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.8927 - acc: 0.5700 - val_loss: 0.9256 - val_acc: 0.5290\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.8861 - acc: 0.5700 - val_loss: 1.0401 - val_acc: 0.4569\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.8928 - acc: 0.5627 - val_loss: 0.9154 - val_acc: 0.5517\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.8808 - acc: 0.5737 - val_loss: 0.9461 - val_acc: 0.5086\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.8851 - acc: 0.5820 - val_loss: 0.9510 - val_acc: 0.4984\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.8778 - acc: 0.5784 - val_loss: 0.9302 - val_acc: 0.5392\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.8764 - acc: 0.5799 - val_loss: 1.0111 - val_acc: 0.5212\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.8807 - acc: 0.5794 - val_loss: 0.9493 - val_acc: 0.5282\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.8774 - acc: 0.5784 - val_loss: 1.0289 - val_acc: 0.4984\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.8785 - acc: 0.5846 - val_loss: 0.9846 - val_acc: 0.4898\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.8809 - acc: 0.5846 - val_loss: 1.0249 - val_acc: 0.4647\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.8777 - acc: 0.5831 - val_loss: 0.9354 - val_acc: 0.5306\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.8720 - acc: 0.5862 - val_loss: 1.0406 - val_acc: 0.5086\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.8852 - acc: 0.5726 - val_loss: 0.9414 - val_acc: 0.5188\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.8751 - acc: 0.5810 - val_loss: 0.9737 - val_acc: 0.5298\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.8785 - acc: 0.5820 - val_loss: 0.9559 - val_acc: 0.5361\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.8772 - acc: 0.5810 - val_loss: 1.0174 - val_acc: 0.4788\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.8800 - acc: 0.5810 - val_loss: 0.9289 - val_acc: 0.5368\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.8744 - acc: 0.5920 - val_loss: 0.9225 - val_acc: 0.5321\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.8728 - acc: 0.5873 - val_loss: 0.9610 - val_acc: 0.5110\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.8746 - acc: 0.5846 - val_loss: 0.9840 - val_acc: 0.5055\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.8770 - acc: 0.5846 - val_loss: 0.9574 - val_acc: 0.5266\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.8761 - acc: 0.5899 - val_loss: 1.0167 - val_acc: 0.4976\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.8749 - acc: 0.5805 - val_loss: 0.9756 - val_acc: 0.5055\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.8721 - acc: 0.5909 - val_loss: 1.0472 - val_acc: 0.5008\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.8791 - acc: 0.5815 - val_loss: 0.9835 - val_acc: 0.4859\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.8780 - acc: 0.5763 - val_loss: 0.9908 - val_acc: 0.5086\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.8773 - acc: 0.5778 - val_loss: 0.9122 - val_acc: 0.5478\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.8725 - acc: 0.5831 - val_loss: 0.9309 - val_acc: 0.5400\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.8702 - acc: 0.5909 - val_loss: 0.9193 - val_acc: 0.5486\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.8746 - acc: 0.5810 - val_loss: 0.9664 - val_acc: 0.5172\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.8680 - acc: 0.5951 - val_loss: 0.9499 - val_acc: 0.5298\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.8690 - acc: 0.5878 - val_loss: 0.9612 - val_acc: 0.5408\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.8792 - acc: 0.5784 - val_loss: 0.9473 - val_acc: 0.5094\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.8709 - acc: 0.5852 - val_loss: 0.9285 - val_acc: 0.5400\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.8645 - acc: 0.5904 - val_loss: 0.9640 - val_acc: 0.5110\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.8693 - acc: 0.5867 - val_loss: 0.9437 - val_acc: 0.5353\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.8689 - acc: 0.5852 - val_loss: 0.9838 - val_acc: 0.5172\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.8728 - acc: 0.5914 - val_loss: 0.9222 - val_acc: 0.5509\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.8672 - acc: 0.5904 - val_loss: 0.9214 - val_acc: 0.5439\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.8663 - acc: 0.5909 - val_loss: 0.9531 - val_acc: 0.5219\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.8716 - acc: 0.5888 - val_loss: 0.9396 - val_acc: 0.5345\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.8665 - acc: 0.5977 - val_loss: 0.9375 - val_acc: 0.5376\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.8679 - acc: 0.5841 - val_loss: 0.9166 - val_acc: 0.5462\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.8668 - acc: 0.5925 - val_loss: 0.9817 - val_acc: 0.5141\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.8687 - acc: 0.5899 - val_loss: 0.9487 - val_acc: 0.5266\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.8661 - acc: 0.5940 - val_loss: 0.9235 - val_acc: 0.5423\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.8673 - acc: 0.5909 - val_loss: 0.9442 - val_acc: 0.5219\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.8675 - acc: 0.5873 - val_loss: 0.9630 - val_acc: 0.5157\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.8733 - acc: 0.5852 - val_loss: 0.9494 - val_acc: 0.5047\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.8638 - acc: 0.5883 - val_loss: 0.9356 - val_acc: 0.5368\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.8661 - acc: 0.5846 - val_loss: 0.9176 - val_acc: 0.5376\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.8658 - acc: 0.5852 - val_loss: 1.1064 - val_acc: 0.5016\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.8692 - acc: 0.5925 - val_loss: 0.9444 - val_acc: 0.5376\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.8641 - acc: 0.5909 - val_loss: 0.9099 - val_acc: 0.5470\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.8677 - acc: 0.5930 - val_loss: 0.9381 - val_acc: 0.5329\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.8667 - acc: 0.5946 - val_loss: 0.9303 - val_acc: 0.5439\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.8649 - acc: 0.5940 - val_loss: 0.9588 - val_acc: 0.5337\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.8650 - acc: 0.5977 - val_loss: 0.9264 - val_acc: 0.5447\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.8650 - acc: 0.5909 - val_loss: 0.9412 - val_acc: 0.5180\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.8654 - acc: 0.5836 - val_loss: 0.9396 - val_acc: 0.5180\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.8634 - acc: 0.5925 - val_loss: 0.9351 - val_acc: 0.5345\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.8609 - acc: 0.5946 - val_loss: 0.9367 - val_acc: 0.5376\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.8691 - acc: 0.5878 - val_loss: 1.0663 - val_acc: 0.5016\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.8714 - acc: 0.5867 - val_loss: 0.9390 - val_acc: 0.5298\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.8649 - acc: 0.5904 - val_loss: 1.0430 - val_acc: 0.5008\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.8680 - acc: 0.5883 - val_loss: 0.9237 - val_acc: 0.5384\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.8626 - acc: 0.5993 - val_loss: 0.9832 - val_acc: 0.4922\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.8688 - acc: 0.5899 - val_loss: 0.9192 - val_acc: 0.5462\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.8626 - acc: 0.5935 - val_loss: 0.9358 - val_acc: 0.5282\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.8634 - acc: 0.5920 - val_loss: 0.9577 - val_acc: 0.5149\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.8653 - acc: 0.5925 - val_loss: 0.9188 - val_acc: 0.5486\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.8654 - acc: 0.5930 - val_loss: 0.9892 - val_acc: 0.4992\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.8687 - acc: 0.5893 - val_loss: 0.9251 - val_acc: 0.5431\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.8622 - acc: 0.5972 - val_loss: 0.9438 - val_acc: 0.5329\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.8616 - acc: 0.5914 - val_loss: 1.0341 - val_acc: 0.5031\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 0.8641 - acc: 0.5815 - val_loss: 0.9228 - val_acc: 0.5361\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.8600 - acc: 0.5993 - val_loss: 0.9470 - val_acc: 0.5431\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.8602 - acc: 0.5883 - val_loss: 0.9504 - val_acc: 0.5290\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.8624 - acc: 0.5852 - val_loss: 0.9364 - val_acc: 0.5408\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.8620 - acc: 0.5961 - val_loss: 0.9487 - val_acc: 0.5313\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.8614 - acc: 0.5857 - val_loss: 0.9459 - val_acc: 0.5384\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.8605 - acc: 0.5904 - val_loss: 0.9345 - val_acc: 0.5423\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.8585 - acc: 0.5899 - val_loss: 0.9391 - val_acc: 0.4992\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.8587 - acc: 0.5977 - val_loss: 0.9878 - val_acc: 0.5141\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.8676 - acc: 0.5899 - val_loss: 0.9464 - val_acc: 0.5329\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.8568 - acc: 0.5972 - val_loss: 0.9709 - val_acc: 0.5204\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.8618 - acc: 0.5956 - val_loss: 0.9305 - val_acc: 0.5313\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.8638 - acc: 0.5893 - val_loss: 1.0025 - val_acc: 0.5047\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.8622 - acc: 0.5909 - val_loss: 0.9888 - val_acc: 0.5031\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.8657 - acc: 0.5878 - val_loss: 0.9150 - val_acc: 0.5345\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.8567 - acc: 0.5956 - val_loss: 0.9149 - val_acc: 0.5541\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.8584 - acc: 0.5925 - val_loss: 0.9487 - val_acc: 0.5368\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.8572 - acc: 0.5998 - val_loss: 0.9360 - val_acc: 0.5259\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.8581 - acc: 0.6014 - val_loss: 0.9252 - val_acc: 0.5321\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.8614 - acc: 0.5867 - val_loss: 0.9449 - val_acc: 0.5157\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.8621 - acc: 0.5930 - val_loss: 0.9228 - val_acc: 0.5313\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.8593 - acc: 0.5878 - val_loss: 1.0146 - val_acc: 0.4765\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.8618 - acc: 0.5956 - val_loss: 0.9547 - val_acc: 0.5298\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.8538 - acc: 0.5914 - val_loss: 0.9152 - val_acc: 0.5478\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.8552 - acc: 0.5951 - val_loss: 0.9315 - val_acc: 0.5455\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.8601 - acc: 0.5904 - val_loss: 0.9307 - val_acc: 0.5447\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.8572 - acc: 0.6014 - val_loss: 0.9524 - val_acc: 0.5392\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.8574 - acc: 0.5935 - val_loss: 0.9214 - val_acc: 0.5439\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.8605 - acc: 0.5956 - val_loss: 0.9887 - val_acc: 0.5251\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.8603 - acc: 0.5909 - val_loss: 0.9173 - val_acc: 0.5462\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.8566 - acc: 0.6014 - val_loss: 0.9358 - val_acc: 0.5384\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.8565 - acc: 0.5987 - val_loss: 0.9607 - val_acc: 0.5212\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.8580 - acc: 0.5951 - val_loss: 0.9109 - val_acc: 0.5455\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.8586 - acc: 0.5789 - val_loss: 0.9484 - val_acc: 0.5259\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.8578 - acc: 0.5920 - val_loss: 0.9505 - val_acc: 0.5055\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.8607 - acc: 0.5904 - val_loss: 1.0422 - val_acc: 0.4781\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.8703 - acc: 0.5794 - val_loss: 1.0292 - val_acc: 0.5188\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.8632 - acc: 0.5893 - val_loss: 0.9983 - val_acc: 0.5306\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.8580 - acc: 0.5998 - val_loss: 0.9276 - val_acc: 0.5408\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.8578 - acc: 0.5899 - val_loss: 0.9459 - val_acc: 0.5102\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.8685 - acc: 0.5852 - val_loss: 0.9425 - val_acc: 0.5408\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.8549 - acc: 0.5967 - val_loss: 0.9510 - val_acc: 0.5368\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.8555 - acc: 0.5930 - val_loss: 0.9210 - val_acc: 0.5408\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.8577 - acc: 0.5852 - val_loss: 1.0050 - val_acc: 0.5078\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.8576 - acc: 0.5920 - val_loss: 0.9205 - val_acc: 0.5431\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.8566 - acc: 0.6008 - val_loss: 0.9272 - val_acc: 0.5337\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.8596 - acc: 0.5946 - val_loss: 0.9501 - val_acc: 0.5141\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.8573 - acc: 0.5961 - val_loss: 0.9215 - val_acc: 0.5329\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.8528 - acc: 0.5987 - val_loss: 0.9333 - val_acc: 0.5306\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.8567 - acc: 0.6014 - val_loss: 0.9285 - val_acc: 0.5439\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.8513 - acc: 0.5930 - val_loss: 0.9458 - val_acc: 0.5313\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.8554 - acc: 0.6014 - val_loss: 0.9233 - val_acc: 0.5329\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.8544 - acc: 0.5977 - val_loss: 0.9394 - val_acc: 0.5259\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.8567 - acc: 0.5904 - val_loss: 0.9118 - val_acc: 0.5509\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.8591 - acc: 0.5935 - val_loss: 0.9287 - val_acc: 0.5494\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 0.8608 - acc: 0.5940 - val_loss: 0.9327 - val_acc: 0.5329\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.8513 - acc: 0.5956 - val_loss: 0.9266 - val_acc: 0.5455\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.8529 - acc: 0.5998 - val_loss: 0.9847 - val_acc: 0.5016\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.8589 - acc: 0.5893 - val_loss: 0.9614 - val_acc: 0.5251\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.8561 - acc: 0.5998 - val_loss: 0.9207 - val_acc: 0.5337\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.8548 - acc: 0.6034 - val_loss: 0.9162 - val_acc: 0.5541\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.8562 - acc: 0.6014 - val_loss: 0.9310 - val_acc: 0.5376\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.8583 - acc: 0.5883 - val_loss: 0.9480 - val_acc: 0.5094\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.8560 - acc: 0.5946 - val_loss: 0.9974 - val_acc: 0.5094\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.8553 - acc: 0.5951 - val_loss: 0.9412 - val_acc: 0.5423\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.8542 - acc: 0.5925 - val_loss: 0.9522 - val_acc: 0.5306\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.8554 - acc: 0.5893 - val_loss: 0.9576 - val_acc: 0.5133\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.8570 - acc: 0.5904 - val_loss: 0.9392 - val_acc: 0.5259\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.8524 - acc: 0.5930 - val_loss: 0.9984 - val_acc: 0.5008\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.8578 - acc: 0.5946 - val_loss: 1.0204 - val_acc: 0.4992\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.8560 - acc: 0.5878 - val_loss: 0.9690 - val_acc: 0.5243\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.8541 - acc: 0.6024 - val_loss: 0.9207 - val_acc: 0.5455\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.8498 - acc: 0.5925 - val_loss: 0.9497 - val_acc: 0.5306\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.8519 - acc: 0.5982 - val_loss: 0.9532 - val_acc: 0.5086\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.8575 - acc: 0.5951 - val_loss: 0.9157 - val_acc: 0.5447\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 0.8514 - acc: 0.5987 - val_loss: 0.9465 - val_acc: 0.5266\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.8532 - acc: 0.5951 - val_loss: 0.9470 - val_acc: 0.5251\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.8584 - acc: 0.5982 - val_loss: 1.0920 - val_acc: 0.5118\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.8646 - acc: 0.6029 - val_loss: 1.0185 - val_acc: 0.5235\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.8556 - acc: 0.5972 - val_loss: 0.9337 - val_acc: 0.5227\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.8515 - acc: 0.5930 - val_loss: 0.9199 - val_acc: 0.5494\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.8526 - acc: 0.5951 - val_loss: 0.9576 - val_acc: 0.5361\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.8521 - acc: 0.5961 - val_loss: 0.9304 - val_acc: 0.5376\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.8534 - acc: 0.5946 - val_loss: 0.9245 - val_acc: 0.5431\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.8504 - acc: 0.6008 - val_loss: 0.9392 - val_acc: 0.5384\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.8513 - acc: 0.5987 - val_loss: 0.9684 - val_acc: 0.5266\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.8548 - acc: 0.5993 - val_loss: 0.9263 - val_acc: 0.5329\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.8498 - acc: 0.5977 - val_loss: 0.9148 - val_acc: 0.5423\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.8534 - acc: 0.5930 - val_loss: 0.9354 - val_acc: 0.5431\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.8518 - acc: 0.5987 - val_loss: 0.9700 - val_acc: 0.5368\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.8505 - acc: 0.5987 - val_loss: 0.9413 - val_acc: 0.5188\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.8530 - acc: 0.6008 - val_loss: 0.9457 - val_acc: 0.5329\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.8570 - acc: 0.5951 - val_loss: 0.9314 - val_acc: 0.5353\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.8544 - acc: 0.5940 - val_loss: 0.9730 - val_acc: 0.5376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/1000\n",
      " - 0s - loss: 0.8551 - acc: 0.6019 - val_loss: 1.1019 - val_acc: 0.4976\n",
      "Epoch 191/1000\n",
      " - 0s - loss: 0.8582 - acc: 0.5993 - val_loss: 0.9291 - val_acc: 0.5400\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.8517 - acc: 0.5972 - val_loss: 0.9174 - val_acc: 0.5556\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.8508 - acc: 0.5982 - val_loss: 0.9582 - val_acc: 0.5368\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.8527 - acc: 0.5951 - val_loss: 0.9954 - val_acc: 0.5227\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.8532 - acc: 0.5961 - val_loss: 0.9504 - val_acc: 0.5384\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.8557 - acc: 0.5951 - val_loss: 0.9616 - val_acc: 0.5282\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.8552 - acc: 0.5935 - val_loss: 0.9401 - val_acc: 0.5423\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.8536 - acc: 0.5940 - val_loss: 0.9289 - val_acc: 0.5447\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.8544 - acc: 0.5982 - val_loss: 0.9272 - val_acc: 0.5282\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.8539 - acc: 0.5946 - val_loss: 0.9114 - val_acc: 0.5580\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.8518 - acc: 0.5967 - val_loss: 0.9556 - val_acc: 0.5125\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.8495 - acc: 0.5982 - val_loss: 0.9499 - val_acc: 0.5125\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.8578 - acc: 0.5873 - val_loss: 0.9708 - val_acc: 0.5251\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.8512 - acc: 0.5972 - val_loss: 0.9213 - val_acc: 0.5408\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.8511 - acc: 0.6050 - val_loss: 0.9145 - val_acc: 0.5462\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.8478 - acc: 0.6045 - val_loss: 0.9811 - val_acc: 0.5125\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.8576 - acc: 0.5925 - val_loss: 0.9174 - val_acc: 0.5400\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.8479 - acc: 0.6024 - val_loss: 0.9282 - val_acc: 0.5353\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.8500 - acc: 0.5982 - val_loss: 0.9520 - val_acc: 0.5321\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.8507 - acc: 0.6014 - val_loss: 1.0518 - val_acc: 0.5235\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.8549 - acc: 0.5982 - val_loss: 0.9151 - val_acc: 0.5462\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.8490 - acc: 0.6040 - val_loss: 0.9565 - val_acc: 0.5235\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.8500 - acc: 0.6014 - val_loss: 1.0173 - val_acc: 0.5259\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.8593 - acc: 0.5967 - val_loss: 0.9621 - val_acc: 0.5235\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.8533 - acc: 0.5846 - val_loss: 1.0974 - val_acc: 0.4969\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.8589 - acc: 0.5935 - val_loss: 0.9312 - val_acc: 0.5353\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.8479 - acc: 0.6066 - val_loss: 0.9340 - val_acc: 0.5282\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.8515 - acc: 0.5883 - val_loss: 0.9743 - val_acc: 0.5353\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.8466 - acc: 0.5956 - val_loss: 1.0409 - val_acc: 0.5008\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.8539 - acc: 0.5935 - val_loss: 0.9375 - val_acc: 0.5337\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.8534 - acc: 0.5993 - val_loss: 1.0033 - val_acc: 0.5243\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.8516 - acc: 0.6024 - val_loss: 0.9819 - val_acc: 0.5008\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.8553 - acc: 0.5867 - val_loss: 0.9750 - val_acc: 0.5306\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.8514 - acc: 0.5972 - val_loss: 0.9347 - val_acc: 0.5368\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.8465 - acc: 0.5940 - val_loss: 0.9652 - val_acc: 0.5306\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.8508 - acc: 0.5920 - val_loss: 0.9487 - val_acc: 0.5157\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.8516 - acc: 0.5930 - val_loss: 0.9140 - val_acc: 0.5596\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.8509 - acc: 0.5925 - val_loss: 0.9913 - val_acc: 0.5274\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.8547 - acc: 0.5935 - val_loss: 0.9256 - val_acc: 0.5337\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.8500 - acc: 0.5961 - val_loss: 1.0354 - val_acc: 0.5063\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.8537 - acc: 0.5935 - val_loss: 0.9196 - val_acc: 0.5478\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.8431 - acc: 0.5914 - val_loss: 0.9421 - val_acc: 0.5368\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.8488 - acc: 0.5987 - val_loss: 0.9266 - val_acc: 0.5384\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.8472 - acc: 0.5909 - val_loss: 0.9436 - val_acc: 0.5470\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.8498 - acc: 0.5914 - val_loss: 0.9336 - val_acc: 0.5361\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.8452 - acc: 0.6061 - val_loss: 0.9378 - val_acc: 0.5494\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.8499 - acc: 0.5946 - val_loss: 0.9330 - val_acc: 0.5384\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.8494 - acc: 0.5961 - val_loss: 0.9164 - val_acc: 0.5431\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.8473 - acc: 0.5946 - val_loss: 1.0045 - val_acc: 0.5219\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.8523 - acc: 0.5993 - val_loss: 0.9134 - val_acc: 0.5517\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.8438 - acc: 0.6066 - val_loss: 0.9364 - val_acc: 0.5141\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.8516 - acc: 0.6034 - val_loss: 0.9194 - val_acc: 0.5470\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.8439 - acc: 0.6055 - val_loss: 0.9170 - val_acc: 0.5361\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.8497 - acc: 0.5930 - val_loss: 0.9971 - val_acc: 0.5141\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.8497 - acc: 0.6003 - val_loss: 0.9449 - val_acc: 0.5423\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.8471 - acc: 0.5987 - val_loss: 0.9188 - val_acc: 0.5415\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.8438 - acc: 0.5993 - val_loss: 0.9357 - val_acc: 0.5400\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.8465 - acc: 0.6071 - val_loss: 0.9324 - val_acc: 0.5345\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.8441 - acc: 0.6003 - val_loss: 0.9138 - val_acc: 0.5439\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.8445 - acc: 0.6029 - val_loss: 0.9533 - val_acc: 0.5282\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.8449 - acc: 0.5972 - val_loss: 0.9234 - val_acc: 0.5368\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.8443 - acc: 0.6102 - val_loss: 0.9521 - val_acc: 0.5212\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.8472 - acc: 0.5972 - val_loss: 1.0441 - val_acc: 0.4890\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.8553 - acc: 0.5972 - val_loss: 0.9428 - val_acc: 0.5321\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.8478 - acc: 0.6019 - val_loss: 0.9555 - val_acc: 0.5157\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 0.8496 - acc: 0.6024 - val_loss: 0.9350 - val_acc: 0.5212\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.8489 - acc: 0.5951 - val_loss: 0.9269 - val_acc: 0.5400\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.8400 - acc: 0.6092 - val_loss: 1.0488 - val_acc: 0.4984\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.8558 - acc: 0.5867 - val_loss: 1.0393 - val_acc: 0.4984\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.8480 - acc: 0.5956 - val_loss: 0.9323 - val_acc: 0.5274\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.8445 - acc: 0.6045 - val_loss: 0.9545 - val_acc: 0.5266\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.8475 - acc: 0.5961 - val_loss: 0.9359 - val_acc: 0.5329\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.8456 - acc: 0.5987 - val_loss: 0.9347 - val_acc: 0.5455\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.8494 - acc: 0.6003 - val_loss: 0.9274 - val_acc: 0.5415\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.8452 - acc: 0.6029 - val_loss: 0.9550 - val_acc: 0.5431\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.8461 - acc: 0.5967 - val_loss: 0.9406 - val_acc: 0.5306\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.8500 - acc: 0.5977 - val_loss: 0.9406 - val_acc: 0.5361\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.8506 - acc: 0.5925 - val_loss: 0.9612 - val_acc: 0.5266\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.8453 - acc: 0.5982 - val_loss: 0.9336 - val_acc: 0.5455\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.8432 - acc: 0.6071 - val_loss: 0.9609 - val_acc: 0.5282\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.8439 - acc: 0.6008 - val_loss: 0.9843 - val_acc: 0.5016\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.8471 - acc: 0.5972 - val_loss: 0.9740 - val_acc: 0.5204\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.8500 - acc: 0.6003 - val_loss: 0.9385 - val_acc: 0.5329\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.8434 - acc: 0.6003 - val_loss: 0.9480 - val_acc: 0.5219\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.8435 - acc: 0.5987 - val_loss: 0.9233 - val_acc: 0.5486\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.8396 - acc: 0.6055 - val_loss: 1.0230 - val_acc: 0.5321\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.8437 - acc: 0.6050 - val_loss: 0.9278 - val_acc: 0.5321\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.8417 - acc: 0.6040 - val_loss: 0.9261 - val_acc: 0.5455\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.8450 - acc: 0.6019 - val_loss: 0.9492 - val_acc: 0.5313\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.8495 - acc: 0.5987 - val_loss: 0.9594 - val_acc: 0.5313\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.8418 - acc: 0.6019 - val_loss: 0.9379 - val_acc: 0.5321\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.8453 - acc: 0.6076 - val_loss: 0.9594 - val_acc: 0.5345\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.8465 - acc: 0.5956 - val_loss: 0.9420 - val_acc: 0.5345\n",
      "Epoch 284/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.8448 - acc: 0.6029 - val_loss: 0.9703 - val_acc: 0.5282\n",
      "Epoch 285/1000\n",
      " - 0s - loss: 0.8430 - acc: 0.6008 - val_loss: 0.9414 - val_acc: 0.5447\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.8421 - acc: 0.5967 - val_loss: 0.9405 - val_acc: 0.5345\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.8464 - acc: 0.5987 - val_loss: 0.9924 - val_acc: 0.5133\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.8505 - acc: 0.5909 - val_loss: 0.9341 - val_acc: 0.5392\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.8432 - acc: 0.6050 - val_loss: 0.9328 - val_acc: 0.5447\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.8432 - acc: 0.6040 - val_loss: 0.9374 - val_acc: 0.5345\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.8463 - acc: 0.5998 - val_loss: 0.9382 - val_acc: 0.5353\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.8425 - acc: 0.6050 - val_loss: 0.9339 - val_acc: 0.5408\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.8449 - acc: 0.6019 - val_loss: 0.9773 - val_acc: 0.5368\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.8426 - acc: 0.6029 - val_loss: 0.9283 - val_acc: 0.5439\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.8416 - acc: 0.6034 - val_loss: 0.9290 - val_acc: 0.5353\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.8421 - acc: 0.5993 - val_loss: 0.9237 - val_acc: 0.5368\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.8442 - acc: 0.6008 - val_loss: 0.9721 - val_acc: 0.5071\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.8448 - acc: 0.6003 - val_loss: 0.9203 - val_acc: 0.5361\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.8423 - acc: 0.5967 - val_loss: 0.9764 - val_acc: 0.5321\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 0.8451 - acc: 0.6050 - val_loss: 1.0258 - val_acc: 0.5180\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.8473 - acc: 0.6066 - val_loss: 0.9337 - val_acc: 0.5306\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.8476 - acc: 0.6024 - val_loss: 0.9945 - val_acc: 0.4922\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.8498 - acc: 0.5935 - val_loss: 0.9453 - val_acc: 0.5321\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.8467 - acc: 0.5972 - val_loss: 1.0115 - val_acc: 0.5118\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.8464 - acc: 0.6003 - val_loss: 0.9366 - val_acc: 0.5384\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.8413 - acc: 0.5998 - val_loss: 0.9424 - val_acc: 0.5290\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.8436 - acc: 0.6050 - val_loss: 0.9432 - val_acc: 0.5361\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.8405 - acc: 0.6050 - val_loss: 0.9539 - val_acc: 0.5188\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.8433 - acc: 0.6040 - val_loss: 0.9453 - val_acc: 0.5165\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.8434 - acc: 0.6082 - val_loss: 0.9378 - val_acc: 0.5376\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.8392 - acc: 0.6050 - val_loss: 0.9559 - val_acc: 0.5400\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.8449 - acc: 0.6045 - val_loss: 0.9225 - val_acc: 0.5470\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.8434 - acc: 0.5977 - val_loss: 0.9500 - val_acc: 0.5337\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.8427 - acc: 0.5987 - val_loss: 0.9325 - val_acc: 0.5353\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.8438 - acc: 0.5993 - val_loss: 0.9513 - val_acc: 0.5376\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.8449 - acc: 0.6082 - val_loss: 0.9372 - val_acc: 0.5447\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.8432 - acc: 0.6003 - val_loss: 0.9507 - val_acc: 0.5212\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.8450 - acc: 0.5998 - val_loss: 0.9726 - val_acc: 0.5259\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.8443 - acc: 0.6040 - val_loss: 0.9293 - val_acc: 0.5329\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.8409 - acc: 0.6040 - val_loss: 0.9196 - val_acc: 0.5392\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.8403 - acc: 0.6055 - val_loss: 0.9786 - val_acc: 0.5235\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.8456 - acc: 0.5899 - val_loss: 0.9378 - val_acc: 0.5400\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.8406 - acc: 0.6034 - val_loss: 0.9285 - val_acc: 0.5423\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.8418 - acc: 0.5977 - val_loss: 1.0032 - val_acc: 0.5219\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.8427 - acc: 0.6034 - val_loss: 0.9539 - val_acc: 0.5259\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.8454 - acc: 0.6061 - val_loss: 0.9516 - val_acc: 0.5266\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.8432 - acc: 0.5951 - val_loss: 0.9473 - val_acc: 0.5180\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.8457 - acc: 0.5967 - val_loss: 0.9340 - val_acc: 0.5423\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.8419 - acc: 0.5972 - val_loss: 0.9668 - val_acc: 0.5290\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.8429 - acc: 0.5967 - val_loss: 0.9194 - val_acc: 0.5486\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.8369 - acc: 0.5982 - val_loss: 0.9712 - val_acc: 0.5266\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.8439 - acc: 0.5967 - val_loss: 0.9654 - val_acc: 0.5306\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.8416 - acc: 0.5967 - val_loss: 0.9592 - val_acc: 0.5376\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.8405 - acc: 0.6050 - val_loss: 0.9539 - val_acc: 0.5259\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.8412 - acc: 0.5998 - val_loss: 0.9745 - val_acc: 0.5329\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.8398 - acc: 0.6055 - val_loss: 0.9793 - val_acc: 0.5118\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.8423 - acc: 0.6034 - val_loss: 0.9241 - val_acc: 0.5431\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.8379 - acc: 0.6019 - val_loss: 0.9675 - val_acc: 0.5274\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.8402 - acc: 0.6024 - val_loss: 0.9273 - val_acc: 0.5384\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.8411 - acc: 0.5998 - val_loss: 0.9449 - val_acc: 0.5274\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 0.8395 - acc: 0.6024 - val_loss: 0.9966 - val_acc: 0.5094\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.8419 - acc: 0.5961 - val_loss: 0.9262 - val_acc: 0.5400\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.8363 - acc: 0.6040 - val_loss: 0.9708 - val_acc: 0.5251\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.8406 - acc: 0.6050 - val_loss: 0.9546 - val_acc: 0.5165\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.8423 - acc: 0.6087 - val_loss: 0.9804 - val_acc: 0.5149\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.8409 - acc: 0.6055 - val_loss: 0.9438 - val_acc: 0.5408\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.8382 - acc: 0.6108 - val_loss: 0.9277 - val_acc: 0.5400\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.8381 - acc: 0.6014 - val_loss: 0.9301 - val_acc: 0.5431\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.8377 - acc: 0.5956 - val_loss: 1.0007 - val_acc: 0.5306\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.8399 - acc: 0.6061 - val_loss: 0.9435 - val_acc: 0.5212\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.8393 - acc: 0.5998 - val_loss: 0.9384 - val_acc: 0.5188\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.8425 - acc: 0.6014 - val_loss: 0.9457 - val_acc: 0.5227\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.8392 - acc: 0.6014 - val_loss: 0.9456 - val_acc: 0.5525\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.8354 - acc: 0.6123 - val_loss: 0.9312 - val_acc: 0.5470\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.8364 - acc: 0.6045 - val_loss: 0.9419 - val_acc: 0.5337\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.8373 - acc: 0.6024 - val_loss: 0.9620 - val_acc: 0.5063\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.8401 - acc: 0.6066 - val_loss: 0.9374 - val_acc: 0.5486\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.8399 - acc: 0.6034 - val_loss: 0.9255 - val_acc: 0.5541\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.8377 - acc: 0.5951 - val_loss: 0.9722 - val_acc: 0.5329\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.8386 - acc: 0.5956 - val_loss: 0.9549 - val_acc: 0.5259\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.8420 - acc: 0.6034 - val_loss: 0.9526 - val_acc: 0.5400\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.8403 - acc: 0.6061 - val_loss: 0.9253 - val_acc: 0.5282\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.8414 - acc: 0.5951 - val_loss: 0.9450 - val_acc: 0.5353\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.8350 - acc: 0.6019 - val_loss: 0.9597 - val_acc: 0.5290\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.8366 - acc: 0.6045 - val_loss: 0.9337 - val_acc: 0.5251\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.8370 - acc: 0.6102 - val_loss: 0.9551 - val_acc: 0.5400\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.8396 - acc: 0.6024 - val_loss: 0.9574 - val_acc: 0.5361\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.8393 - acc: 0.6014 - val_loss: 0.9445 - val_acc: 0.5219\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.8441 - acc: 0.6055 - val_loss: 0.9296 - val_acc: 0.5556\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.8360 - acc: 0.6076 - val_loss: 0.9412 - val_acc: 0.5361\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.8393 - acc: 0.5930 - val_loss: 0.9879 - val_acc: 0.5337\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.8417 - acc: 0.5977 - val_loss: 0.9657 - val_acc: 0.5345\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.8427 - acc: 0.6003 - val_loss: 0.9377 - val_acc: 0.5509\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.8348 - acc: 0.6055 - val_loss: 1.0231 - val_acc: 0.5196\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.8388 - acc: 0.6008 - val_loss: 0.9257 - val_acc: 0.5329\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.8355 - acc: 0.6092 - val_loss: 0.9505 - val_acc: 0.5368\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.8395 - acc: 0.5951 - val_loss: 0.9568 - val_acc: 0.5329\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.8380 - acc: 0.5987 - val_loss: 0.9272 - val_acc: 0.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/1000\n",
      " - 0s - loss: 0.8360 - acc: 0.6092 - val_loss: 1.1093 - val_acc: 0.4969\n",
      "Epoch 380/1000\n",
      " - 0s - loss: 0.8437 - acc: 0.6082 - val_loss: 1.0150 - val_acc: 0.4969\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.8441 - acc: 0.5951 - val_loss: 0.9215 - val_acc: 0.5423\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.8370 - acc: 0.5982 - val_loss: 0.9444 - val_acc: 0.5298\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.8400 - acc: 0.5967 - val_loss: 0.9365 - val_acc: 0.5415\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.8355 - acc: 0.6024 - val_loss: 0.9275 - val_acc: 0.5431\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.8372 - acc: 0.6055 - val_loss: 1.0044 - val_acc: 0.5251\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.8371 - acc: 0.6144 - val_loss: 0.9301 - val_acc: 0.5408\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.8358 - acc: 0.6102 - val_loss: 0.9644 - val_acc: 0.5361\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.8395 - acc: 0.6024 - val_loss: 0.9423 - val_acc: 0.5282\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.8353 - acc: 0.6055 - val_loss: 0.9506 - val_acc: 0.5337\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.8357 - acc: 0.6029 - val_loss: 0.9713 - val_acc: 0.5016\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.8394 - acc: 0.6029 - val_loss: 0.9504 - val_acc: 0.5423\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.8434 - acc: 0.5914 - val_loss: 0.9451 - val_acc: 0.5321\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.8375 - acc: 0.6076 - val_loss: 0.9880 - val_acc: 0.5259\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.8398 - acc: 0.6092 - val_loss: 0.9413 - val_acc: 0.5392\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.8350 - acc: 0.6108 - val_loss: 0.9557 - val_acc: 0.5219\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.8387 - acc: 0.6034 - val_loss: 0.9239 - val_acc: 0.5353\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.8388 - acc: 0.6055 - val_loss: 0.9550 - val_acc: 0.5306\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.8358 - acc: 0.6029 - val_loss: 0.9396 - val_acc: 0.5376\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.8374 - acc: 0.6055 - val_loss: 0.9394 - val_acc: 0.5462\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.8338 - acc: 0.6050 - val_loss: 0.9445 - val_acc: 0.5486\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.8385 - acc: 0.6050 - val_loss: 0.9224 - val_acc: 0.5509\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.8339 - acc: 0.6008 - val_loss: 1.0270 - val_acc: 0.5196\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.8395 - acc: 0.6014 - val_loss: 0.9495 - val_acc: 0.5368\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.8427 - acc: 0.6045 - val_loss: 0.9254 - val_acc: 0.5345\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.8391 - acc: 0.6055 - val_loss: 0.9485 - val_acc: 0.5408\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.8369 - acc: 0.6029 - val_loss: 0.9725 - val_acc: 0.5251\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.8376 - acc: 0.6019 - val_loss: 0.9780 - val_acc: 0.5180\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.8351 - acc: 0.5987 - val_loss: 0.9253 - val_acc: 0.5259\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.8404 - acc: 0.6066 - val_loss: 0.9357 - val_acc: 0.5392\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.8348 - acc: 0.6055 - val_loss: 0.9370 - val_acc: 0.5431\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.8393 - acc: 0.6066 - val_loss: 1.0240 - val_acc: 0.5251\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.8365 - acc: 0.6034 - val_loss: 0.9505 - val_acc: 0.5408\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.8353 - acc: 0.6050 - val_loss: 0.9291 - val_acc: 0.5423\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.8325 - acc: 0.6149 - val_loss: 0.9931 - val_acc: 0.4882\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.8366 - acc: 0.6061 - val_loss: 1.0262 - val_acc: 0.5219\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.8352 - acc: 0.5909 - val_loss: 0.9280 - val_acc: 0.5494\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.8377 - acc: 0.5987 - val_loss: 0.9631 - val_acc: 0.5227\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.8344 - acc: 0.5977 - val_loss: 0.9748 - val_acc: 0.5165\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.8356 - acc: 0.6045 - val_loss: 0.9845 - val_acc: 0.5024\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.8369 - acc: 0.6066 - val_loss: 0.9375 - val_acc: 0.5400\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.8335 - acc: 0.6097 - val_loss: 0.9331 - val_acc: 0.5400\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.8341 - acc: 0.6129 - val_loss: 0.9574 - val_acc: 0.5329\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.8353 - acc: 0.6024 - val_loss: 0.9310 - val_acc: 0.5408\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.8331 - acc: 0.6092 - val_loss: 0.9407 - val_acc: 0.5345\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.8322 - acc: 0.6076 - val_loss: 0.9425 - val_acc: 0.5415\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 0.8335 - acc: 0.6082 - val_loss: 0.9679 - val_acc: 0.5251\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.8365 - acc: 0.6003 - val_loss: 0.9328 - val_acc: 0.5415\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.8372 - acc: 0.6003 - val_loss: 0.9342 - val_acc: 0.5408\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.8353 - acc: 0.6029 - val_loss: 0.9646 - val_acc: 0.5392\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.8368 - acc: 0.6034 - val_loss: 0.9730 - val_acc: 0.5353\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.8377 - acc: 0.6045 - val_loss: 0.9610 - val_acc: 0.5345\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.8379 - acc: 0.6040 - val_loss: 0.9391 - val_acc: 0.5345\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.8339 - acc: 0.6050 - val_loss: 0.9926 - val_acc: 0.5321\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.8364 - acc: 0.6040 - val_loss: 0.9328 - val_acc: 0.5494\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.8319 - acc: 0.6134 - val_loss: 0.9503 - val_acc: 0.5306\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.8311 - acc: 0.6102 - val_loss: 0.9596 - val_acc: 0.5298\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.8358 - acc: 0.6024 - val_loss: 0.9520 - val_acc: 0.5384\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.8347 - acc: 0.6097 - val_loss: 0.9604 - val_acc: 0.5306\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.8373 - acc: 0.6019 - val_loss: 0.9517 - val_acc: 0.5478\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.8342 - acc: 0.6008 - val_loss: 0.9343 - val_acc: 0.5455\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.8295 - acc: 0.6082 - val_loss: 0.9422 - val_acc: 0.5274\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.8368 - acc: 0.5987 - val_loss: 0.9406 - val_acc: 0.5361\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.8352 - acc: 0.5987 - val_loss: 1.0044 - val_acc: 0.5000\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.8395 - acc: 0.6008 - val_loss: 0.9530 - val_acc: 0.5408\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.8310 - acc: 0.6019 - val_loss: 0.9679 - val_acc: 0.5078\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.8354 - acc: 0.6034 - val_loss: 0.9614 - val_acc: 0.5306\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.8368 - acc: 0.6040 - val_loss: 0.9450 - val_acc: 0.5392\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.8350 - acc: 0.6040 - val_loss: 0.9552 - val_acc: 0.5345\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 0.8363 - acc: 0.6045 - val_loss: 0.9266 - val_acc: 0.5306\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.8288 - acc: 0.6129 - val_loss: 0.9906 - val_acc: 0.5321\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.8362 - acc: 0.6019 - val_loss: 0.9772 - val_acc: 0.5071\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.8403 - acc: 0.6055 - val_loss: 0.9441 - val_acc: 0.5313\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.8357 - acc: 0.6029 - val_loss: 0.9798 - val_acc: 0.4992\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.8340 - acc: 0.6071 - val_loss: 0.9424 - val_acc: 0.5439\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.8306 - acc: 0.6123 - val_loss: 0.9437 - val_acc: 0.5298\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.8320 - acc: 0.6055 - val_loss: 0.9332 - val_acc: 0.5455\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.8322 - acc: 0.6134 - val_loss: 0.9631 - val_acc: 0.5259\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.8324 - acc: 0.6139 - val_loss: 0.9627 - val_acc: 0.5290\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.8307 - acc: 0.6024 - val_loss: 0.9432 - val_acc: 0.5408\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.8313 - acc: 0.6181 - val_loss: 0.9364 - val_acc: 0.5486\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.8305 - acc: 0.5967 - val_loss: 1.0045 - val_acc: 0.5298\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.8377 - acc: 0.5940 - val_loss: 0.9412 - val_acc: 0.5251\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.8322 - acc: 0.6066 - val_loss: 0.9754 - val_acc: 0.5384\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.8345 - acc: 0.6029 - val_loss: 0.9373 - val_acc: 0.5447\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.8303 - acc: 0.6097 - val_loss: 0.9689 - val_acc: 0.5313\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.8311 - acc: 0.6087 - val_loss: 0.9564 - val_acc: 0.5274\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.8369 - acc: 0.6066 - val_loss: 0.9596 - val_acc: 0.5329\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.8344 - acc: 0.6040 - val_loss: 0.9444 - val_acc: 0.5423\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.8301 - acc: 0.6123 - val_loss: 0.9473 - val_acc: 0.5337\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.8365 - acc: 0.5998 - val_loss: 0.9526 - val_acc: 0.5392\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.8330 - acc: 0.6050 - val_loss: 0.9394 - val_acc: 0.5447\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.8306 - acc: 0.5993 - val_loss: 1.0444 - val_acc: 0.4922\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.8335 - acc: 0.6034 - val_loss: 0.9611 - val_acc: 0.5368\n",
      "Epoch 474/1000\n",
      " - 0s - loss: 0.8338 - acc: 0.6071 - val_loss: 0.9862 - val_acc: 0.5047\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.8324 - acc: 0.6008 - val_loss: 0.9377 - val_acc: 0.5353\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.8282 - acc: 0.6034 - val_loss: 0.9432 - val_acc: 0.5384\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.8353 - acc: 0.6071 - val_loss: 0.9410 - val_acc: 0.5431\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.8301 - acc: 0.6139 - val_loss: 0.9331 - val_acc: 0.5415\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 0.8316 - acc: 0.6061 - val_loss: 0.9329 - val_acc: 0.5353\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 0.8294 - acc: 0.6123 - val_loss: 0.9447 - val_acc: 0.5447\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 0.8303 - acc: 0.6071 - val_loss: 0.9883 - val_acc: 0.5118\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 0.8379 - acc: 0.6071 - val_loss: 0.9876 - val_acc: 0.5078\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 0.8345 - acc: 0.6045 - val_loss: 1.0059 - val_acc: 0.5016\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 0.8387 - acc: 0.6040 - val_loss: 0.9318 - val_acc: 0.5353\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 0.8287 - acc: 0.6050 - val_loss: 1.0454 - val_acc: 0.4961\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 0.8345 - acc: 0.6034 - val_loss: 0.9581 - val_acc: 0.5313\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 0.8281 - acc: 0.6113 - val_loss: 0.9453 - val_acc: 0.5392\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 0.8320 - acc: 0.6092 - val_loss: 0.9688 - val_acc: 0.5188\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 0.8329 - acc: 0.6055 - val_loss: 0.9963 - val_acc: 0.4976\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 0.8329 - acc: 0.6087 - val_loss: 0.9607 - val_acc: 0.5329\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 0.8331 - acc: 0.6118 - val_loss: 1.0317 - val_acc: 0.5000\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 0.8343 - acc: 0.6034 - val_loss: 0.9488 - val_acc: 0.5431\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 0.8282 - acc: 0.6087 - val_loss: 0.9718 - val_acc: 0.5118\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 0.8382 - acc: 0.6071 - val_loss: 0.9488 - val_acc: 0.5478\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 0.8320 - acc: 0.6024 - val_loss: 0.9356 - val_acc: 0.5353\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 0.8322 - acc: 0.6097 - val_loss: 0.9653 - val_acc: 0.5321\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 0.8277 - acc: 0.6061 - val_loss: 0.9677 - val_acc: 0.5392\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 0.8320 - acc: 0.6019 - val_loss: 0.9528 - val_acc: 0.5400\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 0.8304 - acc: 0.6055 - val_loss: 0.9662 - val_acc: 0.5149\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 0.8383 - acc: 0.6113 - val_loss: 0.9723 - val_acc: 0.5353\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 0.8301 - acc: 0.6071 - val_loss: 0.9812 - val_acc: 0.4937\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 0.8350 - acc: 0.6034 - val_loss: 0.9334 - val_acc: 0.5361\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 0.8284 - acc: 0.6082 - val_loss: 1.0723 - val_acc: 0.5031\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 0.8297 - acc: 0.6113 - val_loss: 0.9387 - val_acc: 0.5470\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 0.8324 - acc: 0.6097 - val_loss: 1.0061 - val_acc: 0.5259\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 0.8330 - acc: 0.6014 - val_loss: 0.9601 - val_acc: 0.5251\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 0.8270 - acc: 0.6071 - val_loss: 0.9335 - val_acc: 0.5321\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 0.8281 - acc: 0.6008 - val_loss: 0.9512 - val_acc: 0.5219\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 0.8314 - acc: 0.6129 - val_loss: 0.9763 - val_acc: 0.5071\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 0.8321 - acc: 0.6034 - val_loss: 0.9394 - val_acc: 0.5415\n",
      "Epoch 511/1000\n",
      " - 0s - loss: 0.8294 - acc: 0.6092 - val_loss: 0.9717 - val_acc: 0.4898\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 0.8329 - acc: 0.6034 - val_loss: 0.9366 - val_acc: 0.5462\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 0.8278 - acc: 0.6092 - val_loss: 0.9667 - val_acc: 0.5313\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 0.8310 - acc: 0.6050 - val_loss: 0.9514 - val_acc: 0.5274\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 0.8261 - acc: 0.6102 - val_loss: 0.9365 - val_acc: 0.5400\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 0.8331 - acc: 0.6071 - val_loss: 0.9656 - val_acc: 0.5298\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 0.8297 - acc: 0.6034 - val_loss: 0.9396 - val_acc: 0.5282\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 0.8331 - acc: 0.6029 - val_loss: 0.9579 - val_acc: 0.5078\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 0.8331 - acc: 0.6029 - val_loss: 0.9315 - val_acc: 0.5392\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 0.8294 - acc: 0.6134 - val_loss: 0.9350 - val_acc: 0.5502\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 0.8276 - acc: 0.6196 - val_loss: 0.9640 - val_acc: 0.5086\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 0.8334 - acc: 0.6071 - val_loss: 0.9640 - val_acc: 0.5368\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 0.8284 - acc: 0.6066 - val_loss: 0.9517 - val_acc: 0.5313\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 0.8318 - acc: 0.6076 - val_loss: 0.9692 - val_acc: 0.5321\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 0.8272 - acc: 0.6113 - val_loss: 0.9799 - val_acc: 0.5353\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 0.8300 - acc: 0.6076 - val_loss: 0.9778 - val_acc: 0.5204\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 0.8271 - acc: 0.6050 - val_loss: 1.0355 - val_acc: 0.5000\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 0.8297 - acc: 0.6008 - val_loss: 0.9982 - val_acc: 0.5345\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 0.8304 - acc: 0.5982 - val_loss: 0.9592 - val_acc: 0.5400\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 0.8288 - acc: 0.6045 - val_loss: 0.9576 - val_acc: 0.5392\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 0.8279 - acc: 0.6045 - val_loss: 0.9486 - val_acc: 0.5447\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 0.8287 - acc: 0.6108 - val_loss: 0.9410 - val_acc: 0.5439\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 0.8307 - acc: 0.6066 - val_loss: 0.9907 - val_acc: 0.5212\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 0.8283 - acc: 0.6097 - val_loss: 0.9767 - val_acc: 0.5031\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 0.8251 - acc: 0.6019 - val_loss: 0.9499 - val_acc: 0.5462\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 0.8307 - acc: 0.6066 - val_loss: 0.9379 - val_acc: 0.5392\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 0.8303 - acc: 0.6092 - val_loss: 0.9319 - val_acc: 0.5455\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 0.8256 - acc: 0.6181 - val_loss: 0.9375 - val_acc: 0.5368\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 0.8281 - acc: 0.6108 - val_loss: 0.9951 - val_acc: 0.5282\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 0.8308 - acc: 0.6082 - val_loss: 0.9560 - val_acc: 0.5415\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 0.8248 - acc: 0.6092 - val_loss: 0.9892 - val_acc: 0.5110\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 0.8355 - acc: 0.5951 - val_loss: 0.9603 - val_acc: 0.5274\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 0.8280 - acc: 0.6071 - val_loss: 0.9446 - val_acc: 0.5470\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 0.8290 - acc: 0.6061 - val_loss: 0.9570 - val_acc: 0.5345\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6134 - val_loss: 0.9505 - val_acc: 0.5431\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 0.8287 - acc: 0.6102 - val_loss: 1.0108 - val_acc: 0.5000\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 0.8329 - acc: 0.5998 - val_loss: 0.9526 - val_acc: 0.5384\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 0.8267 - acc: 0.6071 - val_loss: 0.9795 - val_acc: 0.5110\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 0.8269 - acc: 0.6144 - val_loss: 0.9701 - val_acc: 0.5290\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 0.8318 - acc: 0.5961 - val_loss: 0.9324 - val_acc: 0.5423\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 0.8264 - acc: 0.6024 - val_loss: 0.9858 - val_acc: 0.5400\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 0.8292 - acc: 0.5987 - val_loss: 0.9682 - val_acc: 0.5024\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 0.8290 - acc: 0.6082 - val_loss: 0.9896 - val_acc: 0.5251\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 0.8289 - acc: 0.5940 - val_loss: 0.9506 - val_acc: 0.5447\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 0.8294 - acc: 0.6050 - val_loss: 0.9833 - val_acc: 0.5016\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 0.8334 - acc: 0.6003 - val_loss: 1.0064 - val_acc: 0.5321\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 0.8305 - acc: 0.6071 - val_loss: 0.9353 - val_acc: 0.5470\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 0.8245 - acc: 0.6061 - val_loss: 0.9575 - val_acc: 0.5384\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 0.8297 - acc: 0.6113 - val_loss: 0.9442 - val_acc: 0.5251\n",
      "Epoch 560/1000\n",
      " - 0s - loss: 0.8274 - acc: 0.6066 - val_loss: 0.9888 - val_acc: 0.5298\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 0.8260 - acc: 0.6076 - val_loss: 1.1516 - val_acc: 0.5172\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 0.8372 - acc: 0.6092 - val_loss: 0.9396 - val_acc: 0.5321\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 0.8260 - acc: 0.6129 - val_loss: 0.9433 - val_acc: 0.5462\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6097 - val_loss: 1.0627 - val_acc: 0.4514\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 0.8368 - acc: 0.6034 - val_loss: 0.9364 - val_acc: 0.5368\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 0.8287 - acc: 0.6118 - val_loss: 0.9488 - val_acc: 0.5392\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 0.8254 - acc: 0.6134 - val_loss: 1.0405 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 568/1000\n",
      " - 0s - loss: 0.8302 - acc: 0.6055 - val_loss: 0.9976 - val_acc: 0.5157\n",
      "Epoch 569/1000\n",
      " - 0s - loss: 0.8327 - acc: 0.6061 - val_loss: 0.9367 - val_acc: 0.5306\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6139 - val_loss: 0.9536 - val_acc: 0.5423\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 0.8257 - acc: 0.6003 - val_loss: 1.0421 - val_acc: 0.4953\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 0.8271 - acc: 0.6040 - val_loss: 0.9987 - val_acc: 0.5008\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 0.8339 - acc: 0.5951 - val_loss: 1.0104 - val_acc: 0.5016\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 0.8344 - acc: 0.6029 - val_loss: 0.9593 - val_acc: 0.5353\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 0.8256 - acc: 0.6149 - val_loss: 1.0055 - val_acc: 0.5298\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6082 - val_loss: 0.9399 - val_acc: 0.5306\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 0.8266 - acc: 0.6050 - val_loss: 1.0725 - val_acc: 0.5251\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 0.8352 - acc: 0.5998 - val_loss: 1.0189 - val_acc: 0.5306\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 0.8285 - acc: 0.6061 - val_loss: 0.9782 - val_acc: 0.5298\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 0.8237 - acc: 0.6123 - val_loss: 0.9547 - val_acc: 0.5329\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 0.8260 - acc: 0.6134 - val_loss: 0.9437 - val_acc: 0.5376\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 0.8250 - acc: 0.6108 - val_loss: 0.9875 - val_acc: 0.5235\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 0.8300 - acc: 0.6087 - val_loss: 0.9600 - val_acc: 0.5063\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 0.8271 - acc: 0.6087 - val_loss: 0.9483 - val_acc: 0.5431\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 0.8270 - acc: 0.6082 - val_loss: 1.0485 - val_acc: 0.4914\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 0.8335 - acc: 0.6076 - val_loss: 0.9578 - val_acc: 0.5400\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 0.8245 - acc: 0.6050 - val_loss: 0.9802 - val_acc: 0.5047\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 0.8227 - acc: 0.6144 - val_loss: 0.9392 - val_acc: 0.5423\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 0.8257 - acc: 0.6134 - val_loss: 0.9450 - val_acc: 0.5486\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 0.8299 - acc: 0.6097 - val_loss: 0.9724 - val_acc: 0.5086\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 0.8243 - acc: 0.6082 - val_loss: 0.9984 - val_acc: 0.5321\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 0.8245 - acc: 0.6123 - val_loss: 1.0035 - val_acc: 0.5313\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 0.8295 - acc: 0.6087 - val_loss: 0.9504 - val_acc: 0.5290\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 0.8305 - acc: 0.6087 - val_loss: 0.9610 - val_acc: 0.5423\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 0.8298 - acc: 0.6024 - val_loss: 0.9898 - val_acc: 0.5157\n",
      "Epoch 596/1000\n",
      " - 0s - loss: 0.8250 - acc: 0.5967 - val_loss: 0.9493 - val_acc: 0.5329\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 0.8280 - acc: 0.6050 - val_loss: 1.0090 - val_acc: 0.5188\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 0.8243 - acc: 0.6087 - val_loss: 0.9428 - val_acc: 0.5243\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 0.8264 - acc: 0.6118 - val_loss: 0.9347 - val_acc: 0.5447\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 0.8234 - acc: 0.6092 - val_loss: 0.9728 - val_acc: 0.5400\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 0.8257 - acc: 0.6082 - val_loss: 0.9528 - val_acc: 0.5368\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 0.8233 - acc: 0.6144 - val_loss: 0.9538 - val_acc: 0.5423\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 0.8208 - acc: 0.6102 - val_loss: 1.0442 - val_acc: 0.5086\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 0.8279 - acc: 0.6087 - val_loss: 1.0272 - val_acc: 0.5368\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 0.8339 - acc: 0.6066 - val_loss: 0.9625 - val_acc: 0.5423\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 0.8233 - acc: 0.6155 - val_loss: 0.9786 - val_acc: 0.5031\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 0.8282 - acc: 0.6092 - val_loss: 0.9588 - val_acc: 0.5306\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 0.8262 - acc: 0.6008 - val_loss: 1.0045 - val_acc: 0.4890\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 0.8271 - acc: 0.6061 - val_loss: 0.9584 - val_acc: 0.5400\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 0.8238 - acc: 0.6113 - val_loss: 0.9544 - val_acc: 0.5337\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 0.8270 - acc: 0.6034 - val_loss: 1.0025 - val_acc: 0.5204\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6170 - val_loss: 0.9484 - val_acc: 0.5478\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 0.8267 - acc: 0.6113 - val_loss: 0.9551 - val_acc: 0.5494\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 0.8262 - acc: 0.6134 - val_loss: 0.9601 - val_acc: 0.5321\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 0.8209 - acc: 0.6155 - val_loss: 0.9982 - val_acc: 0.5094\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 0.8283 - acc: 0.6123 - val_loss: 0.9965 - val_acc: 0.5282\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 0.8263 - acc: 0.6087 - val_loss: 0.9606 - val_acc: 0.5313\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 0.8243 - acc: 0.6155 - val_loss: 1.0000 - val_acc: 0.5306\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6087 - val_loss: 0.9574 - val_acc: 0.5415\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 0.8213 - acc: 0.6134 - val_loss: 0.9628 - val_acc: 0.5165\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 0.8231 - acc: 0.6113 - val_loss: 1.0469 - val_acc: 0.5031\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 0.8262 - acc: 0.6066 - val_loss: 0.9560 - val_acc: 0.5549\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 0.8221 - acc: 0.6066 - val_loss: 0.9799 - val_acc: 0.5415\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 0.8254 - acc: 0.6055 - val_loss: 0.9735 - val_acc: 0.5290\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6061 - val_loss: 1.0189 - val_acc: 0.5063\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 0.8239 - acc: 0.6087 - val_loss: 0.9503 - val_acc: 0.5384\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 0.8225 - acc: 0.6102 - val_loss: 0.9570 - val_acc: 0.5392\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6144 - val_loss: 0.9434 - val_acc: 0.5259\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 0.8236 - acc: 0.6092 - val_loss: 0.9582 - val_acc: 0.5329\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 0.8244 - acc: 0.6082 - val_loss: 0.9827 - val_acc: 0.5274\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 0.8213 - acc: 0.6144 - val_loss: 0.9959 - val_acc: 0.5282\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 0.8304 - acc: 0.5998 - val_loss: 0.9467 - val_acc: 0.5368\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 0.8238 - acc: 0.6061 - val_loss: 0.9595 - val_acc: 0.5439\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 0.8215 - acc: 0.6097 - val_loss: 0.9553 - val_acc: 0.5266\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 0.8301 - acc: 0.6129 - val_loss: 0.9866 - val_acc: 0.5243\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 0.8227 - acc: 0.6087 - val_loss: 1.0075 - val_acc: 0.5313\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 0.8250 - acc: 0.6129 - val_loss: 0.9789 - val_acc: 0.5274\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6092 - val_loss: 0.9433 - val_acc: 0.5392\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 0.8246 - acc: 0.6071 - val_loss: 0.9523 - val_acc: 0.5274\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 0.8202 - acc: 0.6129 - val_loss: 0.9555 - val_acc: 0.5227\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 0.8217 - acc: 0.6076 - val_loss: 0.9803 - val_acc: 0.5204\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 0.8240 - acc: 0.6144 - val_loss: 0.9628 - val_acc: 0.5313\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 0.8201 - acc: 0.6202 - val_loss: 0.9681 - val_acc: 0.5259\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 0.8252 - acc: 0.6071 - val_loss: 0.9471 - val_acc: 0.5329\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6123 - val_loss: 1.0657 - val_acc: 0.4945\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 0.8277 - acc: 0.6144 - val_loss: 0.9684 - val_acc: 0.5298\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 0.8251 - acc: 0.6076 - val_loss: 1.0317 - val_acc: 0.5298\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 0.8212 - acc: 0.6149 - val_loss: 0.9565 - val_acc: 0.5392\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 0.8201 - acc: 0.6134 - val_loss: 0.9779 - val_acc: 0.5157\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 0.8268 - acc: 0.6181 - val_loss: 1.0897 - val_acc: 0.4663\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 0.8387 - acc: 0.6019 - val_loss: 0.9621 - val_acc: 0.5337\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 0.8213 - acc: 0.6087 - val_loss: 0.9513 - val_acc: 0.5415\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 0.8209 - acc: 0.6087 - val_loss: 0.9940 - val_acc: 0.5368\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 0.8234 - acc: 0.6108 - val_loss: 0.9578 - val_acc: 0.5345\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 0.8228 - acc: 0.6165 - val_loss: 0.9757 - val_acc: 0.5321\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 0.8239 - acc: 0.6066 - val_loss: 0.9894 - val_acc: 0.5133\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 0.8196 - acc: 0.6087 - val_loss: 0.9698 - val_acc: 0.5118\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 0.8244 - acc: 0.6029 - val_loss: 0.9697 - val_acc: 0.5321\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 0.8219 - acc: 0.6087 - val_loss: 1.0138 - val_acc: 0.5298\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 0.8196 - acc: 0.6139 - val_loss: 0.9527 - val_acc: 0.5353\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 0.8216 - acc: 0.6181 - val_loss: 0.9764 - val_acc: 0.5392\n",
      "Epoch 662/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.8221 - acc: 0.6108 - val_loss: 0.9463 - val_acc: 0.5353\n",
      "Epoch 663/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6118 - val_loss: 1.0289 - val_acc: 0.4969\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 0.8316 - acc: 0.6118 - val_loss: 0.9831 - val_acc: 0.5361\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 0.8210 - acc: 0.6087 - val_loss: 0.9954 - val_acc: 0.5274\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 0.8246 - acc: 0.6014 - val_loss: 0.9646 - val_acc: 0.5251\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 0.8215 - acc: 0.6108 - val_loss: 0.9611 - val_acc: 0.5384\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 0.8158 - acc: 0.6118 - val_loss: 0.9566 - val_acc: 0.5149\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 0.8197 - acc: 0.6144 - val_loss: 0.9650 - val_acc: 0.5415\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 0.8215 - acc: 0.6108 - val_loss: 0.9655 - val_acc: 0.5337\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 0.8177 - acc: 0.6055 - val_loss: 0.9926 - val_acc: 0.5031\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 0.8234 - acc: 0.6097 - val_loss: 0.9623 - val_acc: 0.5313\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 0.8180 - acc: 0.6223 - val_loss: 1.0273 - val_acc: 0.5039\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 0.8210 - acc: 0.6076 - val_loss: 0.9633 - val_acc: 0.5227\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 0.8211 - acc: 0.6102 - val_loss: 0.9985 - val_acc: 0.5298\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 0.8213 - acc: 0.6123 - val_loss: 0.9960 - val_acc: 0.4929\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 0.8266 - acc: 0.6040 - val_loss: 0.9650 - val_acc: 0.4984\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6071 - val_loss: 0.9425 - val_acc: 0.5400\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 0.8216 - acc: 0.6082 - val_loss: 0.9677 - val_acc: 0.5376\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 0.8170 - acc: 0.6108 - val_loss: 0.9635 - val_acc: 0.5290\n",
      "Epoch 681/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6029 - val_loss: 0.9874 - val_acc: 0.5345\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6076 - val_loss: 0.9752 - val_acc: 0.5251\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 0.8227 - acc: 0.6092 - val_loss: 0.9912 - val_acc: 0.5282\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 0.8195 - acc: 0.6108 - val_loss: 1.0984 - val_acc: 0.4992\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 0.8223 - acc: 0.6102 - val_loss: 0.9870 - val_acc: 0.5400\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 0.8191 - acc: 0.6092 - val_loss: 0.9484 - val_acc: 0.5486\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 0.8184 - acc: 0.6196 - val_loss: 1.0002 - val_acc: 0.5329\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6118 - val_loss: 0.9766 - val_acc: 0.5219\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 0.8207 - acc: 0.6118 - val_loss: 0.9728 - val_acc: 0.5455\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 0.8272 - acc: 0.6034 - val_loss: 0.9808 - val_acc: 0.5408\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 0.8202 - acc: 0.6144 - val_loss: 0.9705 - val_acc: 0.5384\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 0.8188 - acc: 0.6108 - val_loss: 0.9806 - val_acc: 0.5282\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 0.8244 - acc: 0.6076 - val_loss: 0.9829 - val_acc: 0.5298\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 0.8217 - acc: 0.6066 - val_loss: 1.0250 - val_acc: 0.5071\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 0.8166 - acc: 0.6139 - val_loss: 0.9672 - val_acc: 0.5282\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 0.8244 - acc: 0.6102 - val_loss: 1.0046 - val_acc: 0.5423\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 0.8211 - acc: 0.6076 - val_loss: 0.9650 - val_acc: 0.5157\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 0.8233 - acc: 0.6196 - val_loss: 0.9445 - val_acc: 0.5345\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 0.8178 - acc: 0.6181 - val_loss: 0.9597 - val_acc: 0.5368\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 0.8194 - acc: 0.6092 - val_loss: 0.9748 - val_acc: 0.5235\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 0.8219 - acc: 0.6139 - val_loss: 0.9550 - val_acc: 0.5423\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 0.8225 - acc: 0.6149 - val_loss: 0.9619 - val_acc: 0.5329\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 0.8164 - acc: 0.6108 - val_loss: 0.9805 - val_acc: 0.5368\n",
      "Epoch 704/1000\n",
      " - 0s - loss: 0.8160 - acc: 0.6196 - val_loss: 0.9567 - val_acc: 0.5266\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 0.8161 - acc: 0.6108 - val_loss: 0.9469 - val_acc: 0.5329\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 0.8162 - acc: 0.6113 - val_loss: 0.9557 - val_acc: 0.5447\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 0.8200 - acc: 0.6108 - val_loss: 0.9942 - val_acc: 0.4953\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 0.8214 - acc: 0.6139 - val_loss: 0.9909 - val_acc: 0.5329\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 0.8184 - acc: 0.6050 - val_loss: 1.0600 - val_acc: 0.5000\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 0.8249 - acc: 0.6102 - val_loss: 0.9957 - val_acc: 0.5086\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 0.8160 - acc: 0.6191 - val_loss: 0.9526 - val_acc: 0.5431\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 0.8228 - acc: 0.6144 - val_loss: 0.9592 - val_acc: 0.5329\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 0.8180 - acc: 0.6108 - val_loss: 0.9636 - val_acc: 0.5321\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 0.8170 - acc: 0.6076 - val_loss: 1.0188 - val_acc: 0.5196\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 0.8218 - acc: 0.6134 - val_loss: 0.9479 - val_acc: 0.5400\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 0.8159 - acc: 0.6061 - val_loss: 0.9796 - val_acc: 0.5337\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 0.8185 - acc: 0.6061 - val_loss: 0.9852 - val_acc: 0.5282\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 0.8220 - acc: 0.6139 - val_loss: 0.9837 - val_acc: 0.5047\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 0.8194 - acc: 0.6123 - val_loss: 0.9946 - val_acc: 0.5345\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 0.8182 - acc: 0.6223 - val_loss: 0.9896 - val_acc: 0.5329\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 0.8214 - acc: 0.6108 - val_loss: 0.9588 - val_acc: 0.5204\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 0.8228 - acc: 0.6118 - val_loss: 0.9668 - val_acc: 0.5384\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 0.8172 - acc: 0.6160 - val_loss: 1.0003 - val_acc: 0.5063\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 0.8212 - acc: 0.6055 - val_loss: 0.9985 - val_acc: 0.5290\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 0.8213 - acc: 0.6118 - val_loss: 0.9693 - val_acc: 0.5259\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 0.8157 - acc: 0.6191 - val_loss: 1.0225 - val_acc: 0.4898\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 0.8207 - acc: 0.6207 - val_loss: 0.9859 - val_acc: 0.5212\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 0.8206 - acc: 0.6144 - val_loss: 0.9631 - val_acc: 0.5306\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 0.8189 - acc: 0.6160 - val_loss: 1.0181 - val_acc: 0.5219\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 0.8188 - acc: 0.6113 - val_loss: 1.0072 - val_acc: 0.5259\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 0.8161 - acc: 0.6123 - val_loss: 0.9968 - val_acc: 0.5024\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 0.8237 - acc: 0.6082 - val_loss: 1.0044 - val_acc: 0.5345\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 0.8216 - acc: 0.6202 - val_loss: 0.9932 - val_acc: 0.5274\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 0.8201 - acc: 0.6108 - val_loss: 0.9495 - val_acc: 0.5478\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 0.8204 - acc: 0.6102 - val_loss: 1.0380 - val_acc: 0.5094\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 0.8153 - acc: 0.6181 - val_loss: 0.9591 - val_acc: 0.5321\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 0.8227 - acc: 0.6108 - val_loss: 0.9543 - val_acc: 0.5470\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 0.8156 - acc: 0.6149 - val_loss: 0.9710 - val_acc: 0.5204\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 0.8190 - acc: 0.6118 - val_loss: 1.0259 - val_acc: 0.4922\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 0.8410 - acc: 0.6003 - val_loss: 1.1046 - val_acc: 0.4945\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 0.8306 - acc: 0.6076 - val_loss: 1.0159 - val_acc: 0.4773\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 0.8216 - acc: 0.6092 - val_loss: 0.9637 - val_acc: 0.5384\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 0.8194 - acc: 0.6149 - val_loss: 1.0175 - val_acc: 0.5227\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 0.8224 - acc: 0.6139 - val_loss: 0.9598 - val_acc: 0.5125\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 0.8171 - acc: 0.6061 - val_loss: 0.9812 - val_acc: 0.5321\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 0.8184 - acc: 0.6160 - val_loss: 0.9921 - val_acc: 0.5290\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6217 - val_loss: 0.9823 - val_acc: 0.5313\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 0.8199 - acc: 0.6092 - val_loss: 0.9663 - val_acc: 0.5196\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 0.8176 - acc: 0.6092 - val_loss: 0.9844 - val_acc: 0.5353\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 0.8226 - acc: 0.6092 - val_loss: 1.0385 - val_acc: 0.5047\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 0.8212 - acc: 0.6118 - val_loss: 1.0322 - val_acc: 0.5313\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6149 - val_loss: 0.9481 - val_acc: 0.5306\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 0.8130 - acc: 0.6170 - val_loss: 0.9687 - val_acc: 0.5188\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6113 - val_loss: 0.9900 - val_acc: 0.5180\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 0.8221 - acc: 0.6134 - val_loss: 0.9970 - val_acc: 0.5110\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 0.8226 - acc: 0.6202 - val_loss: 0.9646 - val_acc: 0.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 757/1000\n",
      " - 0s - loss: 0.8153 - acc: 0.6191 - val_loss: 1.0181 - val_acc: 0.5039\n",
      "Epoch 758/1000\n",
      " - 0s - loss: 0.8202 - acc: 0.6102 - val_loss: 1.0184 - val_acc: 0.5086\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 0.8146 - acc: 0.6196 - val_loss: 1.0284 - val_acc: 0.5047\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 0.8192 - acc: 0.6097 - val_loss: 0.9581 - val_acc: 0.5329\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 0.8171 - acc: 0.6108 - val_loss: 0.9589 - val_acc: 0.5259\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 0.8164 - acc: 0.6160 - val_loss: 0.9660 - val_acc: 0.5368\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 0.8151 - acc: 0.6123 - val_loss: 0.9977 - val_acc: 0.5243\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 0.8149 - acc: 0.6149 - val_loss: 0.9707 - val_acc: 0.5055\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 0.8253 - acc: 0.6113 - val_loss: 1.0034 - val_acc: 0.5415\n",
      "Epoch 766/1000\n",
      " - 0s - loss: 0.8204 - acc: 0.6129 - val_loss: 0.9707 - val_acc: 0.5055\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 0.8199 - acc: 0.6165 - val_loss: 1.0513 - val_acc: 0.5039\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 0.8176 - acc: 0.6202 - val_loss: 0.9844 - val_acc: 0.5227\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 0.8193 - acc: 0.6129 - val_loss: 0.9658 - val_acc: 0.5376\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 0.8093 - acc: 0.6217 - val_loss: 0.9747 - val_acc: 0.5337\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 0.8172 - acc: 0.6123 - val_loss: 1.0153 - val_acc: 0.5298\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 0.8170 - acc: 0.6118 - val_loss: 0.9845 - val_acc: 0.5306\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 0.8199 - acc: 0.6108 - val_loss: 1.0031 - val_acc: 0.5133\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 0.8138 - acc: 0.6165 - val_loss: 1.0688 - val_acc: 0.5016\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 0.8233 - acc: 0.6134 - val_loss: 0.9659 - val_acc: 0.5329\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 0.8105 - acc: 0.6155 - val_loss: 0.9993 - val_acc: 0.5196\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 0.8191 - acc: 0.6118 - val_loss: 0.9588 - val_acc: 0.5509\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 0.8211 - acc: 0.6092 - val_loss: 1.0451 - val_acc: 0.5071\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 0.8207 - acc: 0.6082 - val_loss: 1.0559 - val_acc: 0.4702\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 0.8215 - acc: 0.6160 - val_loss: 1.0887 - val_acc: 0.5243\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 0.8200 - acc: 0.6144 - val_loss: 0.9709 - val_acc: 0.5345\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 0.8119 - acc: 0.6160 - val_loss: 0.9610 - val_acc: 0.5337\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 0.8163 - acc: 0.6191 - val_loss: 0.9808 - val_acc: 0.5172\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6165 - val_loss: 0.9573 - val_acc: 0.5384\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 0.8129 - acc: 0.6170 - val_loss: 0.9973 - val_acc: 0.5298\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 0.8157 - acc: 0.6170 - val_loss: 0.9754 - val_acc: 0.5298\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 0.8169 - acc: 0.6165 - val_loss: 0.9581 - val_acc: 0.5400\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 0.8136 - acc: 0.6108 - val_loss: 0.9945 - val_acc: 0.5321\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 0.8133 - acc: 0.6155 - val_loss: 1.0046 - val_acc: 0.5298\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 0.8120 - acc: 0.6191 - val_loss: 0.9546 - val_acc: 0.5415\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 0.8153 - acc: 0.6207 - val_loss: 1.0547 - val_acc: 0.5055\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 0.8187 - acc: 0.6144 - val_loss: 0.9645 - val_acc: 0.5431\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 0.8142 - acc: 0.6181 - val_loss: 0.9546 - val_acc: 0.5368\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6139 - val_loss: 0.9550 - val_acc: 0.5415\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 0.8104 - acc: 0.6092 - val_loss: 1.0809 - val_acc: 0.4851\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 0.8278 - acc: 0.6045 - val_loss: 0.9774 - val_acc: 0.5329\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 0.8117 - acc: 0.6196 - val_loss: 0.9687 - val_acc: 0.5353\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 0.8166 - acc: 0.6123 - val_loss: 1.0449 - val_acc: 0.5000\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 0.8164 - acc: 0.6087 - val_loss: 0.9549 - val_acc: 0.5290\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 0.8135 - acc: 0.6170 - val_loss: 0.9885 - val_acc: 0.5125\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 0.8163 - acc: 0.6139 - val_loss: 0.9607 - val_acc: 0.5306\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 0.8133 - acc: 0.6092 - val_loss: 1.0081 - val_acc: 0.5102\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 0.8190 - acc: 0.6118 - val_loss: 0.9610 - val_acc: 0.5462\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 0.8119 - acc: 0.6181 - val_loss: 0.9545 - val_acc: 0.5384\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 0.8194 - acc: 0.6139 - val_loss: 0.9654 - val_acc: 0.5337\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 0.8158 - acc: 0.6134 - val_loss: 0.9805 - val_acc: 0.5298\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 0.8126 - acc: 0.6196 - val_loss: 0.9917 - val_acc: 0.5321\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 0.8130 - acc: 0.6228 - val_loss: 0.9810 - val_acc: 0.5165\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 0.8233 - acc: 0.6082 - val_loss: 0.9888 - val_acc: 0.5102\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 0.8157 - acc: 0.6045 - val_loss: 0.9732 - val_acc: 0.5361\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 0.8148 - acc: 0.6181 - val_loss: 1.0263 - val_acc: 0.5039\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 0.8141 - acc: 0.6092 - val_loss: 0.9661 - val_acc: 0.5172\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 0.8088 - acc: 0.6212 - val_loss: 1.0044 - val_acc: 0.5133\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 0.8266 - acc: 0.6102 - val_loss: 0.9694 - val_acc: 0.5219\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 0.8161 - acc: 0.6134 - val_loss: 0.9588 - val_acc: 0.5447\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 0.8112 - acc: 0.6186 - val_loss: 1.0754 - val_acc: 0.5000\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 0.8162 - acc: 0.6123 - val_loss: 0.9828 - val_acc: 0.5400\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 0.8164 - acc: 0.6144 - val_loss: 0.9906 - val_acc: 0.5361\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 0.8146 - acc: 0.6170 - val_loss: 0.9932 - val_acc: 0.5282\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 0.8176 - acc: 0.6212 - val_loss: 0.9512 - val_acc: 0.5392\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 0.8138 - acc: 0.6076 - val_loss: 1.0036 - val_acc: 0.5266\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6055 - val_loss: 1.0825 - val_acc: 0.5235\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 0.8148 - acc: 0.6186 - val_loss: 0.9844 - val_acc: 0.5243\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 0.8150 - acc: 0.6165 - val_loss: 0.9764 - val_acc: 0.5204\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6144 - val_loss: 0.9635 - val_acc: 0.5290\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 0.8177 - acc: 0.6092 - val_loss: 0.9824 - val_acc: 0.5353\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 0.8138 - acc: 0.6160 - val_loss: 1.0110 - val_acc: 0.5321\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 0.8173 - acc: 0.6139 - val_loss: 1.0003 - val_acc: 0.5274\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 0.8137 - acc: 0.6139 - val_loss: 0.9974 - val_acc: 0.5055\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 0.8172 - acc: 0.6108 - val_loss: 0.9809 - val_acc: 0.5313\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 0.8199 - acc: 0.6165 - val_loss: 0.9981 - val_acc: 0.5251\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 0.8131 - acc: 0.6165 - val_loss: 0.9943 - val_acc: 0.5298\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 0.8137 - acc: 0.6123 - val_loss: 1.0614 - val_acc: 0.4906\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 0.8225 - acc: 0.6019 - val_loss: 0.9663 - val_acc: 0.5321\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 0.8108 - acc: 0.6217 - val_loss: 0.9576 - val_acc: 0.5298\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 0.8075 - acc: 0.6223 - val_loss: 1.0004 - val_acc: 0.5235\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6097 - val_loss: 0.9570 - val_acc: 0.5290\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 0.8122 - acc: 0.6191 - val_loss: 0.9601 - val_acc: 0.5368\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 0.8123 - acc: 0.6181 - val_loss: 0.9746 - val_acc: 0.5235\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 0.8103 - acc: 0.6181 - val_loss: 1.0030 - val_acc: 0.5376\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 0.8106 - acc: 0.6129 - val_loss: 1.0114 - val_acc: 0.5384\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6118 - val_loss: 0.9743 - val_acc: 0.5408\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 0.8115 - acc: 0.6118 - val_loss: 1.0473 - val_acc: 0.5039\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 0.8173 - acc: 0.6139 - val_loss: 0.9888 - val_acc: 0.5353\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 0.8142 - acc: 0.6097 - val_loss: 0.9641 - val_acc: 0.5259\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 0.8146 - acc: 0.6228 - val_loss: 0.9825 - val_acc: 0.5306\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 0.8156 - acc: 0.6097 - val_loss: 1.0113 - val_acc: 0.5266\n",
      "Epoch 848/1000\n",
      " - 0s - loss: 0.8147 - acc: 0.6176 - val_loss: 0.9820 - val_acc: 0.5329\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 0.8169 - acc: 0.6170 - val_loss: 0.9568 - val_acc: 0.5400\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 0.8130 - acc: 0.6129 - val_loss: 0.9464 - val_acc: 0.5313\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.8171 - acc: 0.6055 - val_loss: 0.9795 - val_acc: 0.5298\n",
      "Epoch 852/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6134 - val_loss: 0.9843 - val_acc: 0.5337\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 0.8128 - acc: 0.6071 - val_loss: 1.0003 - val_acc: 0.5455\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 0.8168 - acc: 0.6134 - val_loss: 1.0173 - val_acc: 0.5392\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 0.8166 - acc: 0.6118 - val_loss: 0.9601 - val_acc: 0.5408\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 0.8090 - acc: 0.6165 - val_loss: 0.9776 - val_acc: 0.5321\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 0.8096 - acc: 0.6191 - val_loss: 1.0096 - val_acc: 0.4992\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 0.8153 - acc: 0.6118 - val_loss: 0.9594 - val_acc: 0.5337\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 0.8135 - acc: 0.6129 - val_loss: 1.0358 - val_acc: 0.5368\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 0.8155 - acc: 0.6113 - val_loss: 0.9633 - val_acc: 0.5290\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 0.8108 - acc: 0.6223 - val_loss: 1.0000 - val_acc: 0.5133\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 0.8069 - acc: 0.6196 - val_loss: 0.9499 - val_acc: 0.5408\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 0.8119 - acc: 0.6076 - val_loss: 0.9771 - val_acc: 0.5345\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 0.8106 - acc: 0.6144 - val_loss: 0.9632 - val_acc: 0.5353\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 0.8103 - acc: 0.6217 - val_loss: 0.9709 - val_acc: 0.5337\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 0.8163 - acc: 0.6097 - val_loss: 1.0387 - val_acc: 0.5298\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 0.8110 - acc: 0.6196 - val_loss: 1.0587 - val_acc: 0.5008\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 0.8179 - acc: 0.6108 - val_loss: 1.0469 - val_acc: 0.4945\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 0.8235 - acc: 0.6129 - val_loss: 0.9680 - val_acc: 0.5172\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 0.8101 - acc: 0.6170 - val_loss: 0.9561 - val_acc: 0.5392\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 0.8101 - acc: 0.6223 - val_loss: 1.0575 - val_acc: 0.5024\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 0.8150 - acc: 0.6097 - val_loss: 1.0197 - val_acc: 0.5204\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 0.8154 - acc: 0.6176 - val_loss: 0.9608 - val_acc: 0.5439\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 0.8096 - acc: 0.6238 - val_loss: 0.9825 - val_acc: 0.5408\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 0.8099 - acc: 0.6144 - val_loss: 0.9822 - val_acc: 0.5321\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 0.8137 - acc: 0.6160 - val_loss: 0.9650 - val_acc: 0.5313\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 0.8109 - acc: 0.6202 - val_loss: 1.0201 - val_acc: 0.5251\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 0.8190 - acc: 0.6129 - val_loss: 0.9614 - val_acc: 0.5384\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 0.8053 - acc: 0.6191 - val_loss: 0.9806 - val_acc: 0.5290\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 0.8121 - acc: 0.6118 - val_loss: 1.0624 - val_acc: 0.5039\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 0.8139 - acc: 0.6134 - val_loss: 0.9898 - val_acc: 0.5392\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 0.8099 - acc: 0.6207 - val_loss: 1.0069 - val_acc: 0.5031\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 0.8143 - acc: 0.6123 - val_loss: 0.9565 - val_acc: 0.5274\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 0.8084 - acc: 0.6165 - val_loss: 1.0543 - val_acc: 0.4718\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 0.8203 - acc: 0.6113 - val_loss: 0.9874 - val_acc: 0.5259\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 0.8144 - acc: 0.6040 - val_loss: 0.9961 - val_acc: 0.5298\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 0.8121 - acc: 0.6186 - val_loss: 1.0483 - val_acc: 0.5078\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 0.8109 - acc: 0.6217 - val_loss: 0.9719 - val_acc: 0.5321\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 0.8110 - acc: 0.6108 - val_loss: 0.9722 - val_acc: 0.5447\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 0.8190 - acc: 0.5987 - val_loss: 1.0060 - val_acc: 0.5086\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 0.8089 - acc: 0.6270 - val_loss: 0.9788 - val_acc: 0.5055\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 0.8136 - acc: 0.6196 - val_loss: 0.9717 - val_acc: 0.5431\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 0.8081 - acc: 0.6118 - val_loss: 1.0137 - val_acc: 0.4906\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 0.8114 - acc: 0.6191 - val_loss: 0.9717 - val_acc: 0.5313\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 0.8076 - acc: 0.6134 - val_loss: 1.0769 - val_acc: 0.5188\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 0.8138 - acc: 0.6149 - val_loss: 1.0396 - val_acc: 0.5298\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 0.8132 - acc: 0.6170 - val_loss: 1.0615 - val_acc: 0.4726\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 0.8169 - acc: 0.6087 - val_loss: 0.9536 - val_acc: 0.5298\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 0.8099 - acc: 0.6270 - val_loss: 0.9709 - val_acc: 0.5361\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 0.8084 - acc: 0.6176 - val_loss: 1.0341 - val_acc: 0.5251\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 0.8138 - acc: 0.6118 - val_loss: 1.0550 - val_acc: 0.5180\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 0.8129 - acc: 0.6108 - val_loss: 0.9734 - val_acc: 0.5337\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 0.8092 - acc: 0.6176 - val_loss: 0.9944 - val_acc: 0.5282\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 0.8070 - acc: 0.6165 - val_loss: 1.0113 - val_acc: 0.5008\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 0.8146 - acc: 0.6129 - val_loss: 0.9850 - val_acc: 0.5118\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 0.8136 - acc: 0.6097 - val_loss: 0.9693 - val_acc: 0.5251\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 0.8142 - acc: 0.6160 - val_loss: 1.0159 - val_acc: 0.5000\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 0.8131 - acc: 0.6160 - val_loss: 0.9811 - val_acc: 0.5290\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 0.8131 - acc: 0.6217 - val_loss: 0.9801 - val_acc: 0.5282\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 0.8113 - acc: 0.6270 - val_loss: 1.0170 - val_acc: 0.5313\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 0.8125 - acc: 0.6181 - val_loss: 0.9532 - val_acc: 0.5408\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 0.8103 - acc: 0.6118 - val_loss: 0.9605 - val_acc: 0.5361\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 0.8115 - acc: 0.6092 - val_loss: 0.9604 - val_acc: 0.5337\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 0.8097 - acc: 0.6092 - val_loss: 0.9824 - val_acc: 0.5329\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 0.8088 - acc: 0.6118 - val_loss: 1.0604 - val_acc: 0.5196\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 0.8132 - acc: 0.6191 - val_loss: 0.9590 - val_acc: 0.5392\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 0.8059 - acc: 0.6160 - val_loss: 1.0501 - val_acc: 0.4859\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6149 - val_loss: 0.9769 - val_acc: 0.5298\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 0.8081 - acc: 0.6202 - val_loss: 0.9774 - val_acc: 0.5337\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 0.8065 - acc: 0.6149 - val_loss: 0.9966 - val_acc: 0.5408\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 0.8083 - acc: 0.6170 - val_loss: 0.9998 - val_acc: 0.5298\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 0.8094 - acc: 0.6139 - val_loss: 0.9875 - val_acc: 0.5251\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 0.8067 - acc: 0.6186 - val_loss: 1.0566 - val_acc: 0.5282\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 0.8083 - acc: 0.6212 - val_loss: 0.9606 - val_acc: 0.5266\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 0.8059 - acc: 0.6139 - val_loss: 1.0230 - val_acc: 0.5141\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 0.8061 - acc: 0.6144 - val_loss: 0.9875 - val_acc: 0.5415\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 0.8110 - acc: 0.6118 - val_loss: 1.0173 - val_acc: 0.5188\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 0.8190 - acc: 0.6071 - val_loss: 0.9717 - val_acc: 0.5290\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 0.8108 - acc: 0.6160 - val_loss: 0.9674 - val_acc: 0.5455\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 0.8083 - acc: 0.6217 - val_loss: 0.9891 - val_acc: 0.5157\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 0.8114 - acc: 0.6139 - val_loss: 1.0353 - val_acc: 0.5016\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 0.8111 - acc: 0.6181 - val_loss: 0.9631 - val_acc: 0.5431\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 0.8074 - acc: 0.6196 - val_loss: 0.9756 - val_acc: 0.5431\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 0.8059 - acc: 0.6223 - val_loss: 1.0273 - val_acc: 0.5408\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 0.8056 - acc: 0.6149 - val_loss: 0.9873 - val_acc: 0.5212\n",
      "Epoch 936/1000\n",
      " - 0s - loss: 0.8122 - acc: 0.6139 - val_loss: 0.9644 - val_acc: 0.5259\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 0.8073 - acc: 0.6217 - val_loss: 1.0925 - val_acc: 0.4710\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 0.8169 - acc: 0.6087 - val_loss: 1.0416 - val_acc: 0.4843\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 0.8147 - acc: 0.6082 - val_loss: 0.9826 - val_acc: 0.5251\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 0.8132 - acc: 0.6139 - val_loss: 1.0632 - val_acc: 0.5063\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 0.8106 - acc: 0.6092 - val_loss: 0.9883 - val_acc: 0.5024\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 0.8156 - acc: 0.6118 - val_loss: 0.9572 - val_acc: 0.5400\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 0.8062 - acc: 0.6202 - val_loss: 1.0813 - val_acc: 0.4867\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 0.8170 - acc: 0.6123 - val_loss: 0.9729 - val_acc: 0.5345\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 0.8092 - acc: 0.6176 - val_loss: 0.9527 - val_acc: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946/1000\n",
      " - 0s - loss: 0.8081 - acc: 0.6207 - val_loss: 0.9625 - val_acc: 0.5439\n",
      "Epoch 947/1000\n",
      " - 0s - loss: 0.8027 - acc: 0.6259 - val_loss: 1.0122 - val_acc: 0.5149\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 0.8073 - acc: 0.6134 - val_loss: 1.0061 - val_acc: 0.5298\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 0.8102 - acc: 0.6176 - val_loss: 0.9529 - val_acc: 0.5439\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 0.8079 - acc: 0.6196 - val_loss: 1.0049 - val_acc: 0.5329\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 0.8140 - acc: 0.6144 - val_loss: 0.9890 - val_acc: 0.5439\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 0.8086 - acc: 0.6155 - val_loss: 1.0023 - val_acc: 0.5125\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 0.8093 - acc: 0.6170 - val_loss: 1.1236 - val_acc: 0.5024\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 0.8108 - acc: 0.6129 - val_loss: 1.0034 - val_acc: 0.5337\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 0.8048 - acc: 0.6207 - val_loss: 1.0085 - val_acc: 0.5392\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 0.8167 - acc: 0.6134 - val_loss: 0.9950 - val_acc: 0.5408\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 0.8117 - acc: 0.6170 - val_loss: 1.0879 - val_acc: 0.5008\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 0.8069 - acc: 0.6228 - val_loss: 0.9815 - val_acc: 0.5368\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 0.8070 - acc: 0.6228 - val_loss: 0.9728 - val_acc: 0.5345\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 0.8055 - acc: 0.6228 - val_loss: 0.9549 - val_acc: 0.5345\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 0.8029 - acc: 0.6160 - val_loss: 1.0251 - val_acc: 0.5172\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 0.8060 - acc: 0.6259 - val_loss: 1.0354 - val_acc: 0.5172\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 0.8096 - acc: 0.6139 - val_loss: 1.0210 - val_acc: 0.5361\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 0.8100 - acc: 0.6097 - val_loss: 0.9777 - val_acc: 0.5400\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 0.8076 - acc: 0.6181 - val_loss: 0.9873 - val_acc: 0.5094\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 0.8121 - acc: 0.6160 - val_loss: 0.9876 - val_acc: 0.5172\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6139 - val_loss: 0.9893 - val_acc: 0.5008\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 0.8134 - acc: 0.6118 - val_loss: 0.9866 - val_acc: 0.5149\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 0.8023 - acc: 0.6165 - val_loss: 1.0045 - val_acc: 0.5321\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 0.8089 - acc: 0.6238 - val_loss: 0.9958 - val_acc: 0.5196\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 0.8076 - acc: 0.6212 - val_loss: 0.9642 - val_acc: 0.5313\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 0.8072 - acc: 0.6270 - val_loss: 1.0081 - val_acc: 0.5282\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 0.8039 - acc: 0.6144 - val_loss: 0.9799 - val_acc: 0.5282\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 0.8028 - acc: 0.6212 - val_loss: 1.0870 - val_acc: 0.4600\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 0.8105 - acc: 0.6238 - val_loss: 1.1262 - val_acc: 0.5039\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 0.8187 - acc: 0.6144 - val_loss: 1.0128 - val_acc: 0.5086\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 0.8082 - acc: 0.6202 - val_loss: 0.9896 - val_acc: 0.5282\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 0.8092 - acc: 0.6212 - val_loss: 0.9875 - val_acc: 0.5282\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 0.8062 - acc: 0.6123 - val_loss: 1.0462 - val_acc: 0.5047\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 0.8045 - acc: 0.6144 - val_loss: 0.9696 - val_acc: 0.5361\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 0.8050 - acc: 0.6165 - val_loss: 1.0028 - val_acc: 0.5047\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 0.8126 - acc: 0.6118 - val_loss: 1.0936 - val_acc: 0.4961\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 0.8101 - acc: 0.6238 - val_loss: 0.9792 - val_acc: 0.5259\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 0.8057 - acc: 0.6155 - val_loss: 0.9746 - val_acc: 0.5298\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 0.8032 - acc: 0.6223 - val_loss: 1.0304 - val_acc: 0.5071\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 0.8128 - acc: 0.6176 - val_loss: 0.9723 - val_acc: 0.5368\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 0.8063 - acc: 0.6212 - val_loss: 0.9928 - val_acc: 0.5172\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 0.8066 - acc: 0.6238 - val_loss: 0.9950 - val_acc: 0.5368\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 0.8078 - acc: 0.6160 - val_loss: 0.9779 - val_acc: 0.5102\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 0.8065 - acc: 0.6233 - val_loss: 1.0774 - val_acc: 0.4882\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 0.8139 - acc: 0.6113 - val_loss: 1.0019 - val_acc: 0.5290\n",
      "Epoch 992/1000\n",
      " - 0s - loss: 0.8069 - acc: 0.6196 - val_loss: 1.0289 - val_acc: 0.4835\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 0.8256 - acc: 0.6082 - val_loss: 0.9703 - val_acc: 0.5266\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 0.8163 - acc: 0.6149 - val_loss: 1.0195 - val_acc: 0.5329\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 0.8047 - acc: 0.6155 - val_loss: 1.0057 - val_acc: 0.5196\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 0.8026 - acc: 0.6228 - val_loss: 1.0189 - val_acc: 0.5031\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 0.8109 - acc: 0.6144 - val_loss: 1.1633 - val_acc: 0.5008\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 0.8091 - acc: 0.6249 - val_loss: 0.9614 - val_acc: 0.5118\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 0.8073 - acc: 0.6217 - val_loss: 0.9812 - val_acc: 0.5298\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 0.8085 - acc: 0.6196 - val_loss: 0.9997 - val_acc: 0.5290\n"
     ]
    }
   ],
   "source": [
    "n_li = [30, 30, 30]\n",
    "activation_li = [\"relu\", \"relu\", \"relu\", \"softmax\"]\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "model_cnt = KerasDNN(cnt_preprocessed)\n",
    "model_cnt.make_keras_model(n_li, activation_li)\n",
    "model_cnt.run_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 10 µs\n",
      "Train on 1914 samples, validate on 1276 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 1.4281 - acc: 0.3992 - val_loss: 1.0411 - val_acc: 0.4843\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 1.0108 - acc: 0.5324 - val_loss: 1.0263 - val_acc: 0.4922\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.9902 - acc: 0.5376 - val_loss: 1.0178 - val_acc: 0.4929\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.9757 - acc: 0.5371 - val_loss: 0.9994 - val_acc: 0.4953\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.9574 - acc: 0.5376 - val_loss: 0.9791 - val_acc: 0.4984\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.9339 - acc: 0.5455 - val_loss: 0.9514 - val_acc: 0.5188\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.9055 - acc: 0.5575 - val_loss: 0.9160 - val_acc: 0.5549\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.8827 - acc: 0.5831 - val_loss: 0.8856 - val_acc: 0.5987\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.8620 - acc: 0.5836 - val_loss: 0.8767 - val_acc: 0.6379\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.8845 - acc: 0.5951 - val_loss: 0.8633 - val_acc: 0.6567\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.8244 - acc: 0.6447 - val_loss: 0.8091 - val_acc: 0.6881\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.8338 - acc: 0.6165 - val_loss: 0.8563 - val_acc: 0.6324\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.8300 - acc: 0.6322 - val_loss: 0.8661 - val_acc: 0.5541\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.8143 - acc: 0.6186 - val_loss: 0.8284 - val_acc: 0.6340\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.7719 - acc: 0.6520 - val_loss: 0.7784 - val_acc: 0.6513\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.8011 - acc: 0.6306 - val_loss: 0.7899 - val_acc: 0.6716\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.7643 - acc: 0.6541 - val_loss: 0.6891 - val_acc: 0.7531\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.8176 - acc: 0.6703 - val_loss: 0.6564 - val_acc: 0.7602\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.6984 - acc: 0.7011 - val_loss: 0.9642 - val_acc: 0.5400\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.7310 - acc: 0.6594 - val_loss: 0.6554 - val_acc: 0.7610\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.7158 - acc: 0.6954 - val_loss: 0.6534 - val_acc: 0.6904\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.6841 - acc: 0.6933 - val_loss: 0.6911 - val_acc: 0.6654\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.6354 - acc: 0.7048 - val_loss: 0.8266 - val_acc: 0.5627\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.7908 - acc: 0.6249 - val_loss: 0.7254 - val_acc: 0.6842\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.6186 - acc: 0.7356 - val_loss: 0.6656 - val_acc: 0.7053\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.6831 - acc: 0.6787 - val_loss: 0.7341 - val_acc: 0.6881\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.7301 - acc: 0.6505 - val_loss: 0.6469 - val_acc: 0.7696\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.5829 - acc: 0.7560 - val_loss: 0.8904 - val_acc: 0.5141\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.6441 - acc: 0.6964 - val_loss: 0.5350 - val_acc: 0.8127\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.6769 - acc: 0.7001 - val_loss: 0.6054 - val_acc: 0.7759\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.5384 - acc: 0.7785 - val_loss: 0.6395 - val_acc: 0.7061\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.6313 - acc: 0.6991 - val_loss: 0.5094 - val_acc: 0.8064\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.4934 - acc: 0.7936 - val_loss: 0.7897 - val_acc: 0.6669\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.6559 - acc: 0.7252 - val_loss: 0.4827 - val_acc: 0.8276\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.4237 - acc: 0.8438 - val_loss: 0.4446 - val_acc: 0.8401\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.5093 - acc: 0.7832 - val_loss: 0.9627 - val_acc: 0.5768\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.5901 - acc: 0.7571 - val_loss: 0.5485 - val_acc: 0.7931\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.6462 - acc: 0.7247 - val_loss: 0.7666 - val_acc: 0.6928\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.5858 - acc: 0.7581 - val_loss: 0.4845 - val_acc: 0.8049\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.4103 - acc: 0.8396 - val_loss: 0.5170 - val_acc: 0.7790\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.5805 - acc: 0.7335 - val_loss: 0.5143 - val_acc: 0.7516\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.4865 - acc: 0.7915 - val_loss: 0.6968 - val_acc: 0.6716\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.4718 - acc: 0.8072 - val_loss: 0.5085 - val_acc: 0.8009\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.4590 - acc: 0.8166 - val_loss: 0.5418 - val_acc: 0.7884\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.5414 - acc: 0.7497 - val_loss: 0.6051 - val_acc: 0.7249\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.5163 - acc: 0.7665 - val_loss: 0.4380 - val_acc: 0.8378\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.4177 - acc: 0.8307 - val_loss: 0.4820 - val_acc: 0.8150\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.4711 - acc: 0.8020 - val_loss: 0.5099 - val_acc: 0.7892\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.4732 - acc: 0.7905 - val_loss: 0.5144 - val_acc: 0.7727\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.4141 - acc: 0.8286 - val_loss: 0.4236 - val_acc: 0.8370\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.4846 - acc: 0.7785 - val_loss: 0.5821 - val_acc: 0.7312\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.4068 - acc: 0.8433 - val_loss: 0.5072 - val_acc: 0.8009\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.5421 - acc: 0.7900 - val_loss: 0.4825 - val_acc: 0.8088\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.3977 - acc: 0.8448 - val_loss: 0.4851 - val_acc: 0.7868\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.5467 - acc: 0.7586 - val_loss: 0.5936 - val_acc: 0.7312\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.4400 - acc: 0.8245 - val_loss: 0.4044 - val_acc: 0.8621\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.3287 - acc: 0.8845 - val_loss: 0.3797 - val_acc: 0.8574\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.3498 - acc: 0.8673 - val_loss: 0.5510 - val_acc: 0.7939\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.4588 - acc: 0.8145 - val_loss: 0.4969 - val_acc: 0.7884\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.5227 - acc: 0.7691 - val_loss: 0.5843 - val_acc: 0.7508\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.3939 - acc: 0.8412 - val_loss: 0.3999 - val_acc: 0.8503\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.3359 - acc: 0.8704 - val_loss: 0.4327 - val_acc: 0.8299\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.4690 - acc: 0.7978 - val_loss: 0.5553 - val_acc: 0.7578\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.3775 - acc: 0.8506 - val_loss: 0.3695 - val_acc: 0.8707\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.2987 - acc: 0.8986 - val_loss: 0.3562 - val_acc: 0.8723\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.3104 - acc: 0.8793 - val_loss: 0.5588 - val_acc: 0.7782\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.5613 - acc: 0.7539 - val_loss: 0.4004 - val_acc: 0.8566\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.3203 - acc: 0.8908 - val_loss: 0.3924 - val_acc: 0.8542\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.3518 - acc: 0.8553 - val_loss: 0.3763 - val_acc: 0.8582\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.3764 - acc: 0.8438 - val_loss: 0.5952 - val_acc: 0.7335\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.3862 - acc: 0.8417 - val_loss: 0.5024 - val_acc: 0.8009\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.4100 - acc: 0.8375 - val_loss: 0.4999 - val_acc: 0.8072\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.3932 - acc: 0.8359 - val_loss: 0.3709 - val_acc: 0.8644\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.3744 - acc: 0.8480 - val_loss: 0.8597 - val_acc: 0.6356\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.4984 - acc: 0.7952 - val_loss: 0.3815 - val_acc: 0.8574\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.2854 - acc: 0.9033 - val_loss: 0.3591 - val_acc: 0.8613\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.2905 - acc: 0.8892 - val_loss: 0.4197 - val_acc: 0.8331\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.3453 - acc: 0.8615 - val_loss: 0.5072 - val_acc: 0.7931\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.3339 - acc: 0.8699 - val_loss: 0.4330 - val_acc: 0.8252\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.3503 - acc: 0.8527 - val_loss: 0.4672 - val_acc: 0.8056\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.3266 - acc: 0.8730 - val_loss: 0.3498 - val_acc: 0.8754\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.2632 - acc: 0.9039 - val_loss: 0.4121 - val_acc: 0.8480\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.3799 - acc: 0.8438 - val_loss: 0.9457 - val_acc: 0.5956\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.5051 - acc: 0.7795 - val_loss: 0.4003 - val_acc: 0.8386\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.2965 - acc: 0.8856 - val_loss: 0.3762 - val_acc: 0.8527\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.2747 - acc: 0.8960 - val_loss: 0.3778 - val_acc: 0.8534\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.3097 - acc: 0.8824 - val_loss: 0.6786 - val_acc: 0.7100\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.3772 - acc: 0.8412 - val_loss: 0.4052 - val_acc: 0.8339\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.3291 - acc: 0.8694 - val_loss: 0.4373 - val_acc: 0.8292\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.3564 - acc: 0.8542 - val_loss: 0.5421 - val_acc: 0.7853\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.4151 - acc: 0.8250 - val_loss: 0.4681 - val_acc: 0.8182\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.3092 - acc: 0.8819 - val_loss: 0.4685 - val_acc: 0.8127\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.3055 - acc: 0.8788 - val_loss: 0.5801 - val_acc: 0.7610\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.3768 - acc: 0.8521 - val_loss: 0.3553 - val_acc: 0.8691\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.2417 - acc: 0.9222 - val_loss: 0.3402 - val_acc: 0.8715\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 0.2323 - acc: 0.9164 - val_loss: 0.3691 - val_acc: 0.8636\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.4153 - acc: 0.8260 - val_loss: 0.6818 - val_acc: 0.6904\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.3812 - acc: 0.8527 - val_loss: 0.3532 - val_acc: 0.8676\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.2906 - acc: 0.8819 - val_loss: 0.5176 - val_acc: 0.8088\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.4946 - acc: 0.8109 - val_loss: 0.3544 - val_acc: 0.8691\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.2454 - acc: 0.9232 - val_loss: 0.3485 - val_acc: 0.8621\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.2817 - acc: 0.8955 - val_loss: 0.3464 - val_acc: 0.8699\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.2717 - acc: 0.8976 - val_loss: 0.4829 - val_acc: 0.8158\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.3312 - acc: 0.8777 - val_loss: 0.3770 - val_acc: 0.8550\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.2783 - acc: 0.8976 - val_loss: 0.4801 - val_acc: 0.7978\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.2819 - acc: 0.8898 - val_loss: 0.3575 - val_acc: 0.8691\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.2121 - acc: 0.9342 - val_loss: 0.3716 - val_acc: 0.8574\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.3128 - acc: 0.8736 - val_loss: 0.5431 - val_acc: 0.7766\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.3682 - acc: 0.8412 - val_loss: 0.4119 - val_acc: 0.8464\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.3246 - acc: 0.8683 - val_loss: 0.5444 - val_acc: 0.7947\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.3888 - acc: 0.8354 - val_loss: 0.4463 - val_acc: 0.8205\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.3190 - acc: 0.8720 - val_loss: 0.3581 - val_acc: 0.8668\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.2630 - acc: 0.9049 - val_loss: 0.4001 - val_acc: 0.8417\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.2350 - acc: 0.9195 - val_loss: 0.4546 - val_acc: 0.8150\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.3351 - acc: 0.8568 - val_loss: 0.3518 - val_acc: 0.8770\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.2079 - acc: 0.9373 - val_loss: 0.3485 - val_acc: 0.8762\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.2210 - acc: 0.9216 - val_loss: 0.4304 - val_acc: 0.8150\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.2754 - acc: 0.8882 - val_loss: 0.5272 - val_acc: 0.8088\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.5463 - acc: 0.7999 - val_loss: 0.3631 - val_acc: 0.8683\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.2382 - acc: 0.9253 - val_loss: 0.4365 - val_acc: 0.8354\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.2453 - acc: 0.9138 - val_loss: 0.3720 - val_acc: 0.8613\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.2709 - acc: 0.8966 - val_loss: 0.3621 - val_acc: 0.8652\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.2483 - acc: 0.9070 - val_loss: 0.3636 - val_acc: 0.8629\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.2041 - acc: 0.9336 - val_loss: 0.3776 - val_acc: 0.8542\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.3351 - acc: 0.8621 - val_loss: 0.6280 - val_acc: 0.7484\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.3090 - acc: 0.8892 - val_loss: 0.3415 - val_acc: 0.8762\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.2037 - acc: 0.9352 - val_loss: 0.3797 - val_acc: 0.8519\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.1946 - acc: 0.9321 - val_loss: 0.4104 - val_acc: 0.8307\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.2977 - acc: 0.8835 - val_loss: 0.6422 - val_acc: 0.7500\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.3589 - acc: 0.8537 - val_loss: 0.3621 - val_acc: 0.8487\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.2162 - acc: 0.9237 - val_loss: 0.5515 - val_acc: 0.7829\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.5666 - acc: 0.7680 - val_loss: 0.3968 - val_acc: 0.8480\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.2413 - acc: 0.9237 - val_loss: 0.3601 - val_acc: 0.8605\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.1977 - acc: 0.9363 - val_loss: 0.4506 - val_acc: 0.8064\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.2367 - acc: 0.9091 - val_loss: 0.3578 - val_acc: 0.8699\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.1737 - acc: 0.9483 - val_loss: 0.3722 - val_acc: 0.8574\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.2242 - acc: 0.9075 - val_loss: 0.3830 - val_acc: 0.8480\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.2078 - acc: 0.9263 - val_loss: 0.3865 - val_acc: 0.8534\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.3228 - acc: 0.8647 - val_loss: 0.6863 - val_acc: 0.7492\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.4195 - acc: 0.8124 - val_loss: 0.3558 - val_acc: 0.8793\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.1873 - acc: 0.9525 - val_loss: 0.3578 - val_acc: 0.8660\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.1600 - acc: 0.9535 - val_loss: 0.3962 - val_acc: 0.8409\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.2173 - acc: 0.9216 - val_loss: 0.4074 - val_acc: 0.8362\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.2133 - acc: 0.9159 - val_loss: 0.5739 - val_acc: 0.8064\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.3015 - acc: 0.8746 - val_loss: 0.7007 - val_acc: 0.7469\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.4493 - acc: 0.8218 - val_loss: 0.4050 - val_acc: 0.8354\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.1900 - acc: 0.9462 - val_loss: 0.3937 - val_acc: 0.8456\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.2319 - acc: 0.9133 - val_loss: 0.3526 - val_acc: 0.8629\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.1532 - acc: 0.9561 - val_loss: 0.3897 - val_acc: 0.8487\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.2071 - acc: 0.9258 - val_loss: 0.4326 - val_acc: 0.8299\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 0.1815 - acc: 0.9394 - val_loss: 0.3467 - val_acc: 0.8785\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.1537 - acc: 0.9545 - val_loss: 0.3728 - val_acc: 0.8683\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.1766 - acc: 0.9399 - val_loss: 0.3872 - val_acc: 0.8644\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.2196 - acc: 0.9117 - val_loss: 0.7329 - val_acc: 0.7445\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.6666 - acc: 0.7513 - val_loss: 0.4151 - val_acc: 0.8534\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.2322 - acc: 0.9378 - val_loss: 0.3590 - val_acc: 0.8660\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.1599 - acc: 0.9624 - val_loss: 0.3405 - val_acc: 0.8785\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.1436 - acc: 0.9582 - val_loss: 0.3606 - val_acc: 0.8691\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.1714 - acc: 0.9399 - val_loss: 0.3444 - val_acc: 0.8793\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.1560 - acc: 0.9493 - val_loss: 0.4921 - val_acc: 0.8252\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.2365 - acc: 0.9133 - val_loss: 0.3346 - val_acc: 0.8840\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.1311 - acc: 0.9645 - val_loss: 0.3830 - val_acc: 0.8613\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.1314 - acc: 0.9645 - val_loss: 0.3794 - val_acc: 0.8636\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.1442 - acc: 0.9540 - val_loss: 0.5518 - val_acc: 0.7947\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.3923 - acc: 0.8312 - val_loss: 0.4626 - val_acc: 0.8096\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.1835 - acc: 0.9441 - val_loss: 0.3390 - val_acc: 0.8824\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.1233 - acc: 0.9666 - val_loss: 0.3519 - val_acc: 0.8636\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.1333 - acc: 0.9624 - val_loss: 0.3946 - val_acc: 0.8550\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.2491 - acc: 0.8939 - val_loss: 0.8366 - val_acc: 0.7022\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.4620 - acc: 0.8067 - val_loss: 0.3589 - val_acc: 0.8730\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 0.1616 - acc: 0.9613 - val_loss: 0.3390 - val_acc: 0.8824\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.1226 - acc: 0.9707 - val_loss: 0.3389 - val_acc: 0.8824\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.1155 - acc: 0.9707 - val_loss: 0.3460 - val_acc: 0.8738\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.1035 - acc: 0.9723 - val_loss: 0.3596 - val_acc: 0.8715\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.0990 - acc: 0.9775 - val_loss: 0.3501 - val_acc: 0.8762\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.0989 - acc: 0.9744 - val_loss: 0.3758 - val_acc: 0.8699\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.2153 - acc: 0.9289 - val_loss: 0.9969 - val_acc: 0.6998\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.5397 - acc: 0.7753 - val_loss: 0.4196 - val_acc: 0.8386\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.2069 - acc: 0.9336 - val_loss: 0.4066 - val_acc: 0.8597\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.2361 - acc: 0.9080 - val_loss: 0.4881 - val_acc: 0.8268\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.1862 - acc: 0.9321 - val_loss: 0.3397 - val_acc: 0.8770\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.1031 - acc: 0.9749 - val_loss: 0.3586 - val_acc: 0.8660\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.0943 - acc: 0.9775 - val_loss: 0.3612 - val_acc: 0.8676\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.0904 - acc: 0.9807 - val_loss: 0.3572 - val_acc: 0.8723\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.0920 - acc: 0.9781 - val_loss: 0.3835 - val_acc: 0.8801\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.1443 - acc: 0.9514 - val_loss: 0.5158 - val_acc: 0.8315\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.4445 - acc: 0.8182 - val_loss: 0.5035 - val_acc: 0.7798\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.2108 - acc: 0.9289 - val_loss: 0.3453 - val_acc: 0.8762\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.1139 - acc: 0.9749 - val_loss: 0.3507 - val_acc: 0.8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/1000\n",
      " - 0s - loss: 0.0992 - acc: 0.9760 - val_loss: 0.3671 - val_acc: 0.8644\n",
      "Epoch 191/1000\n",
      " - 0s - loss: 0.0913 - acc: 0.9781 - val_loss: 0.3991 - val_acc: 0.8527\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.1033 - acc: 0.9734 - val_loss: 0.5258 - val_acc: 0.8088\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.2143 - acc: 0.9222 - val_loss: 0.3882 - val_acc: 0.8534\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.1187 - acc: 0.9629 - val_loss: 0.4037 - val_acc: 0.8440\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.1121 - acc: 0.9671 - val_loss: 0.3825 - val_acc: 0.8589\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.0853 - acc: 0.9807 - val_loss: 0.3622 - val_acc: 0.8730\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.0734 - acc: 0.9843 - val_loss: 0.3770 - val_acc: 0.8668\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.0799 - acc: 0.9822 - val_loss: 0.4427 - val_acc: 0.8339\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.1652 - acc: 0.9389 - val_loss: 0.8290 - val_acc: 0.7320\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.4185 - acc: 0.8359 - val_loss: 0.6500 - val_acc: 0.7798\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.5134 - acc: 0.8077 - val_loss: 0.3644 - val_acc: 0.8613\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.1450 - acc: 0.9660 - val_loss: 0.3482 - val_acc: 0.8770\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.0947 - acc: 0.9822 - val_loss: 0.3513 - val_acc: 0.8723\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.0775 - acc: 0.9869 - val_loss: 0.3544 - val_acc: 0.8730\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.0814 - acc: 0.9828 - val_loss: 0.3680 - val_acc: 0.8785\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.0920 - acc: 0.9749 - val_loss: 0.4477 - val_acc: 0.8574\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.3972 - acc: 0.8396 - val_loss: 0.6885 - val_acc: 0.7179\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.2601 - acc: 0.9023 - val_loss: 0.3556 - val_acc: 0.8644\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.0946 - acc: 0.9859 - val_loss: 0.3522 - val_acc: 0.8746\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.0770 - acc: 0.9859 - val_loss: 0.3847 - val_acc: 0.8534\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.0686 - acc: 0.9901 - val_loss: 0.3682 - val_acc: 0.8683\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.0642 - acc: 0.9896 - val_loss: 0.3689 - val_acc: 0.8723\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.0642 - acc: 0.9880 - val_loss: 0.4205 - val_acc: 0.8433\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.1060 - acc: 0.9645 - val_loss: 0.4194 - val_acc: 0.8487\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.0632 - acc: 0.9901 - val_loss: 0.3723 - val_acc: 0.8707\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.0596 - acc: 0.9911 - val_loss: 0.3742 - val_acc: 0.8864\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.0641 - acc: 0.9864 - val_loss: 0.3702 - val_acc: 0.8832\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.0625 - acc: 0.9885 - val_loss: 0.3773 - val_acc: 0.8832\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.1119 - acc: 0.9540 - val_loss: 0.8365 - val_acc: 0.7665\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.4159 - acc: 0.8375 - val_loss: 0.4227 - val_acc: 0.8542\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.1264 - acc: 0.9671 - val_loss: 0.3912 - val_acc: 0.8527\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.0688 - acc: 0.9890 - val_loss: 0.3757 - val_acc: 0.8636\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.0585 - acc: 0.9906 - val_loss: 0.3940 - val_acc: 0.8487\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.0596 - acc: 0.9901 - val_loss: 0.3929 - val_acc: 0.8566\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.0540 - acc: 0.9916 - val_loss: 0.4018 - val_acc: 0.8597\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.0496 - acc: 0.9937 - val_loss: 0.3810 - val_acc: 0.8707\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.0523 - acc: 0.9901 - val_loss: 0.3855 - val_acc: 0.8824\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.0488 - acc: 0.9927 - val_loss: 0.3912 - val_acc: 0.8707\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.0472 - acc: 0.9932 - val_loss: 0.3868 - val_acc: 0.8770\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.0449 - acc: 0.9932 - val_loss: 0.3915 - val_acc: 0.8738\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.0450 - acc: 0.9932 - val_loss: 0.3912 - val_acc: 0.8770\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.0441 - acc: 0.9932 - val_loss: 0.3915 - val_acc: 0.8762\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.0486 - acc: 0.9916 - val_loss: 0.4267 - val_acc: 0.8848\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.0864 - acc: 0.9707 - val_loss: 0.6625 - val_acc: 0.8166\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.7120 - acc: 0.7628 - val_loss: 0.5231 - val_acc: 0.8017\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.2123 - acc: 0.9368 - val_loss: 0.3973 - val_acc: 0.8503\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.0887 - acc: 0.9854 - val_loss: 0.3745 - val_acc: 0.8636\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.0589 - acc: 0.9927 - val_loss: 0.3759 - val_acc: 0.8715\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.0512 - acc: 0.9932 - val_loss: 0.3785 - val_acc: 0.8777\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.0466 - acc: 0.9927 - val_loss: 0.3892 - val_acc: 0.8777\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.0444 - acc: 0.9932 - val_loss: 0.3951 - val_acc: 0.8691\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.0415 - acc: 0.9937 - val_loss: 0.3935 - val_acc: 0.8730\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.0398 - acc: 0.9937 - val_loss: 0.3981 - val_acc: 0.8785\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.0386 - acc: 0.9937 - val_loss: 0.3990 - val_acc: 0.8762\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.0378 - acc: 0.9937 - val_loss: 0.3987 - val_acc: 0.8801\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.0374 - acc: 0.9937 - val_loss: 0.4013 - val_acc: 0.8770\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.0364 - acc: 0.9937 - val_loss: 0.4046 - val_acc: 0.8730\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.0363 - acc: 0.9932 - val_loss: 0.4085 - val_acc: 0.8738\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.0351 - acc: 0.9932 - val_loss: 0.4080 - val_acc: 0.8754\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.0341 - acc: 0.9943 - val_loss: 0.4107 - val_acc: 0.8754\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.0333 - acc: 0.9943 - val_loss: 0.4136 - val_acc: 0.8746\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.0334 - acc: 0.9948 - val_loss: 0.4105 - val_acc: 0.8809\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.0330 - acc: 0.9943 - val_loss: 0.4220 - val_acc: 0.8699\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.0315 - acc: 0.9943 - val_loss: 0.4255 - val_acc: 0.8723\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.0344 - acc: 0.9948 - val_loss: 0.4267 - val_acc: 0.8699\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 0.0307 - acc: 0.9948 - val_loss: 0.4237 - val_acc: 0.8691\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.0306 - acc: 0.9943 - val_loss: 0.4179 - val_acc: 0.8801\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.0307 - acc: 0.9943 - val_loss: 0.4186 - val_acc: 0.8801\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.0300 - acc: 0.9943 - val_loss: 0.4216 - val_acc: 0.8801\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.0287 - acc: 0.9953 - val_loss: 0.4299 - val_acc: 0.8715\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.0290 - acc: 0.9948 - val_loss: 0.4329 - val_acc: 0.8683\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.0284 - acc: 0.9953 - val_loss: 0.4511 - val_acc: 0.8589\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.0288 - acc: 0.9948 - val_loss: 0.4275 - val_acc: 0.8730\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.0273 - acc: 0.9953 - val_loss: 0.4273 - val_acc: 0.8762\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.0269 - acc: 0.9953 - val_loss: 0.4314 - val_acc: 0.8730\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.0262 - acc: 0.9953 - val_loss: 0.4316 - val_acc: 0.8746\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.0260 - acc: 0.9958 - val_loss: 0.4332 - val_acc: 0.8770\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.0258 - acc: 0.9958 - val_loss: 0.4321 - val_acc: 0.8785\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.0258 - acc: 0.9958 - val_loss: 0.4320 - val_acc: 0.8754\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.0248 - acc: 0.9958 - val_loss: 0.4377 - val_acc: 0.8723\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.0265 - acc: 0.9948 - val_loss: 0.4356 - val_acc: 0.8770\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.0238 - acc: 0.9958 - val_loss: 0.4373 - val_acc: 0.8754\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.0234 - acc: 0.9958 - val_loss: 0.4430 - val_acc: 0.8715\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.0244 - acc: 0.9953 - val_loss: 0.4452 - val_acc: 0.8699\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.0229 - acc: 0.9958 - val_loss: 0.4440 - val_acc: 0.8738\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.0224 - acc: 0.9958 - val_loss: 0.4481 - val_acc: 0.8723\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.0223 - acc: 0.9963 - val_loss: 0.4491 - val_acc: 0.8676\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.0215 - acc: 0.9958 - val_loss: 0.4506 - val_acc: 0.8699\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.0215 - acc: 0.9969 - val_loss: 0.4564 - val_acc: 0.8676\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.0212 - acc: 0.9958 - val_loss: 0.4460 - val_acc: 0.8777\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.0203 - acc: 0.9963 - val_loss: 0.4573 - val_acc: 0.8683\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.0203 - acc: 0.9969 - val_loss: 0.4520 - val_acc: 0.8770\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.0199 - acc: 0.9974 - val_loss: 0.4537 - val_acc: 0.8707\n",
      "Epoch 284/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0215 - acc: 0.9969 - val_loss: 0.4610 - val_acc: 0.8676\n",
      "Epoch 285/1000\n",
      " - 0s - loss: 0.0194 - acc: 0.9969 - val_loss: 0.4480 - val_acc: 0.8817\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.0197 - acc: 0.9958 - val_loss: 0.4547 - val_acc: 0.8738\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.0187 - acc: 0.9974 - val_loss: 0.4571 - val_acc: 0.8699\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.0184 - acc: 0.9974 - val_loss: 0.4854 - val_acc: 0.8605\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.0195 - acc: 0.9963 - val_loss: 0.4588 - val_acc: 0.8723\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.0179 - acc: 0.9974 - val_loss: 0.4548 - val_acc: 0.8793\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.0188 - acc: 0.9969 - val_loss: 0.4559 - val_acc: 0.8777\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.0183 - acc: 0.9969 - val_loss: 0.4745 - val_acc: 0.8644\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.0183 - acc: 0.9974 - val_loss: 0.4577 - val_acc: 0.8777\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.0173 - acc: 0.9974 - val_loss: 0.4584 - val_acc: 0.8793\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.0177 - acc: 0.9974 - val_loss: 0.4672 - val_acc: 0.8730\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.0169 - acc: 0.9974 - val_loss: 0.4606 - val_acc: 0.8817\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.0161 - acc: 0.9974 - val_loss: 0.4685 - val_acc: 0.8730\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.0161 - acc: 0.9974 - val_loss: 0.4639 - val_acc: 0.8777\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.0161 - acc: 0.9969 - val_loss: 0.4644 - val_acc: 0.8793\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 0.0160 - acc: 0.9969 - val_loss: 0.4693 - val_acc: 0.8730\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.0159 - acc: 0.9974 - val_loss: 0.4652 - val_acc: 0.8785\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.0164 - acc: 0.9969 - val_loss: 0.4698 - val_acc: 0.8746\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.0159 - acc: 0.9974 - val_loss: 0.4737 - val_acc: 0.8715\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.0149 - acc: 0.9974 - val_loss: 0.4702 - val_acc: 0.8762\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.0150 - acc: 0.9974 - val_loss: 0.4718 - val_acc: 0.8754\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.0145 - acc: 0.9974 - val_loss: 0.4742 - val_acc: 0.8738\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.0152 - acc: 0.9969 - val_loss: 0.4762 - val_acc: 0.8707\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.0147 - acc: 0.9974 - val_loss: 0.4726 - val_acc: 0.8762\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9974 - val_loss: 0.4737 - val_acc: 0.8785\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.0143 - acc: 0.9974 - val_loss: 0.4757 - val_acc: 0.8785\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9974 - val_loss: 0.4765 - val_acc: 0.8738\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.0141 - acc: 0.9969 - val_loss: 0.4824 - val_acc: 0.8715\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.0137 - acc: 0.9974 - val_loss: 0.4814 - val_acc: 0.8707\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.0137 - acc: 0.9969 - val_loss: 0.4819 - val_acc: 0.8723\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.0134 - acc: 0.9969 - val_loss: 0.4796 - val_acc: 0.8746\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.0138 - acc: 0.9969 - val_loss: 0.4806 - val_acc: 0.8777\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.0138 - acc: 0.9974 - val_loss: 0.4815 - val_acc: 0.8723\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.0128 - acc: 0.9974 - val_loss: 0.4847 - val_acc: 0.8715\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.0133 - acc: 0.9969 - val_loss: 0.4841 - val_acc: 0.8707\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.0125 - acc: 0.9974 - val_loss: 0.4830 - val_acc: 0.8762\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.0124 - acc: 0.9974 - val_loss: 0.4922 - val_acc: 0.8715\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.0124 - acc: 0.9974 - val_loss: 0.4849 - val_acc: 0.8801\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9974 - val_loss: 0.4902 - val_acc: 0.8730\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.0119 - acc: 0.9974 - val_loss: 0.4912 - val_acc: 0.8676\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9974 - val_loss: 0.4883 - val_acc: 0.8746\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.0123 - acc: 0.9969 - val_loss: 0.4964 - val_acc: 0.8683\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9974 - val_loss: 0.4908 - val_acc: 0.8699\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.0115 - acc: 0.9974 - val_loss: 0.4918 - val_acc: 0.8707\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.0115 - acc: 0.9974 - val_loss: 0.5035 - val_acc: 0.8668\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9969 - val_loss: 0.4996 - val_acc: 0.8676\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.0111 - acc: 0.9974 - val_loss: 0.4914 - val_acc: 0.8801\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.0112 - acc: 0.9974 - val_loss: 0.5156 - val_acc: 0.8629\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9974 - val_loss: 0.4933 - val_acc: 0.8785\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.0119 - acc: 0.9969 - val_loss: 0.4933 - val_acc: 0.8801\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.0114 - acc: 0.9974 - val_loss: 0.4956 - val_acc: 0.8715\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.0107 - acc: 0.9979 - val_loss: 0.5082 - val_acc: 0.8668\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.0110 - acc: 0.9979 - val_loss: 0.4970 - val_acc: 0.8738\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.0110 - acc: 0.9974 - val_loss: 0.4982 - val_acc: 0.8707\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.0111 - acc: 0.9974 - val_loss: 0.4988 - val_acc: 0.8707\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.0105 - acc: 0.9974 - val_loss: 0.4991 - val_acc: 0.8715\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 0.0105 - acc: 0.9974 - val_loss: 0.5001 - val_acc: 0.8730\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.0099 - acc: 0.9979 - val_loss: 0.5025 - val_acc: 0.8738\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9974 - val_loss: 0.5030 - val_acc: 0.8699\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.0097 - acc: 0.9984 - val_loss: 0.5031 - val_acc: 0.8738\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.0099 - acc: 0.9984 - val_loss: 0.5168 - val_acc: 0.8668\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9984 - val_loss: 0.5034 - val_acc: 0.8738\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9979 - val_loss: 0.5058 - val_acc: 0.8723\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.0093 - acc: 0.9984 - val_loss: 0.5051 - val_acc: 0.8730\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.0099 - acc: 0.9984 - val_loss: 0.5049 - val_acc: 0.8754\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9979 - val_loss: 0.5051 - val_acc: 0.8777\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.0093 - acc: 0.9984 - val_loss: 0.5202 - val_acc: 0.8660\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9984 - val_loss: 0.5081 - val_acc: 0.8715\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9984 - val_loss: 0.5180 - val_acc: 0.8691\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9984 - val_loss: 0.5113 - val_acc: 0.8707\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9979 - val_loss: 0.5103 - val_acc: 0.8723\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9984 - val_loss: 0.5261 - val_acc: 0.8660\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9979 - val_loss: 0.5162 - val_acc: 0.8683\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9984 - val_loss: 0.5142 - val_acc: 0.8691\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.0088 - acc: 0.9984 - val_loss: 0.5168 - val_acc: 0.8676\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9984 - val_loss: 0.5165 - val_acc: 0.8691\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.5160 - val_acc: 0.8715\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9984 - val_loss: 0.5287 - val_acc: 0.8676\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9979 - val_loss: 0.5156 - val_acc: 0.8723\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9979 - val_loss: 0.5220 - val_acc: 0.8683\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9979 - val_loss: 0.5177 - val_acc: 0.8762\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.0081 - acc: 0.9984 - val_loss: 0.5201 - val_acc: 0.8691\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9984 - val_loss: 0.5212 - val_acc: 0.8707\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.5196 - val_acc: 0.8715\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.0079 - acc: 0.9984 - val_loss: 0.5217 - val_acc: 0.8691\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9984 - val_loss: 0.5230 - val_acc: 0.8668\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.0079 - acc: 0.9984 - val_loss: 0.5236 - val_acc: 0.8691\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.0077 - acc: 0.9984 - val_loss: 0.5222 - val_acc: 0.8746\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.5276 - val_acc: 0.8668\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.0072 - acc: 0.9984 - val_loss: 0.5270 - val_acc: 0.8691\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.0077 - acc: 0.9984 - val_loss: 0.5262 - val_acc: 0.8660\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.0075 - acc: 0.9979 - val_loss: 0.5245 - val_acc: 0.8691\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.5257 - val_acc: 0.8746\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.5254 - val_acc: 0.8707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/1000\n",
      " - 0s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.5264 - val_acc: 0.8754\n",
      "Epoch 380/1000\n",
      " - 0s - loss: 0.0076 - acc: 0.9979 - val_loss: 0.5276 - val_acc: 0.8715\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.0069 - acc: 0.9984 - val_loss: 0.5332 - val_acc: 0.8676\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.0067 - acc: 0.9984 - val_loss: 0.5302 - val_acc: 0.8691\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.0073 - acc: 0.9979 - val_loss: 0.5327 - val_acc: 0.8668\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.0070 - acc: 0.9984 - val_loss: 0.5339 - val_acc: 0.8668\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.5318 - val_acc: 0.8715\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.0070 - acc: 0.9984 - val_loss: 0.5327 - val_acc: 0.8715\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.0068 - acc: 0.9984 - val_loss: 0.5339 - val_acc: 0.8754\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.5338 - val_acc: 0.8683\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.5340 - val_acc: 0.8683\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.5471 - val_acc: 0.8652\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.0066 - acc: 0.9984 - val_loss: 0.5355 - val_acc: 0.8691\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.0066 - acc: 0.9984 - val_loss: 0.5371 - val_acc: 0.8691\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.0064 - acc: 0.9984 - val_loss: 0.5477 - val_acc: 0.8644\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.5362 - val_acc: 0.8746\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.0067 - acc: 0.9979 - val_loss: 0.5364 - val_acc: 0.8723\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.0063 - acc: 0.9984 - val_loss: 0.5394 - val_acc: 0.8691\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.0061 - acc: 0.9984 - val_loss: 0.5382 - val_acc: 0.8746\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.5402 - val_acc: 0.8699\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.0063 - acc: 0.9984 - val_loss: 0.5398 - val_acc: 0.8676\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.0062 - acc: 0.9979 - val_loss: 0.5389 - val_acc: 0.8699\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.0061 - acc: 0.9979 - val_loss: 0.5399 - val_acc: 0.8707\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.5434 - val_acc: 0.8683\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.0060 - acc: 0.9984 - val_loss: 0.5416 - val_acc: 0.8730\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.5436 - val_acc: 0.8707\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.0054 - acc: 0.9990 - val_loss: 0.5438 - val_acc: 0.8691\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.0057 - acc: 0.9979 - val_loss: 0.5426 - val_acc: 0.8715\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.0060 - acc: 0.9979 - val_loss: 0.5445 - val_acc: 0.8746\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.5436 - val_acc: 0.8676\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.0054 - acc: 0.9990 - val_loss: 0.5488 - val_acc: 0.8683\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.0059 - acc: 0.9984 - val_loss: 0.5457 - val_acc: 0.8683\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.5533 - val_acc: 0.8636\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.5564 - val_acc: 0.8668\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.0061 - acc: 0.9984 - val_loss: 0.5491 - val_acc: 0.8691\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.0054 - acc: 0.9990 - val_loss: 0.5467 - val_acc: 0.8683\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.5491 - val_acc: 0.8746\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.0054 - acc: 0.9990 - val_loss: 0.5536 - val_acc: 0.8676\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.5499 - val_acc: 0.8699\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.0053 - acc: 0.9984 - val_loss: 0.5574 - val_acc: 0.8676\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.0049 - acc: 0.9990 - val_loss: 0.5494 - val_acc: 0.8691\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5512 - val_acc: 0.8683\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.0051 - acc: 0.9984 - val_loss: 0.5516 - val_acc: 0.8738\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.0057 - acc: 0.9984 - val_loss: 0.5520 - val_acc: 0.8683\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.5529 - val_acc: 0.8730\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.0059 - acc: 0.9984 - val_loss: 0.5556 - val_acc: 0.8683\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.0051 - acc: 0.9990 - val_loss: 0.5544 - val_acc: 0.8691\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.5551 - val_acc: 0.8699\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5548 - val_acc: 0.8730\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5553 - val_acc: 0.8699\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5543 - val_acc: 0.8683\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9990 - val_loss: 0.5560 - val_acc: 0.8691\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.0051 - acc: 0.9990 - val_loss: 0.5547 - val_acc: 0.8683\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.0053 - acc: 0.9984 - val_loss: 0.5566 - val_acc: 0.8683\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5702 - val_acc: 0.8668\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.0046 - acc: 0.9990 - val_loss: 0.5574 - val_acc: 0.8699\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9984 - val_loss: 0.5665 - val_acc: 0.8660\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.5581 - val_acc: 0.8738\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.5594 - val_acc: 0.8746\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.5591 - val_acc: 0.8730\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9990 - val_loss: 0.5614 - val_acc: 0.8676\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.0042 - acc: 0.9990 - val_loss: 0.5619 - val_acc: 0.8683\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5609 - val_acc: 0.8691\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.5727 - val_acc: 0.8691\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.0047 - acc: 0.9990 - val_loss: 0.5621 - val_acc: 0.8691\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.5621 - val_acc: 0.8738\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9990 - val_loss: 0.5641 - val_acc: 0.8683\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9990 - val_loss: 0.5635 - val_acc: 0.8754\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.0049 - acc: 0.9984 - val_loss: 0.5631 - val_acc: 0.8730\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.0043 - acc: 0.9990 - val_loss: 0.5651 - val_acc: 0.8683\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.5768 - val_acc: 0.8683\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.0048 - acc: 0.9984 - val_loss: 0.5654 - val_acc: 0.8707\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.0046 - acc: 0.9984 - val_loss: 0.5676 - val_acc: 0.8683\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.5771 - val_acc: 0.8683\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9990 - val_loss: 0.5662 - val_acc: 0.8683\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.5654 - val_acc: 0.8715\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.0043 - acc: 0.9990 - val_loss: 0.5705 - val_acc: 0.8683\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5681 - val_acc: 0.8676\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.5682 - val_acc: 0.8683\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5698 - val_acc: 0.8691\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.0042 - acc: 0.9990 - val_loss: 0.5684 - val_acc: 0.8691\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5721 - val_acc: 0.8683\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.5722 - val_acc: 0.8691\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.0042 - acc: 0.9990 - val_loss: 0.5692 - val_acc: 0.8691\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5728 - val_acc: 0.8699\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5710 - val_acc: 0.8683\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5813 - val_acc: 0.8676\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.0047 - acc: 0.9984 - val_loss: 0.5742 - val_acc: 0.8668\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.5717 - val_acc: 0.8676\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5828 - val_acc: 0.8676\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.5716 - val_acc: 0.8715\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9990 - val_loss: 0.5766 - val_acc: 0.8668\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5744 - val_acc: 0.8683\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9984 - val_loss: 0.5734 - val_acc: 0.8691\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0043 - acc: 0.9984 - val_loss: 0.5740 - val_acc: 0.8691\n",
      "Epoch 474/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5893 - val_acc: 0.8683\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5769 - val_acc: 0.8754\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.5887 - val_acc: 0.8699\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9990 - val_loss: 0.5757 - val_acc: 0.8691\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5775 - val_acc: 0.8699\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5774 - val_acc: 0.8762\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.5763 - val_acc: 0.8738\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.5807 - val_acc: 0.8691\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.5780 - val_acc: 0.8683\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5886 - val_acc: 0.8676\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.5828 - val_acc: 0.8691\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 0.0042 - acc: 0.9984 - val_loss: 0.5826 - val_acc: 0.8683\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9984 - val_loss: 0.5825 - val_acc: 0.8683\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5807 - val_acc: 0.8746\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 0.0042 - acc: 0.9984 - val_loss: 0.5789 - val_acc: 0.8746\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 0.0041 - acc: 0.9984 - val_loss: 0.5799 - val_acc: 0.8699\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5852 - val_acc: 0.8691\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.5816 - val_acc: 0.8691\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.5827 - val_acc: 0.8699\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.5827 - val_acc: 0.8699\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.5832 - val_acc: 0.8676\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9984 - val_loss: 0.5876 - val_acc: 0.8691\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.5831 - val_acc: 0.8691\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5877 - val_acc: 0.8691\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9984 - val_loss: 0.5848 - val_acc: 0.8683\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.5837 - val_acc: 0.8746\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.5852 - val_acc: 0.8707\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 0.0038 - acc: 0.9984 - val_loss: 0.5920 - val_acc: 0.8676\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5841 - val_acc: 0.8699\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5940 - val_acc: 0.8683\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5853 - val_acc: 0.8707\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9984 - val_loss: 0.5869 - val_acc: 0.8770\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9984 - val_loss: 0.5863 - val_acc: 0.8754\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9984 - val_loss: 0.5861 - val_acc: 0.8762\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.6002 - val_acc: 0.8699\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9984 - val_loss: 0.5989 - val_acc: 0.8699\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.5868 - val_acc: 0.8746\n",
      "Epoch 511/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.5988 - val_acc: 0.8691\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5878 - val_acc: 0.8707\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5880 - val_acc: 0.8770\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.5951 - val_acc: 0.8676\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5958 - val_acc: 0.8676\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9990 - val_loss: 0.5916 - val_acc: 0.8691\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.5893 - val_acc: 0.8715\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.5907 - val_acc: 0.8707\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6013 - val_acc: 0.8699\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9995 - val_loss: 0.5925 - val_acc: 0.8777\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9984 - val_loss: 0.5921 - val_acc: 0.8754\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 0.0039 - acc: 0.9990 - val_loss: 0.5909 - val_acc: 0.8754\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.5965 - val_acc: 0.8699\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.5934 - val_acc: 0.8699\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.5956 - val_acc: 0.8691\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.5939 - val_acc: 0.8699\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.5931 - val_acc: 0.8715\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.5953 - val_acc: 0.8691\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.5954 - val_acc: 0.8762\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 0.0040 - acc: 0.9984 - val_loss: 0.5943 - val_acc: 0.8715\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5955 - val_acc: 0.8762\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.5999 - val_acc: 0.8676\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9984 - val_loss: 0.5973 - val_acc: 0.8691\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.6038 - val_acc: 0.8683\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.5954 - val_acc: 0.8699\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9995 - val_loss: 0.5993 - val_acc: 0.8691\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9984 - val_loss: 0.6000 - val_acc: 0.8699\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.5961 - val_acc: 0.8730\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.5964 - val_acc: 0.8738\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6029 - val_acc: 0.8691\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.6001 - val_acc: 0.8707\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.5983 - val_acc: 0.8699\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.5996 - val_acc: 0.8691\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6075 - val_acc: 0.8683\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.5984 - val_acc: 0.8762\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.5988 - val_acc: 0.8723\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.5998 - val_acc: 0.8770\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9995 - val_loss: 0.6022 - val_acc: 0.8707\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.6009 - val_acc: 0.8762\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6121 - val_acc: 0.8691\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.6080 - val_acc: 0.8699\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6001 - val_acc: 0.8707\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6104 - val_acc: 0.8691\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.6028 - val_acc: 0.8754\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6030 - val_acc: 0.8683\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.6011 - val_acc: 0.8762\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6034 - val_acc: 0.8699\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6035 - val_acc: 0.8707\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9984 - val_loss: 0.6135 - val_acc: 0.8691\n",
      "Epoch 560/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6032 - val_acc: 0.8723\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.6143 - val_acc: 0.8691\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6058 - val_acc: 0.8777\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.6078 - val_acc: 0.8691\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.6054 - val_acc: 0.8754\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6054 - val_acc: 0.8699\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6046 - val_acc: 0.8715\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6060 - val_acc: 0.8699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 568/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6069 - val_acc: 0.8754\n",
      "Epoch 569/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9984 - val_loss: 0.6069 - val_acc: 0.8770\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6086 - val_acc: 0.8683\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6060 - val_acc: 0.8754\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6104 - val_acc: 0.8691\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6076 - val_acc: 0.8715\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6186 - val_acc: 0.8691\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6077 - val_acc: 0.8707\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.6106 - val_acc: 0.8683\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6117 - val_acc: 0.8699\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6119 - val_acc: 0.8699\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.6097 - val_acc: 0.8699\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6082 - val_acc: 0.8770\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9984 - val_loss: 0.6083 - val_acc: 0.8762\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6133 - val_acc: 0.8699\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6093 - val_acc: 0.8723\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6102 - val_acc: 0.8707\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6100 - val_acc: 0.8723\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9995 - val_loss: 0.6121 - val_acc: 0.8699\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6128 - val_acc: 0.8785\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9984 - val_loss: 0.6102 - val_acc: 0.8762\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6144 - val_acc: 0.8676\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6112 - val_acc: 0.8715\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6116 - val_acc: 0.8770\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6167 - val_acc: 0.8691\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6202 - val_acc: 0.8699\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6119 - val_acc: 0.8746\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6244 - val_acc: 0.8691\n",
      "Epoch 596/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6128 - val_acc: 0.8723\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6247 - val_acc: 0.8691\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6125 - val_acc: 0.8715\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6143 - val_acc: 0.8707\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6136 - val_acc: 0.8715\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6147 - val_acc: 0.8715\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6177 - val_acc: 0.8683\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6236 - val_acc: 0.8707\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 0.0037 - acc: 0.9990 - val_loss: 0.6179 - val_acc: 0.8683\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6181 - val_acc: 0.8691\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6147 - val_acc: 0.8770\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6242 - val_acc: 0.8699\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6146 - val_acc: 0.8762\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9984 - val_loss: 0.6152 - val_acc: 0.8723\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6160 - val_acc: 0.8762\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.6155 - val_acc: 0.8762\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6173 - val_acc: 0.8762\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6196 - val_acc: 0.8683\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6184 - val_acc: 0.8777\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6209 - val_acc: 0.8676\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6169 - val_acc: 0.8723\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6170 - val_acc: 0.8762\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6172 - val_acc: 0.8754\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6209 - val_acc: 0.8676\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6199 - val_acc: 0.8770\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.6176 - val_acc: 0.8762\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6179 - val_acc: 0.8762\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6227 - val_acc: 0.8676\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6221 - val_acc: 0.8683\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6218 - val_acc: 0.8676\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6193 - val_acc: 0.8770\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6228 - val_acc: 0.8683\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6195 - val_acc: 0.8770\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6191 - val_acc: 0.8754\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6316 - val_acc: 0.8683\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.6194 - val_acc: 0.8762\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6220 - val_acc: 0.8777\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6266 - val_acc: 0.8683\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6309 - val_acc: 0.8707\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.6238 - val_acc: 0.8676\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6230 - val_acc: 0.8770\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9984 - val_loss: 0.6211 - val_acc: 0.8770\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6238 - val_acc: 0.8683\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6215 - val_acc: 0.8762\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6343 - val_acc: 0.8683\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6222 - val_acc: 0.8730\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6222 - val_acc: 0.8762\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6345 - val_acc: 0.8691\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6247 - val_acc: 0.8770\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.6226 - val_acc: 0.8762\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6245 - val_acc: 0.8707\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6239 - val_acc: 0.8770\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6235 - val_acc: 0.8746\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6288 - val_acc: 0.8676\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6253 - val_acc: 0.8762\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6356 - val_acc: 0.8691\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6243 - val_acc: 0.8770\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6295 - val_acc: 0.8668\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6246 - val_acc: 0.8762\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6261 - val_acc: 0.8699\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6253 - val_acc: 0.8762\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6268 - val_acc: 0.8762\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9984 - val_loss: 0.6253 - val_acc: 0.8770\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6267 - val_acc: 0.8770\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.6271 - val_acc: 0.8770\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.6308 - val_acc: 0.8668\n",
      "Epoch 662/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6382 - val_acc: 0.8691\n",
      "Epoch 663/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6263 - val_acc: 0.8762\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6263 - val_acc: 0.8770\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6397 - val_acc: 0.8683\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6292 - val_acc: 0.8770\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6379 - val_acc: 0.8715\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6296 - val_acc: 0.8770\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6285 - val_acc: 0.8715\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6304 - val_acc: 0.8770\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.6276 - val_acc: 0.8770\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6293 - val_acc: 0.8762\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6296 - val_acc: 0.8715\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6373 - val_acc: 0.8699\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6285 - val_acc: 0.8730\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6397 - val_acc: 0.8707\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.6338 - val_acc: 0.8676\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6292 - val_acc: 0.8738\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6300 - val_acc: 0.8715\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6321 - val_acc: 0.8770\n",
      "Epoch 681/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6313 - val_acc: 0.8715\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6297 - val_acc: 0.8770\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6311 - val_acc: 0.8715\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6323 - val_acc: 0.8777\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6319 - val_acc: 0.8770\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6322 - val_acc: 0.8770\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6421 - val_acc: 0.8699\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6397 - val_acc: 0.8715\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6305 - val_acc: 0.8754\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6326 - val_acc: 0.8723\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6337 - val_acc: 0.8770\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6339 - val_acc: 0.8770\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6424 - val_acc: 0.8699\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.6314 - val_acc: 0.8754\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6315 - val_acc: 0.8770\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6317 - val_acc: 0.8770\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6326 - val_acc: 0.8777\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6322 - val_acc: 0.8762\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6341 - val_acc: 0.8715\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6327 - val_acc: 0.8770\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6350 - val_acc: 0.8762\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6348 - val_acc: 0.8715\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6330 - val_acc: 0.8762\n",
      "Epoch 704/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6378 - val_acc: 0.8683\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6374 - val_acc: 0.8691\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6434 - val_acc: 0.8715\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6361 - val_acc: 0.8770\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6356 - val_acc: 0.8770\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6340 - val_acc: 0.8770\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6343 - val_acc: 0.8770\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6363 - val_acc: 0.8770\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6393 - val_acc: 0.8683\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6459 - val_acc: 0.8707\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6349 - val_acc: 0.8723\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6364 - val_acc: 0.8707\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6354 - val_acc: 0.8723\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6356 - val_acc: 0.8762\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6373 - val_acc: 0.8707\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6389 - val_acc: 0.8707\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6390 - val_acc: 0.8707\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6360 - val_acc: 0.8762\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6363 - val_acc: 0.8770\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6380 - val_acc: 0.8770\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6412 - val_acc: 0.8668\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6373 - val_acc: 0.8738\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6496 - val_acc: 0.8707\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6371 - val_acc: 0.8762\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6471 - val_acc: 0.8715\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6375 - val_acc: 0.8762\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6396 - val_acc: 0.8723\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6380 - val_acc: 0.8762\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6396 - val_acc: 0.8723\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6388 - val_acc: 0.8777\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6398 - val_acc: 0.8770\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6388 - val_acc: 0.8762\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6412 - val_acc: 0.8707\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6391 - val_acc: 0.8738\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6392 - val_acc: 0.8777\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6414 - val_acc: 0.8715\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6481 - val_acc: 0.8691\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6421 - val_acc: 0.8707\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6488 - val_acc: 0.8707\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.6419 - val_acc: 0.8699\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6420 - val_acc: 0.8707\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6416 - val_acc: 0.8723\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6405 - val_acc: 0.8777\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6428 - val_acc: 0.8707\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6407 - val_acc: 0.8777\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6410 - val_acc: 0.8777\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6461 - val_acc: 0.8676\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6409 - val_acc: 0.8762\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6440 - val_acc: 0.8754\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.6439 - val_acc: 0.8707\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6507 - val_acc: 0.8699\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6479 - val_acc: 0.8683\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6417 - val_acc: 0.8770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 757/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6425 - val_acc: 0.8730\n",
      "Epoch 758/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6448 - val_acc: 0.8754\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6429 - val_acc: 0.8723\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6420 - val_acc: 0.8770\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6424 - val_acc: 0.8770\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6444 - val_acc: 0.8770\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6438 - val_acc: 0.8723\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6453 - val_acc: 0.8762\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6460 - val_acc: 0.8762\n",
      "Epoch 766/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6446 - val_acc: 0.8770\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6431 - val_acc: 0.8770\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6449 - val_acc: 0.8770\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6447 - val_acc: 0.8770\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6517 - val_acc: 0.8683\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6459 - val_acc: 0.8762\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6470 - val_acc: 0.8691\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6547 - val_acc: 0.8707\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.6474 - val_acc: 0.8691\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6476 - val_acc: 0.8777\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6488 - val_acc: 0.8683\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6473 - val_acc: 0.8754\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6458 - val_acc: 0.8777\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6446 - val_acc: 0.8762\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6470 - val_acc: 0.8715\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6445 - val_acc: 0.8770\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6447 - val_acc: 0.8770\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6469 - val_acc: 0.8723\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6449 - val_acc: 0.8770\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6451 - val_acc: 0.8762\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6510 - val_acc: 0.8676\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6479 - val_acc: 0.8762\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6456 - val_acc: 0.8770\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6569 - val_acc: 0.8707\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6481 - val_acc: 0.8762\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6561 - val_acc: 0.8707\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6484 - val_acc: 0.8762\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6488 - val_acc: 0.8699\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6463 - val_acc: 0.8746\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6500 - val_acc: 0.8691\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6495 - val_acc: 0.8707\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6477 - val_acc: 0.8777\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6475 - val_acc: 0.8777\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6516 - val_acc: 0.8676\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6469 - val_acc: 0.8770\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6483 - val_acc: 0.8770\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9995 - val_loss: 0.6508 - val_acc: 0.8699\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6500 - val_acc: 0.8770\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6511 - val_acc: 0.8691\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6504 - val_acc: 0.8785\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6482 - val_acc: 0.8770\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6499 - val_acc: 0.8730\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6483 - val_acc: 0.8777\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6513 - val_acc: 0.8754\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6583 - val_acc: 0.8676\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6525 - val_acc: 0.8691\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6557 - val_acc: 0.8676\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6516 - val_acc: 0.8691\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6485 - val_acc: 0.8770\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6492 - val_acc: 0.8777\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6507 - val_acc: 0.8730\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6492 - val_acc: 0.8746\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6495 - val_acc: 0.8770\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6497 - val_acc: 0.8777\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6519 - val_acc: 0.8730\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6501 - val_acc: 0.8777\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6608 - val_acc: 0.8691\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6501 - val_acc: 0.8777\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6515 - val_acc: 0.8770\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6527 - val_acc: 0.8707\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6542 - val_acc: 0.8691\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6503 - val_acc: 0.8777\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6517 - val_acc: 0.8723\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6504 - val_acc: 0.8777\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6519 - val_acc: 0.8730\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.6605 - val_acc: 0.8668\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.6565 - val_acc: 0.8683\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6510 - val_acc: 0.8777\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6555 - val_acc: 0.8683\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6539 - val_acc: 0.8762\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6516 - val_acc: 0.8770\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6536 - val_acc: 0.8723\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6626 - val_acc: 0.8691\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6518 - val_acc: 0.8770\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6533 - val_acc: 0.8730\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.6625 - val_acc: 0.8691\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6546 - val_acc: 0.8762\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6540 - val_acc: 0.8715\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6521 - val_acc: 0.8777\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6541 - val_acc: 0.8723\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6571 - val_acc: 0.8676\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6556 - val_acc: 0.8762\n",
      "Epoch 848/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6571 - val_acc: 0.8683\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6535 - val_acc: 0.8777\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6556 - val_acc: 0.8715\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6627 - val_acc: 0.8676\n",
      "Epoch 852/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6535 - val_acc: 0.8777\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6549 - val_acc: 0.8730\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6566 - val_acc: 0.8715\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6560 - val_acc: 0.8715\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6617 - val_acc: 0.8676\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6574 - val_acc: 0.8777\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6573 - val_acc: 0.8770\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6600 - val_acc: 0.8683\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6539 - val_acc: 0.8785\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6542 - val_acc: 0.8770\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6565 - val_acc: 0.8723\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6582 - val_acc: 0.8691\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6635 - val_acc: 0.8676\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6615 - val_acc: 0.8676\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.6577 - val_acc: 0.8699\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6545 - val_acc: 0.8777\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6642 - val_acc: 0.8668\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6624 - val_acc: 0.8683\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6545 - val_acc: 0.8777\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6577 - val_acc: 0.8707\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6648 - val_acc: 0.8676\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6553 - val_acc: 0.8785\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6667 - val_acc: 0.8683\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6664 - val_acc: 0.8683\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6557 - val_acc: 0.8785\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6578 - val_acc: 0.8730\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6566 - val_acc: 0.8777\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6567 - val_acc: 0.8777\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6666 - val_acc: 0.8676\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6593 - val_acc: 0.8699\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6629 - val_acc: 0.8683\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6568 - val_acc: 0.8777\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6605 - val_acc: 0.8785\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6645 - val_acc: 0.8683\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6658 - val_acc: 0.8676\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6566 - val_acc: 0.8785\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6667 - val_acc: 0.8676\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6591 - val_acc: 0.8770\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6579 - val_acc: 0.8777\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6597 - val_acc: 0.8770\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6592 - val_acc: 0.8723\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6650 - val_acc: 0.8668\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6574 - val_acc: 0.8777\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6604 - val_acc: 0.8715\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6602 - val_acc: 0.8762\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6610 - val_acc: 0.8691\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6578 - val_acc: 0.8777\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6626 - val_acc: 0.8676\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6582 - val_acc: 0.8785\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6582 - val_acc: 0.8785\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6585 - val_acc: 0.8785\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6598 - val_acc: 0.8777\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.6622 - val_acc: 0.8683\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6581 - val_acc: 0.8785\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6604 - val_acc: 0.8770\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6619 - val_acc: 0.8699\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6589 - val_acc: 0.8738\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.6613 - val_acc: 0.8770\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6611 - val_acc: 0.8715\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6636 - val_acc: 0.8676\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6639 - val_acc: 0.8683\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6596 - val_acc: 0.8785\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6596 - val_acc: 0.8785\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6703 - val_acc: 0.8676\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6597 - val_acc: 0.8777\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6605 - val_acc: 0.8777\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6603 - val_acc: 0.8793\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6622 - val_acc: 0.8777\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6599 - val_acc: 0.8785\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6650 - val_acc: 0.8676\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6603 - val_acc: 0.8793\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6705 - val_acc: 0.8676\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.6651 - val_acc: 0.8683\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6604 - val_acc: 0.8785\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6650 - val_acc: 0.8683\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6681 - val_acc: 0.8683\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6639 - val_acc: 0.8699\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6606 - val_acc: 0.8785\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6628 - val_acc: 0.8770\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6615 - val_acc: 0.8777\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6659 - val_acc: 0.8676\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6613 - val_acc: 0.8754\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6614 - val_acc: 0.8785\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6675 - val_acc: 0.8683\n",
      "Epoch 936/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6636 - val_acc: 0.8770\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6716 - val_acc: 0.8676\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6616 - val_acc: 0.8777\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6641 - val_acc: 0.8715\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.6622 - val_acc: 0.8730\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6735 - val_acc: 0.8676\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6624 - val_acc: 0.8793\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6666 - val_acc: 0.8691\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6670 - val_acc: 0.8676\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6633 - val_acc: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6634 - val_acc: 0.8785\n",
      "Epoch 947/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6644 - val_acc: 0.8785\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6647 - val_acc: 0.8730\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6656 - val_acc: 0.8699\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6636 - val_acc: 0.8777\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6635 - val_acc: 0.8785\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6661 - val_acc: 0.8762\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6655 - val_acc: 0.8777\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6725 - val_acc: 0.8676\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6668 - val_acc: 0.8699\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6665 - val_acc: 0.8770\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6688 - val_acc: 0.8676\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6682 - val_acc: 0.8676\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6640 - val_acc: 0.8785\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6679 - val_acc: 0.8691\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6638 - val_acc: 0.8777\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6678 - val_acc: 0.8676\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6639 - val_acc: 0.8793\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6652 - val_acc: 0.8785\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6667 - val_acc: 0.8707\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.6642 - val_acc: 0.8777\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.6658 - val_acc: 0.8723\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 0.0024 - acc: 0.9990 - val_loss: 0.6740 - val_acc: 0.8676\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6643 - val_acc: 0.8793\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6672 - val_acc: 0.8699\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.6648 - val_acc: 0.8793\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6727 - val_acc: 0.8683\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6692 - val_acc: 0.8683\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.6651 - val_acc: 0.8793\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6654 - val_acc: 0.8793\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6677 - val_acc: 0.8777\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6689 - val_acc: 0.8699\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6689 - val_acc: 0.8707\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6687 - val_acc: 0.8699\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6681 - val_acc: 0.8707\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6688 - val_acc: 0.8762\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6661 - val_acc: 0.8793\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6698 - val_acc: 0.8683\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6684 - val_acc: 0.8777\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6687 - val_acc: 0.8762\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6667 - val_acc: 0.8785\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6698 - val_acc: 0.8691\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6687 - val_acc: 0.8770\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6679 - val_acc: 0.8770\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6675 - val_acc: 0.8730\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6665 - val_acc: 0.8793\n",
      "Epoch 992/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6691 - val_acc: 0.8699\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.6729 - val_acc: 0.8676\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6685 - val_acc: 0.8707\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.6707 - val_acc: 0.8785\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.6669 - val_acc: 0.8793\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.6686 - val_acc: 0.8770\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.6754 - val_acc: 0.8676\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.6716 - val_acc: 0.8676\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.6668 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "n_li = [120, 60, 30, 10]\n",
    "activation_li = [\"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"]\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "batch_size = 500\n",
    "\n",
    "model = KerasDNN(dnn_preprocessed)\n",
    "model.make_keras_model(n_li, activation_li, lr=lr)\n",
    "model.run_keras_model(epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.86      0.84      0.85       320\n",
      "         IE       0.83      0.85      0.84       327\n",
      "          N       0.91      0.91      0.91       629\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnn_model_classification_report(model.model, dnn_preprocessed, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.hist.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(120, input_dim=60, activation=\"sigmoid\"))\n",
    "model1.add(Dense(60, activation=\"relu\"))\n",
    "model1.add(Dense(3, activation=\"softmax\"))\n",
    "model1.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1914 samples, validate on 1276 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 1.0126 - acc: 0.5303 - val_loss: 1.0234 - val_acc: 0.4945\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.9608 - acc: 0.5423 - val_loss: 0.9908 - val_acc: 0.4976\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.9415 - acc: 0.5470 - val_loss: 0.9717 - val_acc: 0.5016\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.9231 - acc: 0.5507 - val_loss: 0.9511 - val_acc: 0.5102\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.9051 - acc: 0.5559 - val_loss: 0.9303 - val_acc: 0.5235\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.8863 - acc: 0.5648 - val_loss: 0.9166 - val_acc: 0.5470\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.8687 - acc: 0.5888 - val_loss: 0.8973 - val_acc: 0.5384\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.8510 - acc: 0.5935 - val_loss: 0.8845 - val_acc: 0.5337\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.8338 - acc: 0.6024 - val_loss: 0.8610 - val_acc: 0.5713\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.8166 - acc: 0.6290 - val_loss: 0.8655 - val_acc: 0.5337\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.8001 - acc: 0.6238 - val_loss: 0.8250 - val_acc: 0.6599\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.7826 - acc: 0.6641 - val_loss: 0.8141 - val_acc: 0.6097\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.7649 - acc: 0.6714 - val_loss: 0.7878 - val_acc: 0.6544\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.7475 - acc: 0.6865 - val_loss: 0.7754 - val_acc: 0.6560\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.7304 - acc: 0.6970 - val_loss: 0.7543 - val_acc: 0.6716\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.7125 - acc: 0.7090 - val_loss: 0.7366 - val_acc: 0.6787\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.6944 - acc: 0.7200 - val_loss: 0.7175 - val_acc: 0.7390\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.6782 - acc: 0.7351 - val_loss: 0.7007 - val_acc: 0.7100\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.6629 - acc: 0.7409 - val_loss: 0.6849 - val_acc: 0.7461\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.6472 - acc: 0.7529 - val_loss: 0.6774 - val_acc: 0.7061\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.6343 - acc: 0.7508 - val_loss: 0.6572 - val_acc: 0.7281\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.6213 - acc: 0.7638 - val_loss: 0.6443 - val_acc: 0.7900\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.6074 - acc: 0.7743 - val_loss: 0.6287 - val_acc: 0.7704\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.5966 - acc: 0.7722 - val_loss: 0.6126 - val_acc: 0.7892\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.5817 - acc: 0.7769 - val_loss: 0.6051 - val_acc: 0.7712\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.5705 - acc: 0.7837 - val_loss: 0.5899 - val_acc: 0.7900\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.5592 - acc: 0.7936 - val_loss: 0.5787 - val_acc: 0.8166\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.5487 - acc: 0.7957 - val_loss: 0.5648 - val_acc: 0.8150\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.5395 - acc: 0.7978 - val_loss: 0.5582 - val_acc: 0.7962\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.5305 - acc: 0.7983 - val_loss: 0.5512 - val_acc: 0.8182\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.5206 - acc: 0.8114 - val_loss: 0.5445 - val_acc: 0.8103\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.5108 - acc: 0.8119 - val_loss: 0.5339 - val_acc: 0.8080\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.5033 - acc: 0.8156 - val_loss: 0.5262 - val_acc: 0.7994\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.4926 - acc: 0.8197 - val_loss: 0.5220 - val_acc: 0.8072\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.4864 - acc: 0.8265 - val_loss: 0.5020 - val_acc: 0.8292\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.4786 - acc: 0.8281 - val_loss: 0.5233 - val_acc: 0.7821\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.4708 - acc: 0.8307 - val_loss: 0.4868 - val_acc: 0.8331\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.4642 - acc: 0.8318 - val_loss: 0.4951 - val_acc: 0.8174\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.4574 - acc: 0.8349 - val_loss: 0.4839 - val_acc: 0.8260\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.4534 - acc: 0.8454 - val_loss: 0.5586 - val_acc: 0.7194\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.4500 - acc: 0.8333 - val_loss: 0.4867 - val_acc: 0.8080\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.4410 - acc: 0.8454 - val_loss: 0.4674 - val_acc: 0.8323\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.4339 - acc: 0.8433 - val_loss: 0.4567 - val_acc: 0.8331\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.4296 - acc: 0.8469 - val_loss: 0.4479 - val_acc: 0.8386\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.4243 - acc: 0.8501 - val_loss: 0.4428 - val_acc: 0.8331\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.4201 - acc: 0.8501 - val_loss: 0.4544 - val_acc: 0.8268\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.4145 - acc: 0.8516 - val_loss: 0.4429 - val_acc: 0.8417\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.4101 - acc: 0.8516 - val_loss: 0.4366 - val_acc: 0.8307\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.4053 - acc: 0.8568 - val_loss: 0.4309 - val_acc: 0.8378\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.4019 - acc: 0.8521 - val_loss: 0.4354 - val_acc: 0.8378\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.3995 - acc: 0.8584 - val_loss: 0.4234 - val_acc: 0.8527\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.3948 - acc: 0.8626 - val_loss: 0.4359 - val_acc: 0.8323\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.3903 - acc: 0.8558 - val_loss: 0.4254 - val_acc: 0.8393\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.3863 - acc: 0.8642 - val_loss: 0.4332 - val_acc: 0.8323\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.3815 - acc: 0.8642 - val_loss: 0.4268 - val_acc: 0.8346\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.3814 - acc: 0.8605 - val_loss: 0.4032 - val_acc: 0.8534\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.3769 - acc: 0.8657 - val_loss: 0.4253 - val_acc: 0.8315\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.3750 - acc: 0.8626 - val_loss: 0.3986 - val_acc: 0.8503\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.3714 - acc: 0.8652 - val_loss: 0.4020 - val_acc: 0.8480\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.3690 - acc: 0.8631 - val_loss: 0.3929 - val_acc: 0.8582\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.3639 - acc: 0.8642 - val_loss: 0.4012 - val_acc: 0.8503\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.3619 - acc: 0.8741 - val_loss: 0.3917 - val_acc: 0.8534\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.3600 - acc: 0.8662 - val_loss: 0.3889 - val_acc: 0.8534\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.3565 - acc: 0.8704 - val_loss: 0.4058 - val_acc: 0.8448\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.3550 - acc: 0.8730 - val_loss: 0.3894 - val_acc: 0.8566\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.3537 - acc: 0.8710 - val_loss: 0.4344 - val_acc: 0.8103\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.3520 - acc: 0.8678 - val_loss: 0.4184 - val_acc: 0.8268\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.3508 - acc: 0.8715 - val_loss: 0.3820 - val_acc: 0.8589\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.3460 - acc: 0.8720 - val_loss: 0.3881 - val_acc: 0.8487\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.3437 - acc: 0.8767 - val_loss: 0.3926 - val_acc: 0.8503\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.3438 - acc: 0.8762 - val_loss: 0.3873 - val_acc: 0.8542\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.3412 - acc: 0.8730 - val_loss: 0.4039 - val_acc: 0.8354\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.3393 - acc: 0.8751 - val_loss: 0.3905 - val_acc: 0.8448\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.3350 - acc: 0.8788 - val_loss: 0.4144 - val_acc: 0.8331\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.3352 - acc: 0.8757 - val_loss: 0.3836 - val_acc: 0.8503\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.3337 - acc: 0.8783 - val_loss: 0.3674 - val_acc: 0.8605\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.3313 - acc: 0.8793 - val_loss: 0.3655 - val_acc: 0.8676\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.3290 - acc: 0.8814 - val_loss: 0.4049 - val_acc: 0.8323\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.3327 - acc: 0.8819 - val_loss: 0.3793 - val_acc: 0.8480\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.3262 - acc: 0.8819 - val_loss: 0.3783 - val_acc: 0.8534\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.3264 - acc: 0.8809 - val_loss: 0.3652 - val_acc: 0.8683\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.3233 - acc: 0.8772 - val_loss: 0.3591 - val_acc: 0.8660\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.3227 - acc: 0.8814 - val_loss: 0.3634 - val_acc: 0.8621\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.3202 - acc: 0.8851 - val_loss: 0.3692 - val_acc: 0.8534\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.3206 - acc: 0.8798 - val_loss: 0.4378 - val_acc: 0.7939\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.3204 - acc: 0.8861 - val_loss: 0.4543 - val_acc: 0.7908\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.3185 - acc: 0.8824 - val_loss: 0.4157 - val_acc: 0.8307\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.3205 - acc: 0.8840 - val_loss: 0.3566 - val_acc: 0.8668\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.3137 - acc: 0.8887 - val_loss: 0.3608 - val_acc: 0.8605\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.3114 - acc: 0.8861 - val_loss: 0.4006 - val_acc: 0.8331\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.3105 - acc: 0.8866 - val_loss: 0.3552 - val_acc: 0.8660\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.3104 - acc: 0.8913 - val_loss: 0.4226 - val_acc: 0.8213\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.3108 - acc: 0.8856 - val_loss: 0.3511 - val_acc: 0.8668\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.3068 - acc: 0.8877 - val_loss: 0.4054 - val_acc: 0.8245\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 0.3085 - acc: 0.8908 - val_loss: 0.3859 - val_acc: 0.8425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      " - 0s - loss: 0.3049 - acc: 0.8913 - val_loss: 0.3675 - val_acc: 0.8566\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.3037 - acc: 0.8898 - val_loss: 0.3667 - val_acc: 0.8629\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.3055 - acc: 0.8903 - val_loss: 0.3702 - val_acc: 0.8589\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.3017 - acc: 0.8913 - val_loss: 0.3812 - val_acc: 0.8558\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.2987 - acc: 0.8981 - val_loss: 0.3533 - val_acc: 0.8652\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.2973 - acc: 0.8908 - val_loss: 0.3736 - val_acc: 0.8574\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.2971 - acc: 0.8955 - val_loss: 0.3548 - val_acc: 0.8652\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.2987 - acc: 0.8924 - val_loss: 0.4037 - val_acc: 0.8245\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.2964 - acc: 0.8939 - val_loss: 0.4343 - val_acc: 0.8002\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.2964 - acc: 0.8908 - val_loss: 0.3622 - val_acc: 0.8566\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.2943 - acc: 0.8934 - val_loss: 0.3437 - val_acc: 0.8730\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.2941 - acc: 0.8918 - val_loss: 0.3453 - val_acc: 0.8683\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.2913 - acc: 0.8971 - val_loss: 0.3568 - val_acc: 0.8621\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.2894 - acc: 0.8960 - val_loss: 0.3444 - val_acc: 0.8738\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.2904 - acc: 0.8966 - val_loss: 0.3680 - val_acc: 0.8480\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.2886 - acc: 0.8992 - val_loss: 0.3522 - val_acc: 0.8699\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.2882 - acc: 0.8945 - val_loss: 0.4037 - val_acc: 0.8574\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.2880 - acc: 0.8992 - val_loss: 0.3408 - val_acc: 0.8715\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.2872 - acc: 0.8966 - val_loss: 0.3448 - val_acc: 0.8683\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.2826 - acc: 0.8992 - val_loss: 0.4301 - val_acc: 0.7994\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.2861 - acc: 0.8971 - val_loss: 0.3482 - val_acc: 0.8699\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.2852 - acc: 0.8976 - val_loss: 0.3657 - val_acc: 0.8519\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.2818 - acc: 0.9002 - val_loss: 0.3570 - val_acc: 0.8597\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.2803 - acc: 0.9023 - val_loss: 0.4388 - val_acc: 0.8002\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.2817 - acc: 0.8934 - val_loss: 0.3402 - val_acc: 0.8746\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.2797 - acc: 0.9013 - val_loss: 0.3399 - val_acc: 0.8777\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.2799 - acc: 0.8992 - val_loss: 0.4200 - val_acc: 0.8135\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.2781 - acc: 0.9039 - val_loss: 0.3506 - val_acc: 0.8691\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.2747 - acc: 0.9086 - val_loss: 0.3362 - val_acc: 0.8738\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.2749 - acc: 0.9065 - val_loss: 0.3462 - val_acc: 0.8699\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.2730 - acc: 0.9049 - val_loss: 0.3689 - val_acc: 0.8433\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.2733 - acc: 0.9049 - val_loss: 0.3593 - val_acc: 0.8566\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.2716 - acc: 0.9054 - val_loss: 0.3426 - val_acc: 0.8707\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.2729 - acc: 0.9044 - val_loss: 0.3327 - val_acc: 0.8754\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.2692 - acc: 0.9075 - val_loss: 0.3469 - val_acc: 0.8683\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.2698 - acc: 0.9096 - val_loss: 0.3437 - val_acc: 0.8723\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.2700 - acc: 0.9044 - val_loss: 0.3549 - val_acc: 0.8605\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.2667 - acc: 0.9060 - val_loss: 0.3469 - val_acc: 0.8715\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.2682 - acc: 0.9101 - val_loss: 0.3717 - val_acc: 0.8534\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.2681 - acc: 0.9107 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.2642 - acc: 0.9075 - val_loss: 0.3579 - val_acc: 0.8582\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.2650 - acc: 0.9086 - val_loss: 0.3458 - val_acc: 0.8699\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.2648 - acc: 0.9065 - val_loss: 0.3416 - val_acc: 0.8715\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.2597 - acc: 0.9086 - val_loss: 0.4082 - val_acc: 0.8174\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.2652 - acc: 0.9101 - val_loss: 0.3336 - val_acc: 0.8738\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.2633 - acc: 0.9070 - val_loss: 0.3373 - val_acc: 0.8723\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.2613 - acc: 0.9096 - val_loss: 0.3362 - val_acc: 0.8785\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.2601 - acc: 0.9086 - val_loss: 0.3416 - val_acc: 0.8707\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.2593 - acc: 0.9075 - val_loss: 0.3750 - val_acc: 0.8440\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.2571 - acc: 0.9122 - val_loss: 0.3496 - val_acc: 0.8691\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.2565 - acc: 0.9133 - val_loss: 0.3524 - val_acc: 0.8660\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.2563 - acc: 0.9138 - val_loss: 0.3293 - val_acc: 0.8746\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.2563 - acc: 0.9127 - val_loss: 0.3427 - val_acc: 0.8715\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.2547 - acc: 0.9143 - val_loss: 0.3318 - val_acc: 0.8801\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.2569 - acc: 0.9091 - val_loss: 0.3411 - val_acc: 0.8699\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 0.2531 - acc: 0.9148 - val_loss: 0.3370 - val_acc: 0.8707\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.2496 - acc: 0.9159 - val_loss: 0.3647 - val_acc: 0.8464\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.2514 - acc: 0.9143 - val_loss: 0.3843 - val_acc: 0.8495\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.2517 - acc: 0.9159 - val_loss: 0.3372 - val_acc: 0.8707\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.2508 - acc: 0.9159 - val_loss: 0.3493 - val_acc: 0.8683\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.2480 - acc: 0.9185 - val_loss: 0.3247 - val_acc: 0.8824\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.2483 - acc: 0.9148 - val_loss: 0.3317 - val_acc: 0.8754\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.2464 - acc: 0.9190 - val_loss: 0.3326 - val_acc: 0.8715\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.2456 - acc: 0.9164 - val_loss: 0.3326 - val_acc: 0.8738\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.2440 - acc: 0.9195 - val_loss: 0.3399 - val_acc: 0.8683\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.2439 - acc: 0.9190 - val_loss: 0.3532 - val_acc: 0.8699\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.2433 - acc: 0.9195 - val_loss: 0.3241 - val_acc: 0.8817\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.2431 - acc: 0.9164 - val_loss: 0.3600 - val_acc: 0.8660\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.2417 - acc: 0.9175 - val_loss: 0.4116 - val_acc: 0.8166\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.2429 - acc: 0.9185 - val_loss: 0.3247 - val_acc: 0.8785\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.2404 - acc: 0.9180 - val_loss: 0.3229 - val_acc: 0.8809\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.2389 - acc: 0.9175 - val_loss: 0.3243 - val_acc: 0.8785\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.2378 - acc: 0.9190 - val_loss: 0.3436 - val_acc: 0.8683\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.2376 - acc: 0.9232 - val_loss: 0.3241 - val_acc: 0.8832\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.2360 - acc: 0.9227 - val_loss: 0.3208 - val_acc: 0.8824\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 0.2372 - acc: 0.9201 - val_loss: 0.3217 - val_acc: 0.8848\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.2340 - acc: 0.9237 - val_loss: 0.3417 - val_acc: 0.8785\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.2346 - acc: 0.9185 - val_loss: 0.5179 - val_acc: 0.7586\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.2460 - acc: 0.9112 - val_loss: 0.3614 - val_acc: 0.8582\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.2360 - acc: 0.9232 - val_loss: 0.3301 - val_acc: 0.8770\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.2324 - acc: 0.9232 - val_loss: 0.3338 - val_acc: 0.8707\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.2307 - acc: 0.9227 - val_loss: 0.3394 - val_acc: 0.8715\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.2300 - acc: 0.9232 - val_loss: 0.3284 - val_acc: 0.8715\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.2304 - acc: 0.9232 - val_loss: 0.3364 - val_acc: 0.8707\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.2298 - acc: 0.9253 - val_loss: 0.3374 - val_acc: 0.8691\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.2282 - acc: 0.9237 - val_loss: 0.3701 - val_acc: 0.8456\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.2271 - acc: 0.9258 - val_loss: 0.3276 - val_acc: 0.8730\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.2235 - acc: 0.9248 - val_loss: 0.3499 - val_acc: 0.8746\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.2307 - acc: 0.9242 - val_loss: 0.3269 - val_acc: 0.8785\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.2252 - acc: 0.9253 - val_loss: 0.3236 - val_acc: 0.8840\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.2256 - acc: 0.9237 - val_loss: 0.3452 - val_acc: 0.8636\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.2273 - acc: 0.9185 - val_loss: 0.3428 - val_acc: 0.8707\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.2225 - acc: 0.9263 - val_loss: 0.3177 - val_acc: 0.8809\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.2209 - acc: 0.9295 - val_loss: 0.3171 - val_acc: 0.8824\n",
      "Epoch 190/1000\n",
      " - 0s - loss: 0.2187 - acc: 0.9305 - val_loss: 0.3540 - val_acc: 0.8534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/1000\n",
      " - 0s - loss: 0.2202 - acc: 0.9263 - val_loss: 0.3299 - val_acc: 0.8746\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.2175 - acc: 0.9321 - val_loss: 0.3209 - val_acc: 0.8785\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.2201 - acc: 0.9310 - val_loss: 0.3331 - val_acc: 0.8793\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.2169 - acc: 0.9300 - val_loss: 0.3403 - val_acc: 0.8738\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.2185 - acc: 0.9237 - val_loss: 0.3263 - val_acc: 0.8754\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.2175 - acc: 0.9258 - val_loss: 0.3405 - val_acc: 0.8699\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.2148 - acc: 0.9310 - val_loss: 0.3444 - val_acc: 0.8770\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.2174 - acc: 0.9274 - val_loss: 0.3793 - val_acc: 0.8284\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.2183 - acc: 0.9258 - val_loss: 0.3528 - val_acc: 0.8582\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.2137 - acc: 0.9336 - val_loss: 0.3228 - val_acc: 0.8801\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.2140 - acc: 0.9279 - val_loss: 0.3430 - val_acc: 0.8644\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.2105 - acc: 0.9363 - val_loss: 0.3282 - val_acc: 0.8801\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.2105 - acc: 0.9336 - val_loss: 0.3254 - val_acc: 0.8809\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.2091 - acc: 0.9373 - val_loss: 0.3503 - val_acc: 0.8668\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.2093 - acc: 0.9305 - val_loss: 0.3252 - val_acc: 0.8746\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.2080 - acc: 0.9321 - val_loss: 0.3630 - val_acc: 0.8519\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.2126 - acc: 0.9363 - val_loss: 0.3244 - val_acc: 0.8832\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.2072 - acc: 0.9336 - val_loss: 0.3148 - val_acc: 0.8871\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.2057 - acc: 0.9368 - val_loss: 0.3183 - val_acc: 0.8809\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.2047 - acc: 0.9357 - val_loss: 0.3869 - val_acc: 0.8534\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.2078 - acc: 0.9352 - val_loss: 0.3214 - val_acc: 0.8840\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.2017 - acc: 0.9368 - val_loss: 0.3897 - val_acc: 0.8339\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.2057 - acc: 0.9373 - val_loss: 0.3345 - val_acc: 0.8676\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.2017 - acc: 0.9373 - val_loss: 0.3129 - val_acc: 0.8911\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.2035 - acc: 0.9342 - val_loss: 0.3344 - val_acc: 0.8730\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.1997 - acc: 0.9383 - val_loss: 0.3633 - val_acc: 0.8566\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.2000 - acc: 0.9342 - val_loss: 0.3175 - val_acc: 0.8817\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.2006 - acc: 0.9368 - val_loss: 0.3261 - val_acc: 0.8817\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.1964 - acc: 0.9383 - val_loss: 0.3352 - val_acc: 0.8738\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.1988 - acc: 0.9368 - val_loss: 0.3146 - val_acc: 0.8848\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.1953 - acc: 0.9399 - val_loss: 0.3438 - val_acc: 0.8636\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.1959 - acc: 0.9415 - val_loss: 0.3226 - val_acc: 0.8817\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.1958 - acc: 0.9383 - val_loss: 0.3215 - val_acc: 0.8785\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.1936 - acc: 0.9420 - val_loss: 0.3191 - val_acc: 0.8832\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.1920 - acc: 0.9394 - val_loss: 0.3295 - val_acc: 0.8715\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.1959 - acc: 0.9389 - val_loss: 0.3137 - val_acc: 0.8856\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.1952 - acc: 0.9373 - val_loss: 0.3187 - val_acc: 0.8832\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.1906 - acc: 0.9399 - val_loss: 0.3194 - val_acc: 0.8911\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.1899 - acc: 0.9404 - val_loss: 0.3365 - val_acc: 0.8707\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.1924 - acc: 0.9404 - val_loss: 0.3152 - val_acc: 0.8817\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.1889 - acc: 0.9420 - val_loss: 0.3226 - val_acc: 0.8793\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.1864 - acc: 0.9446 - val_loss: 0.3807 - val_acc: 0.8346\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.1891 - acc: 0.9420 - val_loss: 0.3136 - val_acc: 0.8824\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.1868 - acc: 0.9436 - val_loss: 0.3173 - val_acc: 0.8793\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.1865 - acc: 0.9425 - val_loss: 0.3386 - val_acc: 0.8660\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.1857 - acc: 0.9425 - val_loss: 0.3110 - val_acc: 0.8911\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.1836 - acc: 0.9457 - val_loss: 0.3229 - val_acc: 0.8840\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.1844 - acc: 0.9420 - val_loss: 0.3650 - val_acc: 0.8527\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.1851 - acc: 0.9425 - val_loss: 0.3109 - val_acc: 0.8911\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.1812 - acc: 0.9441 - val_loss: 0.3231 - val_acc: 0.8793\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.1807 - acc: 0.9472 - val_loss: 0.3429 - val_acc: 0.8801\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.1842 - acc: 0.9431 - val_loss: 0.3152 - val_acc: 0.8824\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.1807 - acc: 0.9457 - val_loss: 0.3374 - val_acc: 0.8793\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.1814 - acc: 0.9425 - val_loss: 0.3167 - val_acc: 0.8809\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.1778 - acc: 0.9483 - val_loss: 0.3448 - val_acc: 0.8723\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.1808 - acc: 0.9467 - val_loss: 0.3118 - val_acc: 0.8911\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.1767 - acc: 0.9493 - val_loss: 0.3152 - val_acc: 0.8911\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.1779 - acc: 0.9451 - val_loss: 0.3341 - val_acc: 0.8715\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.1776 - acc: 0.9472 - val_loss: 0.3353 - val_acc: 0.8840\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.1760 - acc: 0.9488 - val_loss: 0.3421 - val_acc: 0.8668\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.1749 - acc: 0.9488 - val_loss: 0.3292 - val_acc: 0.8730\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.1716 - acc: 0.9488 - val_loss: 0.3136 - val_acc: 0.8801\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.1737 - acc: 0.9498 - val_loss: 0.4318 - val_acc: 0.8127\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.1755 - acc: 0.9483 - val_loss: 0.3187 - val_acc: 0.8762\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.1697 - acc: 0.9504 - val_loss: 0.3341 - val_acc: 0.8676\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 0.1704 - acc: 0.9514 - val_loss: 0.3247 - val_acc: 0.8793\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.1712 - acc: 0.9519 - val_loss: 0.3227 - val_acc: 0.8801\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.1703 - acc: 0.9498 - val_loss: 0.3224 - val_acc: 0.8887\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.1694 - acc: 0.9483 - val_loss: 0.3502 - val_acc: 0.8644\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.1698 - acc: 0.9504 - val_loss: 0.3106 - val_acc: 0.8926\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.1660 - acc: 0.9519 - val_loss: 0.3116 - val_acc: 0.8856\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.1657 - acc: 0.9530 - val_loss: 0.3314 - val_acc: 0.8754\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.1663 - acc: 0.9504 - val_loss: 0.3379 - val_acc: 0.8699\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.1667 - acc: 0.9525 - val_loss: 0.3219 - val_acc: 0.8809\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.1652 - acc: 0.9535 - val_loss: 0.3192 - val_acc: 0.8809\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.1638 - acc: 0.9540 - val_loss: 0.3109 - val_acc: 0.8895\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.1626 - acc: 0.9561 - val_loss: 0.3104 - val_acc: 0.8887\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.1605 - acc: 0.9551 - val_loss: 0.3353 - val_acc: 0.8644\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.1609 - acc: 0.9535 - val_loss: 0.3112 - val_acc: 0.8879\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.1585 - acc: 0.9566 - val_loss: 0.3608 - val_acc: 0.8770\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.1639 - acc: 0.9493 - val_loss: 0.4032 - val_acc: 0.8315\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.1614 - acc: 0.9540 - val_loss: 0.3292 - val_acc: 0.8691\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.1593 - acc: 0.9566 - val_loss: 0.3190 - val_acc: 0.8777\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.1573 - acc: 0.9572 - val_loss: 0.3191 - val_acc: 0.8848\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.1571 - acc: 0.9561 - val_loss: 0.3275 - val_acc: 0.8738\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.1558 - acc: 0.9530 - val_loss: 0.3125 - val_acc: 0.8871\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.1536 - acc: 0.9587 - val_loss: 0.3194 - val_acc: 0.8832\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.1553 - acc: 0.9530 - val_loss: 0.3171 - val_acc: 0.8911\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.1544 - acc: 0.9561 - val_loss: 0.3436 - val_acc: 0.8582\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.1554 - acc: 0.9566 - val_loss: 0.3097 - val_acc: 0.8903\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.1536 - acc: 0.9545 - val_loss: 0.3709 - val_acc: 0.8542\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.1522 - acc: 0.9619 - val_loss: 0.3138 - val_acc: 0.8824\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.1545 - acc: 0.9556 - val_loss: 0.3119 - val_acc: 0.8832\n",
      "Epoch 284/1000\n",
      " - 0s - loss: 0.1500 - acc: 0.9603 - val_loss: 0.3127 - val_acc: 0.8926\n",
      "Epoch 285/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1497 - acc: 0.9582 - val_loss: 0.3663 - val_acc: 0.8440\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.1508 - acc: 0.9613 - val_loss: 0.3567 - val_acc: 0.8621\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.1503 - acc: 0.9624 - val_loss: 0.3387 - val_acc: 0.8660\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.1485 - acc: 0.9572 - val_loss: 0.3111 - val_acc: 0.8848\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.1464 - acc: 0.9613 - val_loss: 0.3624 - val_acc: 0.8723\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.1500 - acc: 0.9566 - val_loss: 0.3086 - val_acc: 0.8903\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.1433 - acc: 0.9645 - val_loss: 0.3290 - val_acc: 0.8746\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.1442 - acc: 0.9603 - val_loss: 0.4124 - val_acc: 0.8174\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.1473 - acc: 0.9608 - val_loss: 0.3581 - val_acc: 0.8793\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.1450 - acc: 0.9577 - val_loss: 0.3261 - val_acc: 0.8754\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.1419 - acc: 0.9629 - val_loss: 0.4228 - val_acc: 0.8158\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.1439 - acc: 0.9613 - val_loss: 0.3351 - val_acc: 0.8864\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.1407 - acc: 0.9634 - val_loss: 0.3543 - val_acc: 0.8644\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.1420 - acc: 0.9629 - val_loss: 0.3136 - val_acc: 0.8926\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.1393 - acc: 0.9624 - val_loss: 0.3324 - val_acc: 0.8754\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 0.1401 - acc: 0.9613 - val_loss: 0.3339 - val_acc: 0.8691\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.1390 - acc: 0.9634 - val_loss: 0.3279 - val_acc: 0.8817\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.1376 - acc: 0.9634 - val_loss: 0.3378 - val_acc: 0.8723\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.1378 - acc: 0.9624 - val_loss: 0.3353 - val_acc: 0.8699\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.1381 - acc: 0.9681 - val_loss: 0.3545 - val_acc: 0.8668\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.1365 - acc: 0.9639 - val_loss: 0.3147 - val_acc: 0.8840\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.1349 - acc: 0.9645 - val_loss: 0.3225 - val_acc: 0.8793\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.1361 - acc: 0.9660 - val_loss: 0.3204 - val_acc: 0.8856\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.1338 - acc: 0.9655 - val_loss: 0.3909 - val_acc: 0.8534\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.1356 - acc: 0.9666 - val_loss: 0.3213 - val_acc: 0.8793\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.1321 - acc: 0.9681 - val_loss: 0.3296 - val_acc: 0.8824\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.1306 - acc: 0.9666 - val_loss: 0.3365 - val_acc: 0.8691\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.1324 - acc: 0.9702 - val_loss: 0.3124 - val_acc: 0.8942\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.1294 - acc: 0.9681 - val_loss: 0.3534 - val_acc: 0.8668\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.1321 - acc: 0.9650 - val_loss: 0.3086 - val_acc: 0.8879\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.1288 - acc: 0.9702 - val_loss: 0.3238 - val_acc: 0.8754\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.1291 - acc: 0.9671 - val_loss: 0.3174 - val_acc: 0.8817\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.1270 - acc: 0.9671 - val_loss: 0.3373 - val_acc: 0.8809\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.1281 - acc: 0.9671 - val_loss: 0.3191 - val_acc: 0.8793\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.1264 - acc: 0.9707 - val_loss: 0.3144 - val_acc: 0.8840\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.1260 - acc: 0.9697 - val_loss: 0.3318 - val_acc: 0.8887\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.1243 - acc: 0.9718 - val_loss: 0.3315 - val_acc: 0.8754\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.1244 - acc: 0.9697 - val_loss: 0.3273 - val_acc: 0.8879\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.1251 - acc: 0.9687 - val_loss: 0.3420 - val_acc: 0.8652\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.1236 - acc: 0.9713 - val_loss: 0.3673 - val_acc: 0.8597\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.1255 - acc: 0.9692 - val_loss: 0.3169 - val_acc: 0.8895\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.1213 - acc: 0.9697 - val_loss: 0.4244 - val_acc: 0.8323\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.1259 - acc: 0.9660 - val_loss: 0.3104 - val_acc: 0.8856\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.1208 - acc: 0.9739 - val_loss: 0.3343 - val_acc: 0.8715\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.1201 - acc: 0.9728 - val_loss: 0.3382 - val_acc: 0.8707\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.1203 - acc: 0.9687 - val_loss: 0.3127 - val_acc: 0.8903\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.1180 - acc: 0.9707 - val_loss: 0.3336 - val_acc: 0.8676\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.1187 - acc: 0.9760 - val_loss: 0.3197 - val_acc: 0.8793\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.1173 - acc: 0.9723 - val_loss: 0.3307 - val_acc: 0.8707\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.1172 - acc: 0.9760 - val_loss: 0.3155 - val_acc: 0.8856\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.1166 - acc: 0.9744 - val_loss: 0.3098 - val_acc: 0.8887\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.1138 - acc: 0.9749 - val_loss: 0.3094 - val_acc: 0.8895\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.1138 - acc: 0.9760 - val_loss: 0.3335 - val_acc: 0.8864\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.1157 - acc: 0.9739 - val_loss: 0.3100 - val_acc: 0.8895\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.1142 - acc: 0.9775 - val_loss: 0.4014 - val_acc: 0.8393\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.1173 - acc: 0.9718 - val_loss: 0.3096 - val_acc: 0.8832\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 0.1138 - acc: 0.9728 - val_loss: 0.3667 - val_acc: 0.8503\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.1134 - acc: 0.9749 - val_loss: 0.3250 - val_acc: 0.8817\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.1142 - acc: 0.9718 - val_loss: 0.3317 - val_acc: 0.8809\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.1122 - acc: 0.9744 - val_loss: 0.3098 - val_acc: 0.8864\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.1110 - acc: 0.9728 - val_loss: 0.3137 - val_acc: 0.8824\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.1089 - acc: 0.9786 - val_loss: 0.3169 - val_acc: 0.8824\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.1097 - acc: 0.9817 - val_loss: 0.3163 - val_acc: 0.8926\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.1090 - acc: 0.9760 - val_loss: 0.3218 - val_acc: 0.8824\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.1083 - acc: 0.9770 - val_loss: 0.3103 - val_acc: 0.8871\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.1060 - acc: 0.9791 - val_loss: 0.3659 - val_acc: 0.8425\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.1095 - acc: 0.9781 - val_loss: 0.4333 - val_acc: 0.8487\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.1132 - acc: 0.9754 - val_loss: 0.3244 - val_acc: 0.8879\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.1075 - acc: 0.9791 - val_loss: 0.3553 - val_acc: 0.8824\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.1072 - acc: 0.9749 - val_loss: 0.3139 - val_acc: 0.8856\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.1053 - acc: 0.9765 - val_loss: 0.3138 - val_acc: 0.8856\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.1046 - acc: 0.9791 - val_loss: 0.3309 - val_acc: 0.8793\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.1059 - acc: 0.9754 - val_loss: 0.3168 - val_acc: 0.8817\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.1030 - acc: 0.9812 - val_loss: 0.3137 - val_acc: 0.8918\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.1019 - acc: 0.9781 - val_loss: 0.3982 - val_acc: 0.8566\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.1059 - acc: 0.9801 - val_loss: 0.3320 - val_acc: 0.8809\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.1031 - acc: 0.9796 - val_loss: 0.3207 - val_acc: 0.8840\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.1027 - acc: 0.9791 - val_loss: 0.3315 - val_acc: 0.8801\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.1016 - acc: 0.9791 - val_loss: 0.3112 - val_acc: 0.8887\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.0990 - acc: 0.9801 - val_loss: 0.3150 - val_acc: 0.8856\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.0980 - acc: 0.9812 - val_loss: 0.3302 - val_acc: 0.8856\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.0996 - acc: 0.9822 - val_loss: 0.3174 - val_acc: 0.8934\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.0987 - acc: 0.9817 - val_loss: 0.3128 - val_acc: 0.8887\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.0977 - acc: 0.9828 - val_loss: 0.3173 - val_acc: 0.8918\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.0987 - acc: 0.9838 - val_loss: 0.3284 - val_acc: 0.8801\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.0972 - acc: 0.9822 - val_loss: 0.3236 - val_acc: 0.8793\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.0961 - acc: 0.9838 - val_loss: 0.3150 - val_acc: 0.8840\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.0950 - acc: 0.9822 - val_loss: 0.3249 - val_acc: 0.8809\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.0953 - acc: 0.9838 - val_loss: 0.3651 - val_acc: 0.8534\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.0959 - acc: 0.9822 - val_loss: 0.3185 - val_acc: 0.8809\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.0940 - acc: 0.9854 - val_loss: 0.3156 - val_acc: 0.8864\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.0933 - acc: 0.9864 - val_loss: 0.3376 - val_acc: 0.8864\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.3567 - val_acc: 0.8574\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.0928 - acc: 0.9843 - val_loss: 0.3191 - val_acc: 0.8817\n",
      "Epoch 379/1000\n",
      " - 0s - loss: 0.0919 - acc: 0.9859 - val_loss: 0.3337 - val_acc: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1000\n",
      " - 0s - loss: 0.0921 - acc: 0.9828 - val_loss: 0.3216 - val_acc: 0.8856\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.0925 - acc: 0.9833 - val_loss: 0.3165 - val_acc: 0.8918\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.0900 - acc: 0.9848 - val_loss: 0.3154 - val_acc: 0.8840\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.0899 - acc: 0.9864 - val_loss: 0.3176 - val_acc: 0.8871\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.0896 - acc: 0.9864 - val_loss: 0.3194 - val_acc: 0.8856\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.3439 - val_acc: 0.8636\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.0900 - acc: 0.9864 - val_loss: 0.3227 - val_acc: 0.8777\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.0879 - acc: 0.9864 - val_loss: 0.3356 - val_acc: 0.8707\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.0870 - acc: 0.9875 - val_loss: 0.3171 - val_acc: 0.8856\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.0866 - acc: 0.9869 - val_loss: 0.3332 - val_acc: 0.8785\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.0877 - acc: 0.9859 - val_loss: 0.3281 - val_acc: 0.8777\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.0868 - acc: 0.9880 - val_loss: 0.3256 - val_acc: 0.8785\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.0858 - acc: 0.9869 - val_loss: 0.3317 - val_acc: 0.8911\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.0849 - acc: 0.9864 - val_loss: 0.4391 - val_acc: 0.8245\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.0873 - acc: 0.9864 - val_loss: 0.3386 - val_acc: 0.8887\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.0866 - acc: 0.9843 - val_loss: 0.3255 - val_acc: 0.8832\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.0854 - acc: 0.9854 - val_loss: 0.3157 - val_acc: 0.8856\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.0824 - acc: 0.9875 - val_loss: 0.3214 - val_acc: 0.8832\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.0826 - acc: 0.9864 - val_loss: 0.3179 - val_acc: 0.8824\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.0819 - acc: 0.9869 - val_loss: 0.3253 - val_acc: 0.8911\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.0816 - acc: 0.9901 - val_loss: 0.3282 - val_acc: 0.8793\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.0819 - acc: 0.9896 - val_loss: 0.3534 - val_acc: 0.8668\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.0826 - acc: 0.9875 - val_loss: 0.3411 - val_acc: 0.8770\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.0824 - acc: 0.9859 - val_loss: 0.3224 - val_acc: 0.8840\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.0805 - acc: 0.9875 - val_loss: 0.3192 - val_acc: 0.8856\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.0785 - acc: 0.9880 - val_loss: 0.3345 - val_acc: 0.8785\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.0796 - acc: 0.9864 - val_loss: 0.3219 - val_acc: 0.8840\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.0781 - acc: 0.9880 - val_loss: 0.3389 - val_acc: 0.8864\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.0804 - acc: 0.9859 - val_loss: 0.3233 - val_acc: 0.8840\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.0789 - acc: 0.9875 - val_loss: 0.3332 - val_acc: 0.8817\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.0777 - acc: 0.9901 - val_loss: 0.3215 - val_acc: 0.8817\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.0770 - acc: 0.9896 - val_loss: 0.3408 - val_acc: 0.8911\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.0771 - acc: 0.9875 - val_loss: 0.3281 - val_acc: 0.8785\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.0762 - acc: 0.9906 - val_loss: 0.3228 - val_acc: 0.8848\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.0762 - acc: 0.9885 - val_loss: 0.3323 - val_acc: 0.8770\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.0752 - acc: 0.9911 - val_loss: 0.3673 - val_acc: 0.8480\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.0758 - acc: 0.9906 - val_loss: 0.3321 - val_acc: 0.8785\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.0766 - acc: 0.9896 - val_loss: 0.3254 - val_acc: 0.8887\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.0738 - acc: 0.9885 - val_loss: 0.3294 - val_acc: 0.8832\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.0741 - acc: 0.9906 - val_loss: 0.3232 - val_acc: 0.8864\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.0725 - acc: 0.9901 - val_loss: 0.3246 - val_acc: 0.8911\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.0731 - acc: 0.9896 - val_loss: 0.3394 - val_acc: 0.8801\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.0719 - acc: 0.9906 - val_loss: 0.3532 - val_acc: 0.8746\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.0746 - acc: 0.9885 - val_loss: 0.3296 - val_acc: 0.8871\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.0716 - acc: 0.9916 - val_loss: 0.3512 - val_acc: 0.8636\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.0719 - acc: 0.9922 - val_loss: 0.3299 - val_acc: 0.8840\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 0.0711 - acc: 0.9916 - val_loss: 0.3320 - val_acc: 0.8809\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.0708 - acc: 0.9901 - val_loss: 0.3215 - val_acc: 0.8864\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.0703 - acc: 0.9916 - val_loss: 0.3311 - val_acc: 0.8793\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.0695 - acc: 0.9937 - val_loss: 0.3306 - val_acc: 0.8824\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.0694 - acc: 0.9911 - val_loss: 0.3805 - val_acc: 0.8511\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.0712 - acc: 0.9911 - val_loss: 0.3278 - val_acc: 0.8864\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.0684 - acc: 0.9901 - val_loss: 0.3299 - val_acc: 0.8871\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.0689 - acc: 0.9890 - val_loss: 0.3402 - val_acc: 0.8754\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.0681 - acc: 0.9922 - val_loss: 0.3588 - val_acc: 0.8887\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.0688 - acc: 0.9911 - val_loss: 0.3249 - val_acc: 0.8879\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.0661 - acc: 0.9927 - val_loss: 0.3368 - val_acc: 0.8871\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.0676 - acc: 0.9911 - val_loss: 0.3244 - val_acc: 0.8848\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.0666 - acc: 0.9916 - val_loss: 0.3427 - val_acc: 0.8738\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.0653 - acc: 0.9916 - val_loss: 0.3685 - val_acc: 0.8723\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.0663 - acc: 0.9906 - val_loss: 0.3766 - val_acc: 0.8534\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.0658 - acc: 0.9932 - val_loss: 0.3251 - val_acc: 0.8903\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.0639 - acc: 0.9927 - val_loss: 0.3373 - val_acc: 0.8707\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.0640 - acc: 0.9943 - val_loss: 0.3667 - val_acc: 0.8660\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.0653 - acc: 0.9911 - val_loss: 0.3258 - val_acc: 0.8871\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.0631 - acc: 0.9937 - val_loss: 0.3316 - val_acc: 0.8817\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.0629 - acc: 0.9932 - val_loss: 0.3277 - val_acc: 0.8879\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.0631 - acc: 0.9916 - val_loss: 0.3272 - val_acc: 0.8856\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.0622 - acc: 0.9927 - val_loss: 0.3264 - val_acc: 0.8840\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 0.0612 - acc: 0.9937 - val_loss: 0.3732 - val_acc: 0.8527\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.0634 - acc: 0.9906 - val_loss: 0.3804 - val_acc: 0.8534\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.0637 - acc: 0.9943 - val_loss: 0.3248 - val_acc: 0.8824\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.0608 - acc: 0.9948 - val_loss: 0.3619 - val_acc: 0.8644\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.0622 - acc: 0.9948 - val_loss: 0.3260 - val_acc: 0.8840\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.0601 - acc: 0.9948 - val_loss: 0.3401 - val_acc: 0.8793\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.0607 - acc: 0.9943 - val_loss: 0.3475 - val_acc: 0.8746\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.0600 - acc: 0.9958 - val_loss: 0.3279 - val_acc: 0.8840\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.0597 - acc: 0.9937 - val_loss: 0.3340 - val_acc: 0.8824\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.0588 - acc: 0.9948 - val_loss: 0.3280 - val_acc: 0.8864\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.0584 - acc: 0.9958 - val_loss: 0.3296 - val_acc: 0.8895\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.0583 - acc: 0.9948 - val_loss: 0.3278 - val_acc: 0.8879\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.0585 - acc: 0.9937 - val_loss: 0.3770 - val_acc: 0.8605\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.0587 - acc: 0.9963 - val_loss: 0.3271 - val_acc: 0.8864\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.0569 - acc: 0.9958 - val_loss: 0.3385 - val_acc: 0.8785\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.0576 - acc: 0.9953 - val_loss: 0.3455 - val_acc: 0.8770\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.0569 - acc: 0.9963 - val_loss: 0.3582 - val_acc: 0.8660\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.0575 - acc: 0.9943 - val_loss: 0.3314 - val_acc: 0.8824\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.0562 - acc: 0.9953 - val_loss: 0.3313 - val_acc: 0.8824\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.0559 - acc: 0.9953 - val_loss: 0.3577 - val_acc: 0.8871\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.0562 - acc: 0.9953 - val_loss: 0.3303 - val_acc: 0.8879\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.0553 - acc: 0.9953 - val_loss: 0.3308 - val_acc: 0.8856\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.0555 - acc: 0.9948 - val_loss: 0.3288 - val_acc: 0.8879\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.0544 - acc: 0.9963 - val_loss: 0.3407 - val_acc: 0.8895\n",
      "Epoch 473/1000\n",
      " - 0s - loss: 0.0545 - acc: 0.9958 - val_loss: 0.3407 - val_acc: 0.8817\n",
      "Epoch 474/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0534 - acc: 0.9963 - val_loss: 0.3383 - val_acc: 0.8824\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.0540 - acc: 0.9958 - val_loss: 0.3289 - val_acc: 0.8871\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.0530 - acc: 0.9963 - val_loss: 0.3465 - val_acc: 0.8738\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.0537 - acc: 0.9969 - val_loss: 0.3324 - val_acc: 0.8809\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.0531 - acc: 0.9969 - val_loss: 0.3309 - val_acc: 0.8840\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 0.0518 - acc: 0.9958 - val_loss: 0.3322 - val_acc: 0.8864\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 0.0523 - acc: 0.9969 - val_loss: 0.3355 - val_acc: 0.8840\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 0.0518 - acc: 0.9974 - val_loss: 0.3416 - val_acc: 0.8879\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 0.0519 - acc: 0.9958 - val_loss: 0.3353 - val_acc: 0.8793\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 0.0516 - acc: 0.9969 - val_loss: 0.4276 - val_acc: 0.8676\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 0.0537 - acc: 0.9958 - val_loss: 0.3301 - val_acc: 0.8864\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 0.0504 - acc: 0.9969 - val_loss: 0.3556 - val_acc: 0.8879\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 0.0520 - acc: 0.9974 - val_loss: 0.3300 - val_acc: 0.8871\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 0.0499 - acc: 0.9969 - val_loss: 0.3317 - val_acc: 0.8832\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 0.0498 - acc: 0.9974 - val_loss: 0.3337 - val_acc: 0.8864\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 0.0490 - acc: 0.9963 - val_loss: 0.3558 - val_acc: 0.8738\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 0.0504 - acc: 0.9974 - val_loss: 0.3316 - val_acc: 0.8840\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 0.0490 - acc: 0.9963 - val_loss: 0.3353 - val_acc: 0.8887\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 0.0490 - acc: 0.9974 - val_loss: 0.3322 - val_acc: 0.8817\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 0.0482 - acc: 0.9969 - val_loss: 0.3518 - val_acc: 0.8746\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 0.0484 - acc: 0.9958 - val_loss: 0.3362 - val_acc: 0.8864\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 0.0480 - acc: 0.9963 - val_loss: 0.3453 - val_acc: 0.8817\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 0.0486 - acc: 0.9979 - val_loss: 0.3344 - val_acc: 0.8887\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 0.0471 - acc: 0.9974 - val_loss: 0.3401 - val_acc: 0.8824\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 0.0469 - acc: 0.9974 - val_loss: 0.3441 - val_acc: 0.8801\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 0.0473 - acc: 0.9979 - val_loss: 0.3448 - val_acc: 0.8895\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 0.0465 - acc: 0.9984 - val_loss: 0.3340 - val_acc: 0.8848\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 0.0468 - acc: 0.9969 - val_loss: 0.3430 - val_acc: 0.8817\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 0.0462 - acc: 0.9969 - val_loss: 0.3349 - val_acc: 0.8887\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 0.0457 - acc: 0.9979 - val_loss: 0.3349 - val_acc: 0.8879\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 0.0456 - acc: 0.9984 - val_loss: 0.3533 - val_acc: 0.8738\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 0.0458 - acc: 0.9979 - val_loss: 0.3345 - val_acc: 0.8832\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 0.0448 - acc: 0.9979 - val_loss: 0.3392 - val_acc: 0.8832\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 0.0446 - acc: 0.9984 - val_loss: 0.3567 - val_acc: 0.8644\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 0.0451 - acc: 0.9979 - val_loss: 0.3384 - val_acc: 0.8887\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 0.0445 - acc: 0.9979 - val_loss: 0.3352 - val_acc: 0.8848\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 0.0440 - acc: 0.9974 - val_loss: 0.4136 - val_acc: 0.8440\n",
      "Epoch 511/1000\n",
      " - 0s - loss: 0.0474 - acc: 0.9963 - val_loss: 0.3458 - val_acc: 0.8887\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 0.0440 - acc: 0.9974 - val_loss: 0.3381 - val_acc: 0.8895\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 0.0438 - acc: 0.9984 - val_loss: 0.3540 - val_acc: 0.8715\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 0.0438 - acc: 0.9979 - val_loss: 0.3861 - val_acc: 0.8582\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 0.0445 - acc: 0.9979 - val_loss: 0.3389 - val_acc: 0.8832\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 0.0431 - acc: 0.9984 - val_loss: 0.3365 - val_acc: 0.8856\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 0.0421 - acc: 0.9979 - val_loss: 0.3409 - val_acc: 0.8809\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 0.0425 - acc: 0.9979 - val_loss: 0.3433 - val_acc: 0.8879\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 0.0416 - acc: 0.9979 - val_loss: 0.3487 - val_acc: 0.8817\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 0.0418 - acc: 0.9984 - val_loss: 0.3412 - val_acc: 0.8848\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 0.0412 - acc: 0.9974 - val_loss: 0.3459 - val_acc: 0.8871\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 0.0421 - acc: 0.9969 - val_loss: 0.3456 - val_acc: 0.8793\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 0.0409 - acc: 0.9979 - val_loss: 0.3473 - val_acc: 0.8785\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 0.0408 - acc: 0.9979 - val_loss: 0.3402 - val_acc: 0.8840\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 0.0412 - acc: 0.9984 - val_loss: 0.3405 - val_acc: 0.8895\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 0.0403 - acc: 0.9974 - val_loss: 0.3436 - val_acc: 0.8809\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 0.0400 - acc: 0.9984 - val_loss: 0.3477 - val_acc: 0.8809\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 0.0400 - acc: 0.9984 - val_loss: 0.3820 - val_acc: 0.8527\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 0.0403 - acc: 0.9984 - val_loss: 0.3478 - val_acc: 0.8879\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 0.0397 - acc: 0.9984 - val_loss: 0.3408 - val_acc: 0.8832\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 0.0390 - acc: 0.9984 - val_loss: 0.3411 - val_acc: 0.8879\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 0.0392 - acc: 0.9974 - val_loss: 0.3420 - val_acc: 0.8864\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 0.0392 - acc: 0.9984 - val_loss: 0.3405 - val_acc: 0.8856\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 0.0382 - acc: 0.9984 - val_loss: 0.3437 - val_acc: 0.8824\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 0.0389 - acc: 0.9984 - val_loss: 0.3420 - val_acc: 0.8848\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 0.0384 - acc: 0.9984 - val_loss: 0.3471 - val_acc: 0.8801\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 0.0383 - acc: 0.9979 - val_loss: 0.3419 - val_acc: 0.8856\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 0.0376 - acc: 0.9984 - val_loss: 0.3423 - val_acc: 0.8848\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 0.0378 - acc: 0.9984 - val_loss: 0.3442 - val_acc: 0.8832\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 0.0375 - acc: 0.9984 - val_loss: 0.3467 - val_acc: 0.8840\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 0.0370 - acc: 0.9979 - val_loss: 0.3574 - val_acc: 0.8730\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 0.0377 - acc: 0.9984 - val_loss: 0.3556 - val_acc: 0.8848\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 0.0373 - acc: 0.9979 - val_loss: 0.3558 - val_acc: 0.8824\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 0.0369 - acc: 0.9974 - val_loss: 0.3423 - val_acc: 0.8824\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 0.0362 - acc: 0.9984 - val_loss: 0.3442 - val_acc: 0.8848\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 0.0368 - acc: 0.9979 - val_loss: 0.3556 - val_acc: 0.8801\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 0.0364 - acc: 0.9979 - val_loss: 0.3450 - val_acc: 0.8864\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 0.0358 - acc: 0.9990 - val_loss: 0.3482 - val_acc: 0.8848\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 0.0359 - acc: 0.9984 - val_loss: 0.3455 - val_acc: 0.8840\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 0.0357 - acc: 0.9979 - val_loss: 0.3451 - val_acc: 0.8840\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 0.0357 - acc: 0.9979 - val_loss: 0.3448 - val_acc: 0.8840\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 0.0350 - acc: 0.9984 - val_loss: 0.3482 - val_acc: 0.8777\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 0.0350 - acc: 0.9979 - val_loss: 0.3503 - val_acc: 0.8856\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 0.0350 - acc: 0.9984 - val_loss: 0.3473 - val_acc: 0.8856\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 0.0346 - acc: 0.9990 - val_loss: 0.3485 - val_acc: 0.8856\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 0.0347 - acc: 0.9979 - val_loss: 0.3471 - val_acc: 0.8801\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 0.0348 - acc: 0.9979 - val_loss: 0.3598 - val_acc: 0.8801\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 0.0344 - acc: 0.9984 - val_loss: 0.3452 - val_acc: 0.8832\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 0.0338 - acc: 0.9984 - val_loss: 0.3475 - val_acc: 0.8848\n",
      "Epoch 560/1000\n",
      " - 0s - loss: 0.0336 - acc: 0.9984 - val_loss: 0.3522 - val_acc: 0.8809\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 0.0336 - acc: 0.9984 - val_loss: 0.3550 - val_acc: 0.8793\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 0.0334 - acc: 0.9984 - val_loss: 0.3499 - val_acc: 0.8832\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 0.0329 - acc: 0.9984 - val_loss: 0.3536 - val_acc: 0.8856\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 0.0332 - acc: 0.9984 - val_loss: 0.3878 - val_acc: 0.8832\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 0.0340 - acc: 0.9979 - val_loss: 0.3525 - val_acc: 0.8824\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 0.0326 - acc: 0.9984 - val_loss: 0.3557 - val_acc: 0.8809\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 0.0326 - acc: 0.9984 - val_loss: 0.3488 - val_acc: 0.8840\n",
      "Epoch 568/1000\n",
      " - 0s - loss: 0.0324 - acc: 0.9984 - val_loss: 0.3539 - val_acc: 0.8864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569/1000\n",
      " - 0s - loss: 0.0323 - acc: 0.9984 - val_loss: 0.3488 - val_acc: 0.8817\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 0.0324 - acc: 0.9984 - val_loss: 0.3513 - val_acc: 0.8856\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 0.0320 - acc: 0.9984 - val_loss: 0.3519 - val_acc: 0.8832\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 0.0320 - acc: 0.9979 - val_loss: 0.3502 - val_acc: 0.8809\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 0.0315 - acc: 0.9990 - val_loss: 0.3489 - val_acc: 0.8824\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 0.0316 - acc: 0.9990 - val_loss: 0.3497 - val_acc: 0.8824\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 0.0311 - acc: 0.9984 - val_loss: 0.3945 - val_acc: 0.8574\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 0.0327 - acc: 0.9979 - val_loss: 0.3514 - val_acc: 0.8824\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 0.0309 - acc: 0.9979 - val_loss: 0.3525 - val_acc: 0.8809\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 0.0312 - acc: 0.9979 - val_loss: 0.3498 - val_acc: 0.8832\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 0.0305 - acc: 0.9979 - val_loss: 0.3525 - val_acc: 0.8832\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 0.0309 - acc: 0.9984 - val_loss: 0.3532 - val_acc: 0.8848\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 0.0304 - acc: 0.9990 - val_loss: 0.3515 - val_acc: 0.8824\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 0.0303 - acc: 0.9984 - val_loss: 0.3508 - val_acc: 0.8832\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 0.0300 - acc: 0.9984 - val_loss: 0.3517 - val_acc: 0.8817\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 0.0305 - acc: 0.9984 - val_loss: 0.3522 - val_acc: 0.8848\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 0.0299 - acc: 0.9984 - val_loss: 0.3851 - val_acc: 0.8848\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 0.0303 - acc: 0.9990 - val_loss: 0.3548 - val_acc: 0.8871\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 0.0296 - acc: 0.9984 - val_loss: 0.3537 - val_acc: 0.8832\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 0.0295 - acc: 0.9990 - val_loss: 0.3638 - val_acc: 0.8840\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 0.0294 - acc: 0.9984 - val_loss: 0.3780 - val_acc: 0.8723\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 0.0299 - acc: 0.9984 - val_loss: 0.3542 - val_acc: 0.8848\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 0.0288 - acc: 0.9979 - val_loss: 0.3566 - val_acc: 0.8832\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 0.0288 - acc: 0.9990 - val_loss: 0.3684 - val_acc: 0.8824\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 0.0289 - acc: 0.9990 - val_loss: 0.3566 - val_acc: 0.8832\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 0.0287 - acc: 0.9984 - val_loss: 0.3557 - val_acc: 0.8832\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 0.0291 - acc: 0.9979 - val_loss: 0.3573 - val_acc: 0.8824\n",
      "Epoch 596/1000\n",
      " - 0s - loss: 0.0283 - acc: 0.9984 - val_loss: 0.3696 - val_acc: 0.8715\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 0.0286 - acc: 0.9990 - val_loss: 0.3584 - val_acc: 0.8832\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 0.0280 - acc: 0.9990 - val_loss: 0.3609 - val_acc: 0.8856\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 0.0282 - acc: 0.9990 - val_loss: 0.3582 - val_acc: 0.8848\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 0.0276 - acc: 0.9984 - val_loss: 0.3578 - val_acc: 0.8832\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 0.0277 - acc: 0.9990 - val_loss: 0.3570 - val_acc: 0.8848\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 0.0274 - acc: 0.9979 - val_loss: 0.3649 - val_acc: 0.8848\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 0.0275 - acc: 0.9990 - val_loss: 0.3573 - val_acc: 0.8809\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 0.0272 - acc: 0.9984 - val_loss: 0.3617 - val_acc: 0.8793\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 0.0272 - acc: 0.9984 - val_loss: 0.3617 - val_acc: 0.8762\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 0.0270 - acc: 0.9990 - val_loss: 0.3589 - val_acc: 0.8824\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 0.0269 - acc: 0.9990 - val_loss: 0.3573 - val_acc: 0.8832\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 0.0267 - acc: 0.9990 - val_loss: 0.3567 - val_acc: 0.8832\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 0.0266 - acc: 0.9990 - val_loss: 0.3573 - val_acc: 0.8824\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 0.0264 - acc: 0.9990 - val_loss: 0.3577 - val_acc: 0.8824\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 0.0262 - acc: 0.9984 - val_loss: 0.3566 - val_acc: 0.8817\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 0.0261 - acc: 0.9990 - val_loss: 0.3587 - val_acc: 0.8832\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 0.0262 - acc: 0.9984 - val_loss: 0.3611 - val_acc: 0.8824\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 0.0258 - acc: 0.9990 - val_loss: 0.3617 - val_acc: 0.8856\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 0.0256 - acc: 0.9990 - val_loss: 0.3609 - val_acc: 0.8817\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 0.0256 - acc: 0.9990 - val_loss: 0.3609 - val_acc: 0.8793\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 0.0256 - acc: 0.9995 - val_loss: 0.3582 - val_acc: 0.8832\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 0.0254 - acc: 0.9984 - val_loss: 0.3619 - val_acc: 0.8817\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 0.0253 - acc: 0.9984 - val_loss: 0.3611 - val_acc: 0.8832\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 0.0253 - acc: 0.9990 - val_loss: 0.4166 - val_acc: 0.8801\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 0.0268 - acc: 0.9990 - val_loss: 0.3628 - val_acc: 0.8809\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 0.0254 - acc: 0.9990 - val_loss: 0.3662 - val_acc: 0.8809\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 0.0250 - acc: 0.9990 - val_loss: 0.3600 - val_acc: 0.8824\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 0.0249 - acc: 0.9990 - val_loss: 0.3773 - val_acc: 0.8699\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 0.0253 - acc: 0.9990 - val_loss: 0.3673 - val_acc: 0.8785\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 0.0248 - acc: 0.9984 - val_loss: 0.3635 - val_acc: 0.8817\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 0.0246 - acc: 0.9990 - val_loss: 0.3642 - val_acc: 0.8832\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 0.0243 - acc: 0.9984 - val_loss: 0.3641 - val_acc: 0.8809\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 0.0248 - acc: 0.9984 - val_loss: 0.3617 - val_acc: 0.8824\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 0.0245 - acc: 0.9984 - val_loss: 0.3697 - val_acc: 0.8793\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 0.0241 - acc: 0.9984 - val_loss: 0.3661 - val_acc: 0.8809\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 0.0240 - acc: 0.9984 - val_loss: 0.3692 - val_acc: 0.8879\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 0.0238 - acc: 0.9990 - val_loss: 0.3642 - val_acc: 0.8840\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 0.0237 - acc: 0.9990 - val_loss: 0.3643 - val_acc: 0.8817\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 0.0237 - acc: 0.9990 - val_loss: 0.3621 - val_acc: 0.8817\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 0.0235 - acc: 0.9990 - val_loss: 0.3625 - val_acc: 0.8809\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 0.0235 - acc: 0.9990 - val_loss: 0.3620 - val_acc: 0.8817\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 0.0232 - acc: 0.9990 - val_loss: 0.3629 - val_acc: 0.8824\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 0.0232 - acc: 0.9984 - val_loss: 0.3653 - val_acc: 0.8793\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 0.0232 - acc: 0.9984 - val_loss: 0.3668 - val_acc: 0.8848\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 0.0229 - acc: 0.9990 - val_loss: 0.3654 - val_acc: 0.8777\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 0.0227 - acc: 0.9990 - val_loss: 0.3922 - val_acc: 0.8770\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 0.0231 - acc: 0.9984 - val_loss: 0.3693 - val_acc: 0.8793\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 0.0228 - acc: 0.9990 - val_loss: 0.3760 - val_acc: 0.8848\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 0.0228 - acc: 0.9984 - val_loss: 0.3722 - val_acc: 0.8785\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 0.0225 - acc: 0.9990 - val_loss: 0.3663 - val_acc: 0.8809\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 0.0223 - acc: 0.9984 - val_loss: 0.3643 - val_acc: 0.8832\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 0.0224 - acc: 0.9990 - val_loss: 0.3676 - val_acc: 0.8777\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 0.0222 - acc: 0.9990 - val_loss: 0.3667 - val_acc: 0.8848\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 0.0221 - acc: 0.9990 - val_loss: 0.3676 - val_acc: 0.8840\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 0.0221 - acc: 0.9990 - val_loss: 0.3660 - val_acc: 0.8840\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 0.0219 - acc: 0.9984 - val_loss: 0.3676 - val_acc: 0.8801\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 0.0215 - acc: 0.9984 - val_loss: 0.3670 - val_acc: 0.8824\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 0.0219 - acc: 0.9990 - val_loss: 0.3656 - val_acc: 0.8809\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 0.0216 - acc: 0.9990 - val_loss: 0.3653 - val_acc: 0.8817\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 0.0214 - acc: 0.9990 - val_loss: 0.3657 - val_acc: 0.8801\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 0.0213 - acc: 0.9984 - val_loss: 0.3665 - val_acc: 0.8817\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 0.0214 - acc: 0.9984 - val_loss: 0.3679 - val_acc: 0.8817\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 0.0212 - acc: 0.9990 - val_loss: 0.3684 - val_acc: 0.8817\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 0.0211 - acc: 0.9984 - val_loss: 0.3665 - val_acc: 0.8817\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 0.0209 - acc: 0.9990 - val_loss: 0.3684 - val_acc: 0.8817\n",
      "Epoch 662/1000\n",
      " - 0s - loss: 0.0209 - acc: 0.9984 - val_loss: 0.4451 - val_acc: 0.8456\n",
      "Epoch 663/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0231 - acc: 0.9984 - val_loss: 0.3743 - val_acc: 0.8832\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 0.0209 - acc: 0.9990 - val_loss: 0.3799 - val_acc: 0.8754\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 0.0210 - acc: 0.9990 - val_loss: 0.3672 - val_acc: 0.8824\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 0.0207 - acc: 0.9990 - val_loss: 0.3691 - val_acc: 0.8832\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 0.0205 - acc: 0.9990 - val_loss: 0.3700 - val_acc: 0.8824\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 0.0205 - acc: 0.9990 - val_loss: 0.3676 - val_acc: 0.8801\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 0.0205 - acc: 0.9984 - val_loss: 0.3725 - val_acc: 0.8809\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 0.0202 - acc: 0.9990 - val_loss: 0.3704 - val_acc: 0.8824\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 0.0203 - acc: 0.9984 - val_loss: 0.3703 - val_acc: 0.8840\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 0.0197 - acc: 0.9990 - val_loss: 0.3718 - val_acc: 0.8848\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 0.0201 - acc: 0.9984 - val_loss: 0.3714 - val_acc: 0.8785\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 0.0200 - acc: 0.9984 - val_loss: 0.3709 - val_acc: 0.8848\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 0.0198 - acc: 0.9990 - val_loss: 0.3753 - val_acc: 0.8793\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 0.0197 - acc: 0.9984 - val_loss: 0.3723 - val_acc: 0.8840\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 0.0197 - acc: 0.9984 - val_loss: 0.3715 - val_acc: 0.8840\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 0.0197 - acc: 0.9990 - val_loss: 0.3712 - val_acc: 0.8840\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 0.0195 - acc: 0.9984 - val_loss: 0.3762 - val_acc: 0.8824\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 0.0194 - acc: 0.9990 - val_loss: 0.3790 - val_acc: 0.8840\n",
      "Epoch 681/1000\n",
      " - 0s - loss: 0.0193 - acc: 0.9990 - val_loss: 0.3698 - val_acc: 0.8817\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 0.0192 - acc: 0.9990 - val_loss: 0.3704 - val_acc: 0.8809\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 0.0191 - acc: 0.9984 - val_loss: 0.3807 - val_acc: 0.8817\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 0.0192 - acc: 0.9990 - val_loss: 0.3744 - val_acc: 0.8801\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 0.0189 - acc: 0.9995 - val_loss: 0.3808 - val_acc: 0.8785\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 0.0191 - acc: 0.9990 - val_loss: 0.3727 - val_acc: 0.8840\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 0.0189 - acc: 0.9990 - val_loss: 0.3770 - val_acc: 0.8770\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 0.0189 - acc: 0.9984 - val_loss: 0.3716 - val_acc: 0.8809\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 0.0187 - acc: 0.9995 - val_loss: 0.3751 - val_acc: 0.8785\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 0.0185 - acc: 0.9995 - val_loss: 0.3795 - val_acc: 0.8817\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 0.0186 - acc: 0.9990 - val_loss: 0.3735 - val_acc: 0.8824\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 0.0186 - acc: 0.9995 - val_loss: 0.3757 - val_acc: 0.8840\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 0.0184 - acc: 0.9990 - val_loss: 0.3717 - val_acc: 0.8793\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 0.0185 - acc: 0.9990 - val_loss: 0.3772 - val_acc: 0.8793\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 0.0183 - acc: 0.9995 - val_loss: 0.3752 - val_acc: 0.8809\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 0.0183 - acc: 0.9990 - val_loss: 0.3746 - val_acc: 0.8817\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 0.0181 - acc: 0.9990 - val_loss: 0.3762 - val_acc: 0.8801\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 0.0181 - acc: 0.9990 - val_loss: 0.3770 - val_acc: 0.8793\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 0.0179 - acc: 0.9995 - val_loss: 0.3742 - val_acc: 0.8824\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 0.0179 - acc: 0.9990 - val_loss: 0.3805 - val_acc: 0.8840\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 0.0181 - acc: 0.9990 - val_loss: 0.3764 - val_acc: 0.8793\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 0.0177 - acc: 0.9995 - val_loss: 0.3849 - val_acc: 0.8801\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 0.0179 - acc: 0.9995 - val_loss: 0.3745 - val_acc: 0.8809\n",
      "Epoch 704/1000\n",
      " - 0s - loss: 0.0176 - acc: 0.9990 - val_loss: 0.3748 - val_acc: 0.8832\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 0.0177 - acc: 0.9990 - val_loss: 0.3757 - val_acc: 0.8817\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 0.0175 - acc: 0.9990 - val_loss: 0.3755 - val_acc: 0.8832\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 0.0174 - acc: 0.9990 - val_loss: 0.3809 - val_acc: 0.8832\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 0.0174 - acc: 0.9995 - val_loss: 0.3743 - val_acc: 0.8824\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 0.0175 - acc: 0.9995 - val_loss: 0.3763 - val_acc: 0.8832\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 0.0173 - acc: 0.9995 - val_loss: 0.3773 - val_acc: 0.8848\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 0.0173 - acc: 0.9990 - val_loss: 0.3763 - val_acc: 0.8824\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 0.0171 - acc: 0.9995 - val_loss: 0.3774 - val_acc: 0.8817\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 0.0171 - acc: 0.9990 - val_loss: 0.3867 - val_acc: 0.8817\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 0.0172 - acc: 0.9990 - val_loss: 0.3777 - val_acc: 0.8840\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 0.0170 - acc: 0.9995 - val_loss: 0.3800 - val_acc: 0.8801\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 0.0166 - acc: 0.9995 - val_loss: 0.3867 - val_acc: 0.8809\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 0.0168 - acc: 0.9990 - val_loss: 0.3802 - val_acc: 0.8824\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 0.0169 - acc: 0.9990 - val_loss: 0.3802 - val_acc: 0.8824\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 0.0165 - acc: 0.9995 - val_loss: 0.3784 - val_acc: 0.8856\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 0.0167 - acc: 0.9990 - val_loss: 0.3807 - val_acc: 0.8801\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 0.0166 - acc: 0.9990 - val_loss: 0.3792 - val_acc: 0.8848\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 0.0166 - acc: 0.9990 - val_loss: 0.3814 - val_acc: 0.8824\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 0.0166 - acc: 0.9990 - val_loss: 0.3823 - val_acc: 0.8809\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 0.0162 - acc: 0.9995 - val_loss: 0.3812 - val_acc: 0.8840\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 0.0166 - acc: 0.9990 - val_loss: 0.3779 - val_acc: 0.8824\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 0.0161 - acc: 0.9995 - val_loss: 0.3800 - val_acc: 0.8817\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 0.0164 - acc: 0.9995 - val_loss: 0.3791 - val_acc: 0.8848\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 0.0163 - acc: 0.9990 - val_loss: 0.3787 - val_acc: 0.8832\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 0.0161 - acc: 0.9990 - val_loss: 0.3809 - val_acc: 0.8840\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 0.0160 - acc: 0.9995 - val_loss: 0.3834 - val_acc: 0.8824\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 0.0160 - acc: 0.9990 - val_loss: 0.3839 - val_acc: 0.8817\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 0.0160 - acc: 0.9995 - val_loss: 0.3808 - val_acc: 0.8817\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 0.0159 - acc: 0.9990 - val_loss: 0.3834 - val_acc: 0.8809\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 0.0157 - acc: 0.9990 - val_loss: 0.3835 - val_acc: 0.8793\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 0.0157 - acc: 0.9995 - val_loss: 0.3821 - val_acc: 0.8785\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 0.0157 - acc: 0.9990 - val_loss: 0.4037 - val_acc: 0.8746\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 0.0160 - acc: 0.9995 - val_loss: 0.3851 - val_acc: 0.8801\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 0.0156 - acc: 0.9990 - val_loss: 0.3819 - val_acc: 0.8840\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 0.0154 - acc: 0.9995 - val_loss: 0.3871 - val_acc: 0.8801\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 0.0154 - acc: 0.9990 - val_loss: 0.4516 - val_acc: 0.8480\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 0.0196 - acc: 0.9995 - val_loss: 0.3808 - val_acc: 0.8817\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 0.0155 - acc: 0.9995 - val_loss: 0.3812 - val_acc: 0.8824\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 0.0154 - acc: 0.9995 - val_loss: 0.3855 - val_acc: 0.8809\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 0.0154 - acc: 0.9995 - val_loss: 0.3837 - val_acc: 0.8793\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 0.0152 - acc: 0.9995 - val_loss: 0.3807 - val_acc: 0.8832\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 0.0150 - acc: 0.9995 - val_loss: 0.3912 - val_acc: 0.8793\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 0.0151 - acc: 0.9995 - val_loss: 0.3835 - val_acc: 0.8817\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 0.0150 - acc: 0.9995 - val_loss: 0.3841 - val_acc: 0.8840\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 0.0150 - acc: 0.9995 - val_loss: 0.3839 - val_acc: 0.8793\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 0.0149 - acc: 0.9990 - val_loss: 0.3840 - val_acc: 0.8809\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 0.0149 - acc: 0.9990 - val_loss: 0.3814 - val_acc: 0.8785\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 0.0148 - acc: 0.9995 - val_loss: 0.3873 - val_acc: 0.8801\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 0.0149 - acc: 0.9995 - val_loss: 0.3904 - val_acc: 0.8824\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 0.0149 - acc: 0.9995 - val_loss: 0.3833 - val_acc: 0.8809\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 0.0146 - acc: 0.9995 - val_loss: 0.3854 - val_acc: 0.8809\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 0.0148 - acc: 0.9995 - val_loss: 0.3856 - val_acc: 0.8801\n",
      "Epoch 757/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9995 - val_loss: 0.3857 - val_acc: 0.8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 758/1000\n",
      " - 0s - loss: 0.0145 - acc: 0.9990 - val_loss: 0.3850 - val_acc: 0.8824\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9995 - val_loss: 0.3828 - val_acc: 0.8809\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 0.0147 - acc: 0.9990 - val_loss: 0.3828 - val_acc: 0.8809\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9995 - val_loss: 0.3840 - val_acc: 0.8832\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9995 - val_loss: 0.3869 - val_acc: 0.8809\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 0.0142 - acc: 0.9995 - val_loss: 0.3851 - val_acc: 0.8824\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 0.0142 - acc: 0.9995 - val_loss: 0.3886 - val_acc: 0.8840\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 0.0144 - acc: 0.9990 - val_loss: 0.3855 - val_acc: 0.8809\n",
      "Epoch 766/1000\n",
      " - 0s - loss: 0.0141 - acc: 0.9995 - val_loss: 0.3844 - val_acc: 0.8817\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 0.0141 - acc: 0.9995 - val_loss: 0.3867 - val_acc: 0.8824\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 0.0140 - acc: 0.9995 - val_loss: 0.3903 - val_acc: 0.8840\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 0.0141 - acc: 0.9995 - val_loss: 0.3884 - val_acc: 0.8793\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 0.0138 - acc: 0.9995 - val_loss: 0.3881 - val_acc: 0.8809\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 0.0140 - acc: 0.9990 - val_loss: 0.3865 - val_acc: 0.8817\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 0.0139 - acc: 0.9990 - val_loss: 0.3868 - val_acc: 0.8824\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 0.0139 - acc: 0.9990 - val_loss: 0.3849 - val_acc: 0.8801\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 0.0139 - acc: 0.9990 - val_loss: 0.3878 - val_acc: 0.8801\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 0.0138 - acc: 0.9990 - val_loss: 0.3871 - val_acc: 0.8840\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 0.0137 - acc: 0.9990 - val_loss: 0.3913 - val_acc: 0.8824\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 0.0136 - acc: 0.9990 - val_loss: 0.4711 - val_acc: 0.8762\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 0.0158 - acc: 0.9995 - val_loss: 0.3896 - val_acc: 0.8832\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 0.0140 - acc: 0.9995 - val_loss: 0.3881 - val_acc: 0.8817\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 0.0135 - acc: 0.9995 - val_loss: 0.3945 - val_acc: 0.8801\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 0.0136 - acc: 0.9995 - val_loss: 0.3887 - val_acc: 0.8817\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 0.0135 - acc: 0.9995 - val_loss: 0.3948 - val_acc: 0.8785\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 0.0135 - acc: 0.9995 - val_loss: 0.3892 - val_acc: 0.8824\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 0.0135 - acc: 0.9990 - val_loss: 0.3919 - val_acc: 0.8817\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 0.0133 - acc: 0.9990 - val_loss: 0.3902 - val_acc: 0.8817\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 0.0132 - acc: 0.9995 - val_loss: 0.3898 - val_acc: 0.8793\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 0.0133 - acc: 0.9990 - val_loss: 0.3943 - val_acc: 0.8785\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 0.3909 - val_acc: 0.8785\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 0.0132 - acc: 0.9990 - val_loss: 0.3908 - val_acc: 0.8793\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 0.0131 - acc: 0.9990 - val_loss: 0.3895 - val_acc: 0.8824\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 0.0130 - acc: 0.9995 - val_loss: 0.3884 - val_acc: 0.8809\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 0.0130 - acc: 0.9995 - val_loss: 0.3922 - val_acc: 0.8793\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 0.0130 - acc: 0.9995 - val_loss: 0.3897 - val_acc: 0.8809\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 0.0128 - acc: 0.9990 - val_loss: 0.3918 - val_acc: 0.8824\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 0.0129 - acc: 0.9995 - val_loss: 0.3902 - val_acc: 0.8817\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 0.0129 - acc: 0.9990 - val_loss: 0.3968 - val_acc: 0.8777\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 0.0129 - acc: 0.9995 - val_loss: 0.3911 - val_acc: 0.8817\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 0.0127 - acc: 0.9990 - val_loss: 0.3946 - val_acc: 0.8785\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 0.0127 - acc: 0.9995 - val_loss: 0.3914 - val_acc: 0.8801\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 0.0127 - acc: 0.9995 - val_loss: 0.3940 - val_acc: 0.8801\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 0.0127 - acc: 0.9990 - val_loss: 0.3944 - val_acc: 0.8801\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 0.0128 - acc: 0.9995 - val_loss: 0.3913 - val_acc: 0.8817\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 0.0125 - acc: 0.9995 - val_loss: 0.3926 - val_acc: 0.8809\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 0.0126 - acc: 0.9995 - val_loss: 0.3965 - val_acc: 0.8793\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 0.0128 - acc: 0.9995 - val_loss: 0.3924 - val_acc: 0.8824\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 0.0125 - acc: 0.9990 - val_loss: 0.3908 - val_acc: 0.8817\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 0.0124 - acc: 0.9990 - val_loss: 0.3941 - val_acc: 0.8801\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 0.0124 - acc: 0.9990 - val_loss: 0.3942 - val_acc: 0.8809\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 0.0124 - acc: 0.9990 - val_loss: 0.3972 - val_acc: 0.8793\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 0.0123 - acc: 0.9995 - val_loss: 0.3917 - val_acc: 0.8817\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9995 - val_loss: 0.3916 - val_acc: 0.8824\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3930 - val_acc: 0.8809\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3917 - val_acc: 0.8817\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 0.0121 - acc: 0.9995 - val_loss: 0.3934 - val_acc: 0.8824\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 0.0123 - acc: 0.9995 - val_loss: 0.3955 - val_acc: 0.8809\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 0.0121 - acc: 0.9995 - val_loss: 0.3925 - val_acc: 0.8824\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 0.0120 - acc: 0.9995 - val_loss: 0.3940 - val_acc: 0.8824\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 0.0120 - acc: 0.9995 - val_loss: 0.3928 - val_acc: 0.8801\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 0.0120 - acc: 0.9995 - val_loss: 0.3946 - val_acc: 0.8817\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 0.0120 - acc: 0.9990 - val_loss: 0.3930 - val_acc: 0.8793\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 0.0120 - acc: 0.9995 - val_loss: 0.3954 - val_acc: 0.8809\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 0.0119 - acc: 0.9995 - val_loss: 0.3957 - val_acc: 0.8824\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9995 - val_loss: 0.3955 - val_acc: 0.8824\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9995 - val_loss: 0.3956 - val_acc: 0.8801\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 0.0116 - acc: 0.9995 - val_loss: 0.3934 - val_acc: 0.8809\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9995 - val_loss: 0.3987 - val_acc: 0.8809\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9995 - val_loss: 0.3968 - val_acc: 0.8793\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 0.0118 - acc: 0.9990 - val_loss: 0.3958 - val_acc: 0.8817\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9995 - val_loss: 0.3965 - val_acc: 0.8801\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9995 - val_loss: 0.3939 - val_acc: 0.8809\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9995 - val_loss: 0.3944 - val_acc: 0.8809\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9990 - val_loss: 0.3976 - val_acc: 0.8817\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 0.0117 - acc: 0.9995 - val_loss: 0.3953 - val_acc: 0.8809\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 0.0115 - acc: 0.9990 - val_loss: 0.3979 - val_acc: 0.8809\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 0.0114 - acc: 0.9995 - val_loss: 0.3959 - val_acc: 0.8824\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 0.0115 - acc: 0.9995 - val_loss: 0.3981 - val_acc: 0.8793\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 0.0113 - acc: 0.9995 - val_loss: 0.3958 - val_acc: 0.8809\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 0.0113 - acc: 0.9995 - val_loss: 0.3957 - val_acc: 0.8817\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 0.0114 - acc: 0.9990 - val_loss: 0.3955 - val_acc: 0.8793\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 0.0113 - acc: 0.9995 - val_loss: 0.4023 - val_acc: 0.8817\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 0.0113 - acc: 0.9995 - val_loss: 0.3967 - val_acc: 0.8824\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 0.0115 - acc: 0.9990 - val_loss: 0.3964 - val_acc: 0.8809\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 0.0112 - acc: 0.9995 - val_loss: 0.3963 - val_acc: 0.8801\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 0.0112 - acc: 0.9990 - val_loss: 0.4030 - val_acc: 0.8801\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 0.0113 - acc: 0.9995 - val_loss: 0.4037 - val_acc: 0.8785\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 0.0112 - acc: 0.9995 - val_loss: 0.3993 - val_acc: 0.8801\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 0.0111 - acc: 0.9995 - val_loss: 0.4019 - val_acc: 0.8793\n",
      "Epoch 848/1000\n",
      " - 0s - loss: 0.0111 - acc: 0.9990 - val_loss: 0.4008 - val_acc: 0.8793\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 0.0110 - acc: 0.9995 - val_loss: 0.3996 - val_acc: 0.8801\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 0.0109 - acc: 0.9995 - val_loss: 0.3987 - val_acc: 0.8824\n",
      "Epoch 851/1000\n",
      " - 0s - loss: 0.0110 - acc: 0.9995 - val_loss: 0.3975 - val_acc: 0.8817\n",
      "Epoch 852/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0110 - acc: 0.9990 - val_loss: 0.3991 - val_acc: 0.8832\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 0.0109 - acc: 0.9995 - val_loss: 0.3974 - val_acc: 0.8824\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 0.0108 - acc: 0.9995 - val_loss: 0.3982 - val_acc: 0.8832\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 0.0110 - acc: 0.9990 - val_loss: 0.3969 - val_acc: 0.8817\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 0.0108 - acc: 0.9990 - val_loss: 0.4002 - val_acc: 0.8809\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 0.0108 - acc: 0.9995 - val_loss: 0.4000 - val_acc: 0.8793\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 0.0107 - acc: 0.9995 - val_loss: 0.4026 - val_acc: 0.8785\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 0.0108 - acc: 0.9995 - val_loss: 0.4018 - val_acc: 0.8793\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 0.0109 - acc: 0.9990 - val_loss: 0.3974 - val_acc: 0.8832\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 0.0107 - acc: 0.9995 - val_loss: 0.3973 - val_acc: 0.8793\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 0.0107 - acc: 0.9995 - val_loss: 0.4009 - val_acc: 0.8809\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 0.0106 - acc: 0.9995 - val_loss: 0.4024 - val_acc: 0.8801\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 0.0106 - acc: 0.9990 - val_loss: 0.4006 - val_acc: 0.8824\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 0.0106 - acc: 0.9990 - val_loss: 0.3978 - val_acc: 0.8801\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 0.0106 - acc: 0.9990 - val_loss: 0.3999 - val_acc: 0.8817\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 0.0106 - acc: 0.9990 - val_loss: 0.4025 - val_acc: 0.8793\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 0.0104 - acc: 0.9995 - val_loss: 0.3984 - val_acc: 0.8809\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 0.0105 - acc: 0.9990 - val_loss: 0.3993 - val_acc: 0.8824\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 0.0104 - acc: 0.9995 - val_loss: 0.4001 - val_acc: 0.8817\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 0.0103 - acc: 0.9995 - val_loss: 0.3996 - val_acc: 0.8824\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 0.0104 - acc: 0.9990 - val_loss: 0.3999 - val_acc: 0.8809\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 0.0103 - acc: 0.9995 - val_loss: 0.4003 - val_acc: 0.8809\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 0.0103 - acc: 0.9995 - val_loss: 0.3998 - val_acc: 0.8824\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9995 - val_loss: 0.4007 - val_acc: 0.8824\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 0.0103 - acc: 0.9995 - val_loss: 0.4003 - val_acc: 0.8817\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9995 - val_loss: 0.4005 - val_acc: 0.8809\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9990 - val_loss: 0.4051 - val_acc: 0.8785\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 0.0103 - acc: 0.9990 - val_loss: 0.4019 - val_acc: 0.8832\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9995 - val_loss: 0.4015 - val_acc: 0.8824\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9990 - val_loss: 0.4010 - val_acc: 0.8817\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 0.0100 - acc: 0.9995 - val_loss: 0.4039 - val_acc: 0.8809\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 0.0101 - acc: 0.9995 - val_loss: 0.4018 - val_acc: 0.8832\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9990 - val_loss: 0.4029 - val_acc: 0.8817\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 0.0100 - acc: 0.9995 - val_loss: 0.4007 - val_acc: 0.8817\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 0.0100 - acc: 0.9990 - val_loss: 0.4499 - val_acc: 0.8550\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 0.0116 - acc: 0.9995 - val_loss: 0.4037 - val_acc: 0.8793\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 0.0102 - acc: 0.9995 - val_loss: 0.4029 - val_acc: 0.8809\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 0.0100 - acc: 0.9995 - val_loss: 0.4054 - val_acc: 0.8801\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 0.0101 - acc: 0.9995 - val_loss: 0.4030 - val_acc: 0.8824\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 0.0099 - acc: 0.9995 - val_loss: 0.4054 - val_acc: 0.8785\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 0.0101 - acc: 0.9990 - val_loss: 0.4031 - val_acc: 0.8824\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 0.0099 - acc: 0.9990 - val_loss: 0.4013 - val_acc: 0.8785\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9995 - val_loss: 0.4032 - val_acc: 0.8824\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9990 - val_loss: 0.4035 - val_acc: 0.8817\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 0.0097 - acc: 0.9995 - val_loss: 0.4072 - val_acc: 0.8801\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9990 - val_loss: 0.4049 - val_acc: 0.8832\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 0.0097 - acc: 0.9995 - val_loss: 0.4030 - val_acc: 0.8824\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9990 - val_loss: 0.4042 - val_acc: 0.8809\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 0.0097 - acc: 0.9995 - val_loss: 0.4045 - val_acc: 0.8817\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 0.0098 - acc: 0.9990 - val_loss: 0.4043 - val_acc: 0.8817\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 0.0097 - acc: 0.9990 - val_loss: 0.4049 - val_acc: 0.8817\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 0.0096 - acc: 0.9995 - val_loss: 0.4065 - val_acc: 0.8793\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 0.0096 - acc: 0.9990 - val_loss: 0.4050 - val_acc: 0.8817\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 0.0096 - acc: 0.9995 - val_loss: 0.4043 - val_acc: 0.8801\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9995 - val_loss: 0.4076 - val_acc: 0.8785\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9995 - val_loss: 0.4052 - val_acc: 0.8824\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9995 - val_loss: 0.4052 - val_acc: 0.8824\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9995 - val_loss: 0.4067 - val_acc: 0.8824\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9990 - val_loss: 0.4093 - val_acc: 0.8793\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9995 - val_loss: 0.4052 - val_acc: 0.8817\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 0.0095 - acc: 0.9990 - val_loss: 0.4067 - val_acc: 0.8809\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9990 - val_loss: 0.4064 - val_acc: 0.8832\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 0.0093 - acc: 0.9990 - val_loss: 0.4077 - val_acc: 0.8801\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 0.0093 - acc: 0.9995 - val_loss: 0.4065 - val_acc: 0.8801\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9995 - val_loss: 0.4050 - val_acc: 0.8832\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 0.0093 - acc: 0.9995 - val_loss: 0.4063 - val_acc: 0.8832\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9990 - val_loss: 0.4069 - val_acc: 0.8817\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9995 - val_loss: 0.4103 - val_acc: 0.8817\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 0.0094 - acc: 0.9990 - val_loss: 0.4105 - val_acc: 0.8793\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9995 - val_loss: 0.4065 - val_acc: 0.8832\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9995 - val_loss: 0.4088 - val_acc: 0.8793\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 0.4077 - val_acc: 0.8817\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9990 - val_loss: 0.4072 - val_acc: 0.8817\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9990 - val_loss: 0.4093 - val_acc: 0.8809\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9995 - val_loss: 0.4092 - val_acc: 0.8809\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9995 - val_loss: 0.4066 - val_acc: 0.8793\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9990 - val_loss: 0.4073 - val_acc: 0.8832\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9995 - val_loss: 0.4063 - val_acc: 0.8809\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9995 - val_loss: 0.4974 - val_acc: 0.8754\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 0.0122 - acc: 0.9995 - val_loss: 0.4093 - val_acc: 0.8824\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 0.0092 - acc: 0.9995 - val_loss: 0.4084 - val_acc: 0.8762\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 0.0091 - acc: 0.9995 - val_loss: 0.4090 - val_acc: 0.8817\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9995 - val_loss: 0.4120 - val_acc: 0.8809\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 0.0090 - acc: 0.9995 - val_loss: 0.4090 - val_acc: 0.8817\n",
      "Epoch 936/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9995 - val_loss: 0.4088 - val_acc: 0.8817\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9995 - val_loss: 0.4077 - val_acc: 0.8785\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.4097 - val_acc: 0.8817\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.4097 - val_acc: 0.8824\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.4104 - val_acc: 0.8817\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 0.0087 - acc: 0.9995 - val_loss: 0.4076 - val_acc: 0.8785\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 0.0089 - acc: 0.9990 - val_loss: 0.4104 - val_acc: 0.8817\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 0.0088 - acc: 0.9995 - val_loss: 0.4090 - val_acc: 0.8817\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 0.0088 - acc: 0.9995 - val_loss: 0.4092 - val_acc: 0.8777\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 0.0087 - acc: 0.9995 - val_loss: 0.4130 - val_acc: 0.8777\n",
      "Epoch 946/1000\n",
      " - 0s - loss: 0.0088 - acc: 0.9990 - val_loss: 0.4093 - val_acc: 0.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      " - 0s - loss: 0.0087 - acc: 0.9990 - val_loss: 0.4156 - val_acc: 0.8770\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 0.0088 - acc: 0.9990 - val_loss: 0.4110 - val_acc: 0.8824\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9995 - val_loss: 0.4103 - val_acc: 0.8809\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 0.0087 - acc: 0.9995 - val_loss: 0.4112 - val_acc: 0.8809\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 0.0087 - acc: 0.9990 - val_loss: 0.4107 - val_acc: 0.8809\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9990 - val_loss: 0.4116 - val_acc: 0.8809\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9990 - val_loss: 0.4112 - val_acc: 0.8817\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9990 - val_loss: 0.4107 - val_acc: 0.8809\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 0.0086 - acc: 0.9990 - val_loss: 0.4135 - val_acc: 0.8793\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 0.0085 - acc: 0.9995 - val_loss: 0.4126 - val_acc: 0.8817\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 0.0084 - acc: 0.9995 - val_loss: 0.4111 - val_acc: 0.8801\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 0.0085 - acc: 0.9995 - val_loss: 0.4121 - val_acc: 0.8801\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 0.0085 - acc: 0.9990 - val_loss: 0.4112 - val_acc: 0.8809\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 0.0085 - acc: 0.9995 - val_loss: 0.4182 - val_acc: 0.8793\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 0.0085 - acc: 0.9990 - val_loss: 0.4116 - val_acc: 0.8801\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 0.0084 - acc: 0.9990 - val_loss: 0.4111 - val_acc: 0.8793\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 0.0084 - acc: 0.9995 - val_loss: 0.4123 - val_acc: 0.8809\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4138 - val_acc: 0.8801\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9995 - val_loss: 0.4118 - val_acc: 0.8801\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 0.0084 - acc: 0.9995 - val_loss: 0.4147 - val_acc: 0.8785\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9995 - val_loss: 0.4111 - val_acc: 0.8785\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4166 - val_acc: 0.8793\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9995 - val_loss: 0.4151 - val_acc: 0.8777\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4132 - val_acc: 0.8793\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 0.0081 - acc: 0.9995 - val_loss: 0.4153 - val_acc: 0.8777\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9990 - val_loss: 0.4124 - val_acc: 0.8793\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4132 - val_acc: 0.8801\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4127 - val_acc: 0.8824\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4174 - val_acc: 0.8817\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9995 - val_loss: 0.4166 - val_acc: 0.8809\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 0.0083 - acc: 0.9990 - val_loss: 0.4132 - val_acc: 0.8809\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 0.0081 - acc: 0.9990 - val_loss: 0.4133 - val_acc: 0.8817\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 0.0081 - acc: 0.9995 - val_loss: 0.4149 - val_acc: 0.8793\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 0.0081 - acc: 0.9995 - val_loss: 0.4157 - val_acc: 0.8793\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9995 - val_loss: 0.4211 - val_acc: 0.8785\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 0.0082 - acc: 0.9990 - val_loss: 0.4160 - val_acc: 0.8801\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9995 - val_loss: 0.4151 - val_acc: 0.8801\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4157 - val_acc: 0.8809\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4180 - val_acc: 0.8793\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4146 - val_acc: 0.8809\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4161 - val_acc: 0.8809\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9995 - val_loss: 0.4162 - val_acc: 0.8801\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4184 - val_acc: 0.8793\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4159 - val_acc: 0.8809\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4147 - val_acc: 0.8801\n",
      "Epoch 992/1000\n",
      " - 0s - loss: 0.0079 - acc: 0.9995 - val_loss: 0.4161 - val_acc: 0.8817\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 0.0079 - acc: 0.9995 - val_loss: 0.4150 - val_acc: 0.8785\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 0.0080 - acc: 0.9990 - val_loss: 0.4149 - val_acc: 0.8793\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 0.0078 - acc: 0.9995 - val_loss: 0.4190 - val_acc: 0.8785\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 0.0078 - acc: 0.9990 - val_loss: 0.4163 - val_acc: 0.8809\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 0.0079 - acc: 0.9990 - val_loss: 0.4148 - val_acc: 0.8785\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 0.0078 - acc: 0.9990 - val_loss: 0.4158 - val_acc: 0.8801\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 0.0078 - acc: 0.9990 - val_loss: 0.4164 - val_acc: 0.8801\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 0.0077 - acc: 0.9995 - val_loss: 0.4158 - val_acc: 0.8809\n",
      "CPU times: user 55.4 s, sys: 11.1 s, total: 1min 6s\n",
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist1 = model1.fit(X_train, y_train, epochs=1000, batch_size=100,\n",
    "                   validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.87      0.86      0.87       320\n",
      "         IE       0.83      0.86      0.85       327\n",
      "          N       0.91      0.90      0.91       629\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnn_model_classification_report(model1, dnn_preprocessed, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(120, input_dim=60, activation=\"sigmoid\"))\n",
    "model2.add(Dense(3, activation=\"softmax\"))\n",
    "model2.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1914 samples, validate on 1276 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 1.1153 - acc: 0.4817 - val_loss: 1.1180 - val_acc: 0.4890\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 1.0427 - acc: 0.5199 - val_loss: 1.0758 - val_acc: 0.4773\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 1.0190 - acc: 0.5251 - val_loss: 1.0524 - val_acc: 0.4812\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.9958 - acc: 0.5298 - val_loss: 1.0367 - val_acc: 0.4922\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.9755 - acc: 0.5361 - val_loss: 1.0186 - val_acc: 0.5024\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.9566 - acc: 0.5491 - val_loss: 0.9891 - val_acc: 0.5086\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.9385 - acc: 0.5522 - val_loss: 0.9820 - val_acc: 0.4969\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.9215 - acc: 0.5569 - val_loss: 0.9627 - val_acc: 0.4984\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.9056 - acc: 0.5564 - val_loss: 0.9431 - val_acc: 0.5149\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.8883 - acc: 0.5742 - val_loss: 0.9224 - val_acc: 0.5259\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.8743 - acc: 0.5737 - val_loss: 0.9053 - val_acc: 0.5368\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.8605 - acc: 0.5810 - val_loss: 0.8897 - val_acc: 0.5870\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.8454 - acc: 0.5956 - val_loss: 0.8776 - val_acc: 0.5564\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.8318 - acc: 0.6034 - val_loss: 0.8699 - val_acc: 0.5392\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.8187 - acc: 0.6034 - val_loss: 0.8476 - val_acc: 0.6113\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.8058 - acc: 0.6238 - val_loss: 0.8372 - val_acc: 0.5839\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.7923 - acc: 0.6290 - val_loss: 0.8392 - val_acc: 0.5455\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.7808 - acc: 0.6317 - val_loss: 0.8112 - val_acc: 0.6121\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.7679 - acc: 0.6468 - val_loss: 0.7951 - val_acc: 0.6646\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.7565 - acc: 0.6656 - val_loss: 0.7878 - val_acc: 0.6277\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.7445 - acc: 0.6656 - val_loss: 0.7716 - val_acc: 0.6591\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.7330 - acc: 0.6745 - val_loss: 0.7590 - val_acc: 0.7116\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.7225 - acc: 0.6949 - val_loss: 0.7503 - val_acc: 0.6732\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.7112 - acc: 0.6991 - val_loss: 0.7445 - val_acc: 0.6497\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.7008 - acc: 0.6975 - val_loss: 0.7284 - val_acc: 0.6826\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.6904 - acc: 0.7111 - val_loss: 0.7141 - val_acc: 0.7343\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.6815 - acc: 0.7283 - val_loss: 0.7090 - val_acc: 0.7038\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.6708 - acc: 0.7382 - val_loss: 0.6981 - val_acc: 0.7108\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.6617 - acc: 0.7346 - val_loss: 0.6888 - val_acc: 0.7539\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.6540 - acc: 0.7534 - val_loss: 0.6824 - val_acc: 0.7132\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.6438 - acc: 0.7466 - val_loss: 0.6707 - val_acc: 0.7328\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.6353 - acc: 0.7581 - val_loss: 0.6626 - val_acc: 0.7351\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.6274 - acc: 0.7618 - val_loss: 0.6586 - val_acc: 0.7273\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.6193 - acc: 0.7644 - val_loss: 0.6449 - val_acc: 0.7641\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.6119 - acc: 0.7743 - val_loss: 0.6428 - val_acc: 0.7398\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.6037 - acc: 0.7748 - val_loss: 0.6337 - val_acc: 0.7516\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.5965 - acc: 0.7837 - val_loss: 0.6232 - val_acc: 0.7743\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.5907 - acc: 0.7837 - val_loss: 0.6140 - val_acc: 0.7829\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.5827 - acc: 0.7941 - val_loss: 0.6039 - val_acc: 0.8119\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.5767 - acc: 0.8041 - val_loss: 0.6040 - val_acc: 0.7790\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.5694 - acc: 0.7931 - val_loss: 0.5905 - val_acc: 0.8245\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.5636 - acc: 0.8093 - val_loss: 0.5879 - val_acc: 0.8009\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.5581 - acc: 0.8072 - val_loss: 0.5854 - val_acc: 0.7876\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.5516 - acc: 0.8051 - val_loss: 0.5742 - val_acc: 0.8072\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.5465 - acc: 0.8156 - val_loss: 0.5664 - val_acc: 0.8174\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.5403 - acc: 0.8208 - val_loss: 0.5644 - val_acc: 0.8064\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.5350 - acc: 0.8197 - val_loss: 0.5549 - val_acc: 0.8260\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.5292 - acc: 0.8271 - val_loss: 0.5499 - val_acc: 0.8331\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.5251 - acc: 0.8302 - val_loss: 0.5487 - val_acc: 0.8111\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.5196 - acc: 0.8255 - val_loss: 0.5524 - val_acc: 0.7978\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.5156 - acc: 0.8250 - val_loss: 0.5478 - val_acc: 0.7994\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.5110 - acc: 0.8281 - val_loss: 0.5364 - val_acc: 0.8096\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.5061 - acc: 0.8380 - val_loss: 0.5294 - val_acc: 0.8245\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.5011 - acc: 0.8349 - val_loss: 0.5215 - val_acc: 0.8425\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.4972 - acc: 0.8443 - val_loss: 0.5173 - val_acc: 0.8362\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.4936 - acc: 0.8417 - val_loss: 0.5128 - val_acc: 0.8433\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.4895 - acc: 0.8464 - val_loss: 0.5165 - val_acc: 0.8205\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.4856 - acc: 0.8427 - val_loss: 0.5065 - val_acc: 0.8354\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.4818 - acc: 0.8454 - val_loss: 0.5026 - val_acc: 0.8331\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.4777 - acc: 0.8448 - val_loss: 0.4974 - val_acc: 0.8433\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.4755 - acc: 0.8480 - val_loss: 0.4947 - val_acc: 0.8323\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.4710 - acc: 0.8511 - val_loss: 0.5015 - val_acc: 0.8260\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.4681 - acc: 0.8485 - val_loss: 0.4879 - val_acc: 0.8511\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.4646 - acc: 0.8537 - val_loss: 0.4924 - val_acc: 0.8307\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.4605 - acc: 0.8464 - val_loss: 0.4804 - val_acc: 0.8440\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.4582 - acc: 0.8563 - val_loss: 0.4779 - val_acc: 0.8440\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.4551 - acc: 0.8527 - val_loss: 0.4803 - val_acc: 0.8378\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.4519 - acc: 0.8516 - val_loss: 0.4732 - val_acc: 0.8417\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.4498 - acc: 0.8527 - val_loss: 0.4722 - val_acc: 0.8378\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.4469 - acc: 0.8537 - val_loss: 0.4673 - val_acc: 0.8425\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.4445 - acc: 0.8553 - val_loss: 0.4656 - val_acc: 0.8417\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.4414 - acc: 0.8553 - val_loss: 0.4622 - val_acc: 0.8409\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.4387 - acc: 0.8568 - val_loss: 0.4591 - val_acc: 0.8464\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.4361 - acc: 0.8595 - val_loss: 0.4564 - val_acc: 0.8503\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.4335 - acc: 0.8574 - val_loss: 0.4550 - val_acc: 0.8456\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.4318 - acc: 0.8589 - val_loss: 0.4663 - val_acc: 0.8331\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.4305 - acc: 0.8542 - val_loss: 0.4553 - val_acc: 0.8386\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.4272 - acc: 0.8563 - val_loss: 0.4473 - val_acc: 0.8605\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.4265 - acc: 0.8621 - val_loss: 0.4471 - val_acc: 0.8597\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.4237 - acc: 0.8584 - val_loss: 0.4442 - val_acc: 0.8589\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.4213 - acc: 0.8605 - val_loss: 0.4452 - val_acc: 0.8503\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.4186 - acc: 0.8600 - val_loss: 0.4450 - val_acc: 0.8487\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.4177 - acc: 0.8642 - val_loss: 0.4409 - val_acc: 0.8448\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.4148 - acc: 0.8636 - val_loss: 0.4402 - val_acc: 0.8472\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.4123 - acc: 0.8631 - val_loss: 0.4348 - val_acc: 0.8574\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.4120 - acc: 0.8662 - val_loss: 0.4382 - val_acc: 0.8425\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.4091 - acc: 0.8589 - val_loss: 0.4312 - val_acc: 0.8511\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.4067 - acc: 0.8563 - val_loss: 0.4339 - val_acc: 0.8511\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.4058 - acc: 0.8605 - val_loss: 0.4282 - val_acc: 0.8534\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.4034 - acc: 0.8694 - val_loss: 0.4435 - val_acc: 0.8386\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.4028 - acc: 0.8615 - val_loss: 0.4283 - val_acc: 0.8527\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.4004 - acc: 0.8662 - val_loss: 0.4374 - val_acc: 0.8401\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.3990 - acc: 0.8631 - val_loss: 0.4475 - val_acc: 0.8229\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.3977 - acc: 0.8678 - val_loss: 0.4196 - val_acc: 0.8644\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 0.3961 - acc: 0.8652 - val_loss: 0.4189 - val_acc: 0.8644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      " - 0s - loss: 0.3949 - acc: 0.8668 - val_loss: 0.4181 - val_acc: 0.8597\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.3931 - acc: 0.8652 - val_loss: 0.4217 - val_acc: 0.8542\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.3915 - acc: 0.8668 - val_loss: 0.4154 - val_acc: 0.8621\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.3903 - acc: 0.8662 - val_loss: 0.4238 - val_acc: 0.8464\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.3877 - acc: 0.8631 - val_loss: 0.4197 - val_acc: 0.8511\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.3870 - acc: 0.8657 - val_loss: 0.4101 - val_acc: 0.8636\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.3846 - acc: 0.8678 - val_loss: 0.4342 - val_acc: 0.8393\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.3859 - acc: 0.8668 - val_loss: 0.4100 - val_acc: 0.8613\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.3821 - acc: 0.8689 - val_loss: 0.4106 - val_acc: 0.8613\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.3818 - acc: 0.8673 - val_loss: 0.4135 - val_acc: 0.8597\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.3806 - acc: 0.8668 - val_loss: 0.4150 - val_acc: 0.8456\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.3803 - acc: 0.8715 - val_loss: 0.4029 - val_acc: 0.8668\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.3774 - acc: 0.8710 - val_loss: 0.4188 - val_acc: 0.8487\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.3774 - acc: 0.8673 - val_loss: 0.4035 - val_acc: 0.8644\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.3752 - acc: 0.8694 - val_loss: 0.4025 - val_acc: 0.8621\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.3748 - acc: 0.8678 - val_loss: 0.4007 - val_acc: 0.8636\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.3733 - acc: 0.8710 - val_loss: 0.4025 - val_acc: 0.8636\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.3726 - acc: 0.8668 - val_loss: 0.4026 - val_acc: 0.8589\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.3711 - acc: 0.8710 - val_loss: 0.4091 - val_acc: 0.8487\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.3696 - acc: 0.8725 - val_loss: 0.3945 - val_acc: 0.8715\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.3689 - acc: 0.8715 - val_loss: 0.3955 - val_acc: 0.8597\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.3684 - acc: 0.8710 - val_loss: 0.3930 - val_acc: 0.8715\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.3667 - acc: 0.8736 - val_loss: 0.3995 - val_acc: 0.8613\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.3658 - acc: 0.8720 - val_loss: 0.3943 - val_acc: 0.8613\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.3651 - acc: 0.8725 - val_loss: 0.3976 - val_acc: 0.8605\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.3636 - acc: 0.8710 - val_loss: 0.4070 - val_acc: 0.8511\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.3643 - acc: 0.8699 - val_loss: 0.4080 - val_acc: 0.8472\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.3621 - acc: 0.8715 - val_loss: 0.4014 - val_acc: 0.8550\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.3611 - acc: 0.8762 - val_loss: 0.3876 - val_acc: 0.8699\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.3605 - acc: 0.8741 - val_loss: 0.3876 - val_acc: 0.8683\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.3593 - acc: 0.8746 - val_loss: 0.3870 - val_acc: 0.8668\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.3576 - acc: 0.8746 - val_loss: 0.3983 - val_acc: 0.8527\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.3581 - acc: 0.8715 - val_loss: 0.3878 - val_acc: 0.8660\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.3572 - acc: 0.8757 - val_loss: 0.3843 - val_acc: 0.8691\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.3568 - acc: 0.8730 - val_loss: 0.3902 - val_acc: 0.8621\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.3548 - acc: 0.8704 - val_loss: 0.3825 - val_acc: 0.8715\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.3535 - acc: 0.8736 - val_loss: 0.4240 - val_acc: 0.8245\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.3550 - acc: 0.8772 - val_loss: 0.3879 - val_acc: 0.8636\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.3516 - acc: 0.8746 - val_loss: 0.3815 - val_acc: 0.8691\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.3516 - acc: 0.8746 - val_loss: 0.3902 - val_acc: 0.8652\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.3518 - acc: 0.8715 - val_loss: 0.3792 - val_acc: 0.8691\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.3496 - acc: 0.8736 - val_loss: 0.3813 - val_acc: 0.8629\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.3490 - acc: 0.8746 - val_loss: 0.3802 - val_acc: 0.8668\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.3484 - acc: 0.8746 - val_loss: 0.3786 - val_acc: 0.8676\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.3480 - acc: 0.8730 - val_loss: 0.3806 - val_acc: 0.8621\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.3462 - acc: 0.8804 - val_loss: 0.3769 - val_acc: 0.8738\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.3454 - acc: 0.8710 - val_loss: 0.3842 - val_acc: 0.8589\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.3451 - acc: 0.8762 - val_loss: 0.3889 - val_acc: 0.8597\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.3455 - acc: 0.8751 - val_loss: 0.3764 - val_acc: 0.8676\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.3432 - acc: 0.8772 - val_loss: 0.3737 - val_acc: 0.8660\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.3427 - acc: 0.8736 - val_loss: 0.3803 - val_acc: 0.8652\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.3427 - acc: 0.8798 - val_loss: 0.3822 - val_acc: 0.8613\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.3426 - acc: 0.8777 - val_loss: 0.3862 - val_acc: 0.8597\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.3425 - acc: 0.8798 - val_loss: 0.3724 - val_acc: 0.8738\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.3407 - acc: 0.8741 - val_loss: 0.3803 - val_acc: 0.8644\n",
      "Epoch 151/1000\n",
      " - 0s - loss: 0.3394 - acc: 0.8809 - val_loss: 0.3720 - val_acc: 0.8683\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.3398 - acc: 0.8762 - val_loss: 0.3714 - val_acc: 0.8699\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.3379 - acc: 0.8772 - val_loss: 0.3705 - val_acc: 0.8723\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.3376 - acc: 0.8814 - val_loss: 0.3711 - val_acc: 0.8668\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.3371 - acc: 0.8757 - val_loss: 0.3795 - val_acc: 0.8636\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.3368 - acc: 0.8783 - val_loss: 0.3754 - val_acc: 0.8668\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.3360 - acc: 0.8798 - val_loss: 0.3684 - val_acc: 0.8738\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.3352 - acc: 0.8772 - val_loss: 0.3720 - val_acc: 0.8707\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.3343 - acc: 0.8798 - val_loss: 0.3710 - val_acc: 0.8629\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.3342 - acc: 0.8793 - val_loss: 0.3680 - val_acc: 0.8683\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.3330 - acc: 0.8804 - val_loss: 0.3662 - val_acc: 0.8738\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.3334 - acc: 0.8798 - val_loss: 0.3756 - val_acc: 0.8636\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.3324 - acc: 0.8804 - val_loss: 0.3720 - val_acc: 0.8652\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.3313 - acc: 0.8804 - val_loss: 0.3660 - val_acc: 0.8715\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.3305 - acc: 0.8783 - val_loss: 0.3733 - val_acc: 0.8629\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.3301 - acc: 0.8788 - val_loss: 0.3727 - val_acc: 0.8652\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.3300 - acc: 0.8793 - val_loss: 0.3655 - val_acc: 0.8660\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.3293 - acc: 0.8814 - val_loss: 0.3638 - val_acc: 0.8707\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.3289 - acc: 0.8798 - val_loss: 0.3709 - val_acc: 0.8652\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.3280 - acc: 0.8804 - val_loss: 0.3689 - val_acc: 0.8676\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 0.3277 - acc: 0.8809 - val_loss: 0.3673 - val_acc: 0.8636\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.3268 - acc: 0.8824 - val_loss: 0.3703 - val_acc: 0.8652\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.3259 - acc: 0.8851 - val_loss: 0.3646 - val_acc: 0.8676\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.3255 - acc: 0.8809 - val_loss: 0.3709 - val_acc: 0.8652\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.3258 - acc: 0.8809 - val_loss: 0.3618 - val_acc: 0.8691\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.3247 - acc: 0.8809 - val_loss: 0.3624 - val_acc: 0.8699\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.3249 - acc: 0.8798 - val_loss: 0.3668 - val_acc: 0.8644\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.3228 - acc: 0.8856 - val_loss: 0.3659 - val_acc: 0.8636\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.3235 - acc: 0.8809 - val_loss: 0.3668 - val_acc: 0.8652\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.3222 - acc: 0.8809 - val_loss: 0.3605 - val_acc: 0.8683\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.3222 - acc: 0.8809 - val_loss: 0.3593 - val_acc: 0.8715\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.3209 - acc: 0.8819 - val_loss: 0.3636 - val_acc: 0.8691\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.3211 - acc: 0.8804 - val_loss: 0.3631 - val_acc: 0.8683\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.3207 - acc: 0.8804 - val_loss: 0.3638 - val_acc: 0.8668\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.3201 - acc: 0.8845 - val_loss: 0.3571 - val_acc: 0.8746\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.3198 - acc: 0.8835 - val_loss: 0.3572 - val_acc: 0.8746\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.3198 - acc: 0.8819 - val_loss: 0.3631 - val_acc: 0.8683\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.3181 - acc: 0.8840 - val_loss: 0.3559 - val_acc: 0.8730\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.3180 - acc: 0.8793 - val_loss: 0.3648 - val_acc: 0.8660\n",
      "Epoch 190/1000\n",
      " - 0s - loss: 0.3177 - acc: 0.8866 - val_loss: 0.3561 - val_acc: 0.8715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/1000\n",
      " - 0s - loss: 0.3171 - acc: 0.8809 - val_loss: 0.3545 - val_acc: 0.8746\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.3163 - acc: 0.8845 - val_loss: 0.3616 - val_acc: 0.8652\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.3158 - acc: 0.8845 - val_loss: 0.3643 - val_acc: 0.8668\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.3158 - acc: 0.8835 - val_loss: 0.3568 - val_acc: 0.8683\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.3152 - acc: 0.8851 - val_loss: 0.3541 - val_acc: 0.8738\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.3140 - acc: 0.8851 - val_loss: 0.3564 - val_acc: 0.8683\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.3146 - acc: 0.8830 - val_loss: 0.3599 - val_acc: 0.8668\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.3128 - acc: 0.8877 - val_loss: 0.3574 - val_acc: 0.8683\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.3128 - acc: 0.8866 - val_loss: 0.3523 - val_acc: 0.8738\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.3127 - acc: 0.8830 - val_loss: 0.3622 - val_acc: 0.8636\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.3117 - acc: 0.8856 - val_loss: 0.3522 - val_acc: 0.8730\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.3116 - acc: 0.8840 - val_loss: 0.3515 - val_acc: 0.8738\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.3113 - acc: 0.8824 - val_loss: 0.3581 - val_acc: 0.8668\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.3110 - acc: 0.8851 - val_loss: 0.3524 - val_acc: 0.8683\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.3108 - acc: 0.8819 - val_loss: 0.3566 - val_acc: 0.8715\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.3092 - acc: 0.8830 - val_loss: 0.3532 - val_acc: 0.8715\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.3091 - acc: 0.8851 - val_loss: 0.3587 - val_acc: 0.8668\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.3093 - acc: 0.8866 - val_loss: 0.3623 - val_acc: 0.8636\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.3090 - acc: 0.8861 - val_loss: 0.3561 - val_acc: 0.8683\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.3084 - acc: 0.8866 - val_loss: 0.3544 - val_acc: 0.8715\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.3080 - acc: 0.8882 - val_loss: 0.3597 - val_acc: 0.8691\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.3071 - acc: 0.8892 - val_loss: 0.3546 - val_acc: 0.8683\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.3065 - acc: 0.8856 - val_loss: 0.3539 - val_acc: 0.8691\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.3065 - acc: 0.8856 - val_loss: 0.3656 - val_acc: 0.8629\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.3068 - acc: 0.8887 - val_loss: 0.3541 - val_acc: 0.8699\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.3057 - acc: 0.8898 - val_loss: 0.3498 - val_acc: 0.8699\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.3049 - acc: 0.8882 - val_loss: 0.3505 - val_acc: 0.8754\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.3056 - acc: 0.8866 - val_loss: 0.3526 - val_acc: 0.8707\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.3038 - acc: 0.8871 - val_loss: 0.3512 - val_acc: 0.8715\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.3052 - acc: 0.8871 - val_loss: 0.3481 - val_acc: 0.8707\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.3031 - acc: 0.8898 - val_loss: 0.3479 - val_acc: 0.8730\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.3025 - acc: 0.8882 - val_loss: 0.3471 - val_acc: 0.8707\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.3023 - acc: 0.8856 - val_loss: 0.3475 - val_acc: 0.8707\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.3026 - acc: 0.8887 - val_loss: 0.3762 - val_acc: 0.8495\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.3034 - acc: 0.8913 - val_loss: 0.3460 - val_acc: 0.8723\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.3011 - acc: 0.8861 - val_loss: 0.3464 - val_acc: 0.8723\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.3018 - acc: 0.8866 - val_loss: 0.3533 - val_acc: 0.8683\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.3009 - acc: 0.8882 - val_loss: 0.3457 - val_acc: 0.8730\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.3000 - acc: 0.8892 - val_loss: 0.3473 - val_acc: 0.8730\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.2986 - acc: 0.8877 - val_loss: 0.3643 - val_acc: 0.8629\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.2998 - acc: 0.8903 - val_loss: 0.3498 - val_acc: 0.8715\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.2987 - acc: 0.8913 - val_loss: 0.3469 - val_acc: 0.8699\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.2978 - acc: 0.8882 - val_loss: 0.3473 - val_acc: 0.8746\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.2972 - acc: 0.8898 - val_loss: 0.3454 - val_acc: 0.8723\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.2972 - acc: 0.8871 - val_loss: 0.3562 - val_acc: 0.8660\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.2974 - acc: 0.8939 - val_loss: 0.3460 - val_acc: 0.8730\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.2967 - acc: 0.8903 - val_loss: 0.3543 - val_acc: 0.8683\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.2965 - acc: 0.8950 - val_loss: 0.3477 - val_acc: 0.8699\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.2957 - acc: 0.8924 - val_loss: 0.3472 - val_acc: 0.8730\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.2959 - acc: 0.8924 - val_loss: 0.3452 - val_acc: 0.8746\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.2951 - acc: 0.8877 - val_loss: 0.3510 - val_acc: 0.8668\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.2958 - acc: 0.8908 - val_loss: 0.3590 - val_acc: 0.8683\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.2946 - acc: 0.8913 - val_loss: 0.3580 - val_acc: 0.8691\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.2956 - acc: 0.8898 - val_loss: 0.3478 - val_acc: 0.8699\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.2936 - acc: 0.8913 - val_loss: 0.3418 - val_acc: 0.8746\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.2931 - acc: 0.8908 - val_loss: 0.3412 - val_acc: 0.8762\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.2930 - acc: 0.8871 - val_loss: 0.3589 - val_acc: 0.8660\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.2926 - acc: 0.8913 - val_loss: 0.3406 - val_acc: 0.8754\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.2920 - acc: 0.8892 - val_loss: 0.3435 - val_acc: 0.8723\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.2913 - acc: 0.8924 - val_loss: 0.3724 - val_acc: 0.8495\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.2924 - acc: 0.8950 - val_loss: 0.3437 - val_acc: 0.8730\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.2914 - acc: 0.8918 - val_loss: 0.3455 - val_acc: 0.8730\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.2904 - acc: 0.8924 - val_loss: 0.3409 - val_acc: 0.8738\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.2904 - acc: 0.8913 - val_loss: 0.3420 - val_acc: 0.8738\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.2895 - acc: 0.8898 - val_loss: 0.3446 - val_acc: 0.8762\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 0.2901 - acc: 0.8908 - val_loss: 0.3450 - val_acc: 0.8730\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.2885 - acc: 0.8971 - val_loss: 0.3396 - val_acc: 0.8746\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.2890 - acc: 0.8903 - val_loss: 0.3468 - val_acc: 0.8699\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.2883 - acc: 0.8950 - val_loss: 0.3433 - val_acc: 0.8754\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.2881 - acc: 0.8924 - val_loss: 0.3466 - val_acc: 0.8715\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.2880 - acc: 0.8950 - val_loss: 0.3458 - val_acc: 0.8730\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.2873 - acc: 0.8955 - val_loss: 0.3384 - val_acc: 0.8730\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.2868 - acc: 0.8950 - val_loss: 0.3478 - val_acc: 0.8738\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.2877 - acc: 0.8939 - val_loss: 0.3556 - val_acc: 0.8699\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.2865 - acc: 0.8934 - val_loss: 0.3405 - val_acc: 0.8738\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.2857 - acc: 0.8950 - val_loss: 0.3477 - val_acc: 0.8715\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.2856 - acc: 0.8950 - val_loss: 0.3418 - val_acc: 0.8738\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.2866 - acc: 0.8934 - val_loss: 0.3371 - val_acc: 0.8730\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.2849 - acc: 0.8908 - val_loss: 0.3477 - val_acc: 0.8683\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.2852 - acc: 0.8945 - val_loss: 0.3437 - val_acc: 0.8707\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.2842 - acc: 0.8934 - val_loss: 0.3460 - val_acc: 0.8683\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.2835 - acc: 0.8976 - val_loss: 0.3377 - val_acc: 0.8738\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.2833 - acc: 0.8934 - val_loss: 0.3380 - val_acc: 0.8754\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.2825 - acc: 0.8945 - val_loss: 0.3518 - val_acc: 0.8691\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.2830 - acc: 0.8981 - val_loss: 0.3429 - val_acc: 0.8715\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.2824 - acc: 0.8950 - val_loss: 0.3467 - val_acc: 0.8707\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.2821 - acc: 0.8966 - val_loss: 0.3384 - val_acc: 0.8738\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.2811 - acc: 0.8966 - val_loss: 0.3369 - val_acc: 0.8738\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.2809 - acc: 0.8997 - val_loss: 0.3394 - val_acc: 0.8746\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.2812 - acc: 0.8966 - val_loss: 0.3357 - val_acc: 0.8770\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.2809 - acc: 0.8971 - val_loss: 0.3433 - val_acc: 0.8691\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.2800 - acc: 0.9007 - val_loss: 0.3416 - val_acc: 0.8707\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.2796 - acc: 0.8981 - val_loss: 0.3474 - val_acc: 0.8707\n",
      "Epoch 284/1000\n",
      " - 0s - loss: 0.2799 - acc: 0.9007 - val_loss: 0.3346 - val_acc: 0.8785\n",
      "Epoch 285/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.2788 - acc: 0.8945 - val_loss: 0.3454 - val_acc: 0.8762\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.2801 - acc: 0.8971 - val_loss: 0.3346 - val_acc: 0.8754\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.2785 - acc: 0.8934 - val_loss: 0.3354 - val_acc: 0.8754\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.2775 - acc: 0.8971 - val_loss: 0.3390 - val_acc: 0.8754\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.2782 - acc: 0.8971 - val_loss: 0.3385 - val_acc: 0.8730\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.2773 - acc: 0.9007 - val_loss: 0.3389 - val_acc: 0.8746\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.2767 - acc: 0.8992 - val_loss: 0.3360 - val_acc: 0.8770\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.2762 - acc: 0.8997 - val_loss: 0.3361 - val_acc: 0.8777\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.2764 - acc: 0.8992 - val_loss: 0.3343 - val_acc: 0.8746\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.2750 - acc: 0.8971 - val_loss: 0.3421 - val_acc: 0.8715\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.2754 - acc: 0.8992 - val_loss: 0.3502 - val_acc: 0.8707\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.2759 - acc: 0.9007 - val_loss: 0.3450 - val_acc: 0.8762\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.2754 - acc: 0.9002 - val_loss: 0.3323 - val_acc: 0.8793\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.2750 - acc: 0.8971 - val_loss: 0.3365 - val_acc: 0.8746\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.2749 - acc: 0.8997 - val_loss: 0.3319 - val_acc: 0.8770\n",
      "Epoch 300/1000\n",
      " - 0s - loss: 0.2741 - acc: 0.8966 - val_loss: 0.3324 - val_acc: 0.8801\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.2734 - acc: 0.8960 - val_loss: 0.3387 - val_acc: 0.8730\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.2726 - acc: 0.9054 - val_loss: 0.3326 - val_acc: 0.8809\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.2732 - acc: 0.8992 - val_loss: 0.3371 - val_acc: 0.8746\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.2726 - acc: 0.9018 - val_loss: 0.3351 - val_acc: 0.8777\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.2724 - acc: 0.9002 - val_loss: 0.3358 - val_acc: 0.8738\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.2716 - acc: 0.9002 - val_loss: 0.3328 - val_acc: 0.8817\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.2715 - acc: 0.8981 - val_loss: 0.3329 - val_acc: 0.8770\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.2715 - acc: 0.9033 - val_loss: 0.3394 - val_acc: 0.8738\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.2703 - acc: 0.9049 - val_loss: 0.3314 - val_acc: 0.8777\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.2705 - acc: 0.9002 - val_loss: 0.3352 - val_acc: 0.8801\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.2707 - acc: 0.9013 - val_loss: 0.3383 - val_acc: 0.8746\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.2701 - acc: 0.9044 - val_loss: 0.3324 - val_acc: 0.8801\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.2702 - acc: 0.9007 - val_loss: 0.3312 - val_acc: 0.8777\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.2699 - acc: 0.9013 - val_loss: 0.3340 - val_acc: 0.8793\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.2678 - acc: 0.9039 - val_loss: 0.3364 - val_acc: 0.8746\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.2679 - acc: 0.9049 - val_loss: 0.3332 - val_acc: 0.8770\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.2685 - acc: 0.9033 - val_loss: 0.3323 - val_acc: 0.8785\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.2679 - acc: 0.9028 - val_loss: 0.3389 - val_acc: 0.8762\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.2676 - acc: 0.9065 - val_loss: 0.3296 - val_acc: 0.8817\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.2676 - acc: 0.9028 - val_loss: 0.3355 - val_acc: 0.8738\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.2665 - acc: 0.9049 - val_loss: 0.3404 - val_acc: 0.8777\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.2674 - acc: 0.9028 - val_loss: 0.3343 - val_acc: 0.8762\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.2661 - acc: 0.9033 - val_loss: 0.3286 - val_acc: 0.8785\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.2662 - acc: 0.9065 - val_loss: 0.3338 - val_acc: 0.8777\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.2655 - acc: 0.9070 - val_loss: 0.3361 - val_acc: 0.8762\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.2651 - acc: 0.9039 - val_loss: 0.3300 - val_acc: 0.8777\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.2651 - acc: 0.9054 - val_loss: 0.3302 - val_acc: 0.8777\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.2662 - acc: 0.9013 - val_loss: 0.3294 - val_acc: 0.8762\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.2641 - acc: 0.9039 - val_loss: 0.3285 - val_acc: 0.8809\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.2639 - acc: 0.9028 - val_loss: 0.3295 - val_acc: 0.8785\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.2633 - acc: 0.9065 - val_loss: 0.3461 - val_acc: 0.8676\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.2645 - acc: 0.9070 - val_loss: 0.3359 - val_acc: 0.8770\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.2633 - acc: 0.9075 - val_loss: 0.3308 - val_acc: 0.8801\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.2625 - acc: 0.9054 - val_loss: 0.3273 - val_acc: 0.8809\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.2629 - acc: 0.9060 - val_loss: 0.3353 - val_acc: 0.8762\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.2628 - acc: 0.9033 - val_loss: 0.3270 - val_acc: 0.8793\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.2617 - acc: 0.9039 - val_loss: 0.3331 - val_acc: 0.8762\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.2620 - acc: 0.9065 - val_loss: 0.3325 - val_acc: 0.8770\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.2613 - acc: 0.9091 - val_loss: 0.3282 - val_acc: 0.8777\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.2608 - acc: 0.9049 - val_loss: 0.3275 - val_acc: 0.8770\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 0.2607 - acc: 0.9028 - val_loss: 0.3475 - val_acc: 0.8723\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.2608 - acc: 0.9096 - val_loss: 0.3358 - val_acc: 0.8738\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.2596 - acc: 0.9086 - val_loss: 0.3317 - val_acc: 0.8785\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.2588 - acc: 0.9049 - val_loss: 0.3691 - val_acc: 0.8393\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.2619 - acc: 0.9091 - val_loss: 0.3302 - val_acc: 0.8809\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.2590 - acc: 0.9080 - val_loss: 0.3277 - val_acc: 0.8840\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.2586 - acc: 0.9060 - val_loss: 0.3291 - val_acc: 0.8801\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.2581 - acc: 0.9070 - val_loss: 0.3410 - val_acc: 0.8770\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.2580 - acc: 0.9070 - val_loss: 0.3516 - val_acc: 0.8691\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.2584 - acc: 0.9154 - val_loss: 0.3254 - val_acc: 0.8793\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.2569 - acc: 0.9096 - val_loss: 0.3253 - val_acc: 0.8832\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.2572 - acc: 0.9070 - val_loss: 0.3259 - val_acc: 0.8864\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.2570 - acc: 0.9101 - val_loss: 0.3400 - val_acc: 0.8793\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.2564 - acc: 0.9122 - val_loss: 0.3288 - val_acc: 0.8785\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.2553 - acc: 0.9122 - val_loss: 0.3346 - val_acc: 0.8770\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.2558 - acc: 0.9060 - val_loss: 0.3246 - val_acc: 0.8809\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.2557 - acc: 0.9075 - val_loss: 0.3337 - val_acc: 0.8723\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.2553 - acc: 0.9075 - val_loss: 0.3319 - val_acc: 0.8738\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.2548 - acc: 0.9122 - val_loss: 0.3246 - val_acc: 0.8762\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.2540 - acc: 0.9080 - val_loss: 0.3299 - val_acc: 0.8793\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.2536 - acc: 0.9091 - val_loss: 0.3307 - val_acc: 0.8785\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.2541 - acc: 0.9112 - val_loss: 0.3349 - val_acc: 0.8809\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.2533 - acc: 0.9127 - val_loss: 0.3271 - val_acc: 0.8801\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.2528 - acc: 0.9080 - val_loss: 0.3284 - val_acc: 0.8801\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.2522 - acc: 0.9154 - val_loss: 0.3248 - val_acc: 0.8840\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.2524 - acc: 0.9112 - val_loss: 0.3249 - val_acc: 0.8809\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.2523 - acc: 0.9096 - val_loss: 0.3304 - val_acc: 0.8777\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.2517 - acc: 0.9122 - val_loss: 0.3256 - val_acc: 0.8785\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.2514 - acc: 0.9086 - val_loss: 0.3331 - val_acc: 0.8785\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.2511 - acc: 0.9101 - val_loss: 0.3364 - val_acc: 0.8770\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.2520 - acc: 0.9101 - val_loss: 0.3295 - val_acc: 0.8801\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.2507 - acc: 0.9127 - val_loss: 0.3254 - val_acc: 0.8817\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.2504 - acc: 0.9096 - val_loss: 0.3239 - val_acc: 0.8824\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.2497 - acc: 0.9086 - val_loss: 0.3257 - val_acc: 0.8809\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.2496 - acc: 0.9127 - val_loss: 0.3230 - val_acc: 0.8785\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.2488 - acc: 0.9096 - val_loss: 0.3276 - val_acc: 0.8809\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.2487 - acc: 0.9154 - val_loss: 0.3253 - val_acc: 0.8809\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.2486 - acc: 0.9080 - val_loss: 0.3337 - val_acc: 0.8801\n",
      "Epoch 379/1000\n",
      " - 0s - loss: 0.2486 - acc: 0.9122 - val_loss: 0.3270 - val_acc: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/1000\n",
      " - 0s - loss: 0.2475 - acc: 0.9133 - val_loss: 0.3282 - val_acc: 0.8801\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.2482 - acc: 0.9133 - val_loss: 0.3251 - val_acc: 0.8809\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.2470 - acc: 0.9112 - val_loss: 0.3248 - val_acc: 0.8817\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.2470 - acc: 0.9112 - val_loss: 0.3244 - val_acc: 0.8832\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.2463 - acc: 0.9143 - val_loss: 0.3275 - val_acc: 0.8793\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.2464 - acc: 0.9148 - val_loss: 0.3300 - val_acc: 0.8793\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.2465 - acc: 0.9143 - val_loss: 0.3327 - val_acc: 0.8817\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.2463 - acc: 0.9169 - val_loss: 0.3318 - val_acc: 0.8801\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.2460 - acc: 0.9127 - val_loss: 0.3227 - val_acc: 0.8824\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.2453 - acc: 0.9133 - val_loss: 0.3256 - val_acc: 0.8785\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.2449 - acc: 0.9143 - val_loss: 0.3230 - val_acc: 0.8864\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.2455 - acc: 0.9148 - val_loss: 0.3361 - val_acc: 0.8730\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.2445 - acc: 0.9143 - val_loss: 0.3227 - val_acc: 0.8832\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.2439 - acc: 0.9148 - val_loss: 0.3281 - val_acc: 0.8801\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.2442 - acc: 0.9138 - val_loss: 0.3218 - val_acc: 0.8809\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.2434 - acc: 0.9127 - val_loss: 0.3311 - val_acc: 0.8840\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.2441 - acc: 0.9133 - val_loss: 0.3253 - val_acc: 0.8793\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.2424 - acc: 0.9185 - val_loss: 0.3309 - val_acc: 0.8801\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.2428 - acc: 0.9154 - val_loss: 0.3282 - val_acc: 0.8793\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.2425 - acc: 0.9159 - val_loss: 0.3306 - val_acc: 0.8809\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.2419 - acc: 0.9159 - val_loss: 0.3308 - val_acc: 0.8793\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.2419 - acc: 0.9159 - val_loss: 0.3258 - val_acc: 0.8809\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.2403 - acc: 0.9159 - val_loss: 0.3463 - val_acc: 0.8574\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.2436 - acc: 0.9190 - val_loss: 0.3217 - val_acc: 0.8809\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.2423 - acc: 0.9143 - val_loss: 0.3227 - val_acc: 0.8832\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.2403 - acc: 0.9148 - val_loss: 0.3209 - val_acc: 0.8801\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.2406 - acc: 0.9143 - val_loss: 0.3234 - val_acc: 0.8809\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.2394 - acc: 0.9180 - val_loss: 0.3234 - val_acc: 0.8809\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.2391 - acc: 0.9164 - val_loss: 0.3330 - val_acc: 0.8754\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.2393 - acc: 0.9216 - val_loss: 0.3214 - val_acc: 0.8777\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.2389 - acc: 0.9169 - val_loss: 0.3241 - val_acc: 0.8801\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.2383 - acc: 0.9211 - val_loss: 0.3264 - val_acc: 0.8793\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.2383 - acc: 0.9227 - val_loss: 0.3226 - val_acc: 0.8832\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.2379 - acc: 0.9159 - val_loss: 0.3324 - val_acc: 0.8785\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.2386 - acc: 0.9180 - val_loss: 0.3189 - val_acc: 0.8856\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.2380 - acc: 0.9180 - val_loss: 0.3239 - val_acc: 0.8785\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.2369 - acc: 0.9206 - val_loss: 0.3271 - val_acc: 0.8770\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.2369 - acc: 0.9237 - val_loss: 0.3339 - val_acc: 0.8723\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.2378 - acc: 0.9227 - val_loss: 0.3260 - val_acc: 0.8801\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.2361 - acc: 0.9190 - val_loss: 0.3331 - val_acc: 0.8738\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.2357 - acc: 0.9185 - val_loss: 0.3399 - val_acc: 0.8691\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.2366 - acc: 0.9242 - val_loss: 0.3197 - val_acc: 0.8817\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.2357 - acc: 0.9180 - val_loss: 0.3318 - val_acc: 0.8793\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.2355 - acc: 0.9227 - val_loss: 0.3185 - val_acc: 0.8848\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.2347 - acc: 0.9206 - val_loss: 0.3185 - val_acc: 0.8832\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.2342 - acc: 0.9180 - val_loss: 0.3242 - val_acc: 0.8817\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 0.2337 - acc: 0.9222 - val_loss: 0.3229 - val_acc: 0.8793\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.2335 - acc: 0.9232 - val_loss: 0.3191 - val_acc: 0.8832\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.2333 - acc: 0.9222 - val_loss: 0.3224 - val_acc: 0.8809\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.2327 - acc: 0.9248 - val_loss: 0.3184 - val_acc: 0.8895\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.2332 - acc: 0.9227 - val_loss: 0.3222 - val_acc: 0.8871\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.2334 - acc: 0.9232 - val_loss: 0.3208 - val_acc: 0.8824\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.2320 - acc: 0.9237 - val_loss: 0.3195 - val_acc: 0.8817\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.2321 - acc: 0.9258 - val_loss: 0.3174 - val_acc: 0.8848\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.2318 - acc: 0.9232 - val_loss: 0.3189 - val_acc: 0.8895\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.2316 - acc: 0.9253 - val_loss: 0.3180 - val_acc: 0.8879\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.2317 - acc: 0.9222 - val_loss: 0.3189 - val_acc: 0.8824\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.2310 - acc: 0.9227 - val_loss: 0.3218 - val_acc: 0.8801\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.2305 - acc: 0.9242 - val_loss: 0.3192 - val_acc: 0.8832\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.2303 - acc: 0.9253 - val_loss: 0.3275 - val_acc: 0.8832\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.2303 - acc: 0.9258 - val_loss: 0.3173 - val_acc: 0.8879\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.2311 - acc: 0.9237 - val_loss: 0.3169 - val_acc: 0.8864\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.2296 - acc: 0.9232 - val_loss: 0.3219 - val_acc: 0.8824\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.2289 - acc: 0.9242 - val_loss: 0.3187 - val_acc: 0.8848\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.2287 - acc: 0.9242 - val_loss: 0.3182 - val_acc: 0.8817\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.2278 - acc: 0.9201 - val_loss: 0.3178 - val_acc: 0.8832\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.2281 - acc: 0.9274 - val_loss: 0.3212 - val_acc: 0.8856\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.2283 - acc: 0.9269 - val_loss: 0.3162 - val_acc: 0.8848\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.2279 - acc: 0.9232 - val_loss: 0.3204 - val_acc: 0.8864\n",
      "Epoch 449/1000\n",
      " - 0s - loss: 0.2278 - acc: 0.9269 - val_loss: 0.3174 - val_acc: 0.8918\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.2274 - acc: 0.9269 - val_loss: 0.3169 - val_acc: 0.8895\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.2266 - acc: 0.9263 - val_loss: 0.3177 - val_acc: 0.8856\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.2261 - acc: 0.9274 - val_loss: 0.3162 - val_acc: 0.8840\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.2261 - acc: 0.9242 - val_loss: 0.3195 - val_acc: 0.8840\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.2259 - acc: 0.9284 - val_loss: 0.3179 - val_acc: 0.8887\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.2259 - acc: 0.9258 - val_loss: 0.3205 - val_acc: 0.8856\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.2256 - acc: 0.9227 - val_loss: 0.3172 - val_acc: 0.8864\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.2249 - acc: 0.9274 - val_loss: 0.3174 - val_acc: 0.8911\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.2247 - acc: 0.9263 - val_loss: 0.3166 - val_acc: 0.8871\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.2239 - acc: 0.9284 - val_loss: 0.3156 - val_acc: 0.8879\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.2242 - acc: 0.9258 - val_loss: 0.3205 - val_acc: 0.8840\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.2243 - acc: 0.9253 - val_loss: 0.3187 - val_acc: 0.8832\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.2240 - acc: 0.9300 - val_loss: 0.3172 - val_acc: 0.8879\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.2235 - acc: 0.9269 - val_loss: 0.3272 - val_acc: 0.8793\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.2233 - acc: 0.9295 - val_loss: 0.3153 - val_acc: 0.8864\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.2226 - acc: 0.9279 - val_loss: 0.3175 - val_acc: 0.8887\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.2223 - acc: 0.9300 - val_loss: 0.3255 - val_acc: 0.8785\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.2221 - acc: 0.9289 - val_loss: 0.3313 - val_acc: 0.8746\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.2217 - acc: 0.9300 - val_loss: 0.3199 - val_acc: 0.8824\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.2212 - acc: 0.9289 - val_loss: 0.3170 - val_acc: 0.8856\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.2208 - acc: 0.9310 - val_loss: 0.3207 - val_acc: 0.8817\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.2207 - acc: 0.9295 - val_loss: 0.3199 - val_acc: 0.8824\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.2207 - acc: 0.9310 - val_loss: 0.3210 - val_acc: 0.8832\n",
      "Epoch 473/1000\n",
      " - 0s - loss: 0.2199 - acc: 0.9300 - val_loss: 0.3208 - val_acc: 0.8801\n",
      "Epoch 474/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.2198 - acc: 0.9316 - val_loss: 0.3164 - val_acc: 0.8864\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.2197 - acc: 0.9300 - val_loss: 0.3163 - val_acc: 0.8895\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.2201 - acc: 0.9316 - val_loss: 0.3202 - val_acc: 0.8832\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.2194 - acc: 0.9321 - val_loss: 0.3152 - val_acc: 0.8879\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.2187 - acc: 0.9305 - val_loss: 0.3234 - val_acc: 0.8793\n",
      "Epoch 479/1000\n",
      " - 0s - loss: 0.2193 - acc: 0.9305 - val_loss: 0.3181 - val_acc: 0.8864\n",
      "Epoch 480/1000\n",
      " - 0s - loss: 0.2198 - acc: 0.9310 - val_loss: 0.3162 - val_acc: 0.8840\n",
      "Epoch 481/1000\n",
      " - 0s - loss: 0.2179 - acc: 0.9305 - val_loss: 0.3269 - val_acc: 0.8824\n",
      "Epoch 482/1000\n",
      " - 0s - loss: 0.2184 - acc: 0.9352 - val_loss: 0.3204 - val_acc: 0.8832\n",
      "Epoch 483/1000\n",
      " - 0s - loss: 0.2180 - acc: 0.9253 - val_loss: 0.3153 - val_acc: 0.8871\n",
      "Epoch 484/1000\n",
      " - 0s - loss: 0.2171 - acc: 0.9326 - val_loss: 0.3142 - val_acc: 0.8864\n",
      "Epoch 485/1000\n",
      " - 0s - loss: 0.2171 - acc: 0.9300 - val_loss: 0.3151 - val_acc: 0.8856\n",
      "Epoch 486/1000\n",
      " - 0s - loss: 0.2171 - acc: 0.9310 - val_loss: 0.3241 - val_acc: 0.8864\n",
      "Epoch 487/1000\n",
      " - 0s - loss: 0.2170 - acc: 0.9305 - val_loss: 0.3152 - val_acc: 0.8848\n",
      "Epoch 488/1000\n",
      " - 0s - loss: 0.2161 - acc: 0.9295 - val_loss: 0.3162 - val_acc: 0.8840\n",
      "Epoch 489/1000\n",
      " - 0s - loss: 0.2160 - acc: 0.9321 - val_loss: 0.3166 - val_acc: 0.8887\n",
      "Epoch 490/1000\n",
      " - 0s - loss: 0.2160 - acc: 0.9310 - val_loss: 0.3141 - val_acc: 0.8879\n",
      "Epoch 491/1000\n",
      " - 0s - loss: 0.2156 - acc: 0.9305 - val_loss: 0.3316 - val_acc: 0.8746\n",
      "Epoch 492/1000\n",
      " - 0s - loss: 0.2154 - acc: 0.9363 - val_loss: 0.3128 - val_acc: 0.8871\n",
      "Epoch 493/1000\n",
      " - 0s - loss: 0.2151 - acc: 0.9289 - val_loss: 0.3148 - val_acc: 0.8848\n",
      "Epoch 494/1000\n",
      " - 0s - loss: 0.2151 - acc: 0.9326 - val_loss: 0.3206 - val_acc: 0.8832\n",
      "Epoch 495/1000\n",
      " - 0s - loss: 0.2146 - acc: 0.9342 - val_loss: 0.3252 - val_acc: 0.8879\n",
      "Epoch 496/1000\n",
      " - 0s - loss: 0.2148 - acc: 0.9347 - val_loss: 0.3179 - val_acc: 0.8848\n",
      "Epoch 497/1000\n",
      " - 0s - loss: 0.2132 - acc: 0.9342 - val_loss: 0.3168 - val_acc: 0.8895\n",
      "Epoch 498/1000\n",
      " - 0s - loss: 0.2135 - acc: 0.9316 - val_loss: 0.3168 - val_acc: 0.8871\n",
      "Epoch 499/1000\n",
      " - 0s - loss: 0.2131 - acc: 0.9321 - val_loss: 0.3159 - val_acc: 0.8856\n",
      "Epoch 500/1000\n",
      " - 0s - loss: 0.2128 - acc: 0.9352 - val_loss: 0.3183 - val_acc: 0.8903\n",
      "Epoch 501/1000\n",
      " - 0s - loss: 0.2129 - acc: 0.9326 - val_loss: 0.3155 - val_acc: 0.8887\n",
      "Epoch 502/1000\n",
      " - 0s - loss: 0.2126 - acc: 0.9352 - val_loss: 0.3162 - val_acc: 0.8848\n",
      "Epoch 503/1000\n",
      " - 0s - loss: 0.2123 - acc: 0.9326 - val_loss: 0.3211 - val_acc: 0.8801\n",
      "Epoch 504/1000\n",
      " - 0s - loss: 0.2116 - acc: 0.9347 - val_loss: 0.3180 - val_acc: 0.8871\n",
      "Epoch 505/1000\n",
      " - 0s - loss: 0.2118 - acc: 0.9336 - val_loss: 0.3137 - val_acc: 0.8918\n",
      "Epoch 506/1000\n",
      " - 0s - loss: 0.2110 - acc: 0.9347 - val_loss: 0.3131 - val_acc: 0.8887\n",
      "Epoch 507/1000\n",
      " - 0s - loss: 0.2109 - acc: 0.9357 - val_loss: 0.3137 - val_acc: 0.8864\n",
      "Epoch 508/1000\n",
      " - 0s - loss: 0.2108 - acc: 0.9352 - val_loss: 0.3131 - val_acc: 0.8903\n",
      "Epoch 509/1000\n",
      " - 0s - loss: 0.2108 - acc: 0.9331 - val_loss: 0.3245 - val_acc: 0.8793\n",
      "Epoch 510/1000\n",
      " - 0s - loss: 0.2100 - acc: 0.9342 - val_loss: 0.3147 - val_acc: 0.8856\n",
      "Epoch 511/1000\n",
      " - 0s - loss: 0.2099 - acc: 0.9373 - val_loss: 0.3126 - val_acc: 0.8903\n",
      "Epoch 512/1000\n",
      " - 0s - loss: 0.2098 - acc: 0.9352 - val_loss: 0.3183 - val_acc: 0.8824\n",
      "Epoch 513/1000\n",
      " - 0s - loss: 0.2092 - acc: 0.9363 - val_loss: 0.3130 - val_acc: 0.8903\n",
      "Epoch 514/1000\n",
      " - 0s - loss: 0.2089 - acc: 0.9352 - val_loss: 0.3212 - val_acc: 0.8785\n",
      "Epoch 515/1000\n",
      " - 0s - loss: 0.2088 - acc: 0.9378 - val_loss: 0.3142 - val_acc: 0.8871\n",
      "Epoch 516/1000\n",
      " - 0s - loss: 0.2084 - acc: 0.9342 - val_loss: 0.3120 - val_acc: 0.8887\n",
      "Epoch 517/1000\n",
      " - 0s - loss: 0.2082 - acc: 0.9357 - val_loss: 0.3152 - val_acc: 0.8848\n",
      "Epoch 518/1000\n",
      " - 0s - loss: 0.2079 - acc: 0.9363 - val_loss: 0.3175 - val_acc: 0.8856\n",
      "Epoch 519/1000\n",
      " - 0s - loss: 0.2074 - acc: 0.9363 - val_loss: 0.3160 - val_acc: 0.8848\n",
      "Epoch 520/1000\n",
      " - 0s - loss: 0.2075 - acc: 0.9368 - val_loss: 0.3121 - val_acc: 0.8895\n",
      "Epoch 521/1000\n",
      " - 0s - loss: 0.2075 - acc: 0.9352 - val_loss: 0.3173 - val_acc: 0.8840\n",
      "Epoch 522/1000\n",
      " - 0s - loss: 0.2067 - acc: 0.9352 - val_loss: 0.3126 - val_acc: 0.8887\n",
      "Epoch 523/1000\n",
      " - 0s - loss: 0.2060 - acc: 0.9352 - val_loss: 0.3317 - val_acc: 0.8777\n",
      "Epoch 524/1000\n",
      " - 0s - loss: 0.2072 - acc: 0.9368 - val_loss: 0.3148 - val_acc: 0.8871\n",
      "Epoch 525/1000\n",
      " - 0s - loss: 0.2055 - acc: 0.9368 - val_loss: 0.3134 - val_acc: 0.8879\n",
      "Epoch 526/1000\n",
      " - 0s - loss: 0.2055 - acc: 0.9363 - val_loss: 0.3116 - val_acc: 0.8895\n",
      "Epoch 527/1000\n",
      " - 0s - loss: 0.2053 - acc: 0.9331 - val_loss: 0.3160 - val_acc: 0.8840\n",
      "Epoch 528/1000\n",
      " - 0s - loss: 0.2047 - acc: 0.9357 - val_loss: 0.3127 - val_acc: 0.8887\n",
      "Epoch 529/1000\n",
      " - 0s - loss: 0.2046 - acc: 0.9373 - val_loss: 0.3174 - val_acc: 0.8817\n",
      "Epoch 530/1000\n",
      " - 0s - loss: 0.2049 - acc: 0.9383 - val_loss: 0.3178 - val_acc: 0.8840\n",
      "Epoch 531/1000\n",
      " - 0s - loss: 0.2040 - acc: 0.9378 - val_loss: 0.3144 - val_acc: 0.8840\n",
      "Epoch 532/1000\n",
      " - 0s - loss: 0.2035 - acc: 0.9363 - val_loss: 0.3114 - val_acc: 0.8911\n",
      "Epoch 533/1000\n",
      " - 0s - loss: 0.2036 - acc: 0.9342 - val_loss: 0.3202 - val_acc: 0.8848\n",
      "Epoch 534/1000\n",
      " - 0s - loss: 0.2033 - acc: 0.9368 - val_loss: 0.3147 - val_acc: 0.8879\n",
      "Epoch 535/1000\n",
      " - 0s - loss: 0.2034 - acc: 0.9368 - val_loss: 0.3113 - val_acc: 0.8903\n",
      "Epoch 536/1000\n",
      " - 0s - loss: 0.2026 - acc: 0.9389 - val_loss: 0.3109 - val_acc: 0.8903\n",
      "Epoch 537/1000\n",
      " - 0s - loss: 0.2020 - acc: 0.9368 - val_loss: 0.3130 - val_acc: 0.8864\n",
      "Epoch 538/1000\n",
      " - 0s - loss: 0.2030 - acc: 0.9342 - val_loss: 0.3145 - val_acc: 0.8848\n",
      "Epoch 539/1000\n",
      " - 0s - loss: 0.2027 - acc: 0.9389 - val_loss: 0.3160 - val_acc: 0.8887\n",
      "Epoch 540/1000\n",
      " - 0s - loss: 0.2016 - acc: 0.9363 - val_loss: 0.3129 - val_acc: 0.8895\n",
      "Epoch 541/1000\n",
      " - 0s - loss: 0.2019 - acc: 0.9394 - val_loss: 0.3106 - val_acc: 0.8911\n",
      "Epoch 542/1000\n",
      " - 0s - loss: 0.2020 - acc: 0.9368 - val_loss: 0.3113 - val_acc: 0.8864\n",
      "Epoch 543/1000\n",
      " - 0s - loss: 0.2007 - acc: 0.9420 - val_loss: 0.3102 - val_acc: 0.8864\n",
      "Epoch 544/1000\n",
      " - 0s - loss: 0.2008 - acc: 0.9394 - val_loss: 0.3107 - val_acc: 0.8895\n",
      "Epoch 545/1000\n",
      " - 0s - loss: 0.2007 - acc: 0.9378 - val_loss: 0.3101 - val_acc: 0.8934\n",
      "Epoch 546/1000\n",
      " - 0s - loss: 0.2002 - acc: 0.9378 - val_loss: 0.3100 - val_acc: 0.8911\n",
      "Epoch 547/1000\n",
      " - 0s - loss: 0.1998 - acc: 0.9368 - val_loss: 0.3251 - val_acc: 0.8762\n",
      "Epoch 548/1000\n",
      " - 0s - loss: 0.2000 - acc: 0.9415 - val_loss: 0.3101 - val_acc: 0.8887\n",
      "Epoch 549/1000\n",
      " - 0s - loss: 0.1992 - acc: 0.9363 - val_loss: 0.3186 - val_acc: 0.8817\n",
      "Epoch 550/1000\n",
      " - 0s - loss: 0.1992 - acc: 0.9404 - val_loss: 0.3222 - val_acc: 0.8817\n",
      "Epoch 551/1000\n",
      " - 0s - loss: 0.1996 - acc: 0.9420 - val_loss: 0.3098 - val_acc: 0.8918\n",
      "Epoch 552/1000\n",
      " - 0s - loss: 0.1986 - acc: 0.9410 - val_loss: 0.3119 - val_acc: 0.8895\n",
      "Epoch 553/1000\n",
      " - 0s - loss: 0.1980 - acc: 0.9383 - val_loss: 0.3095 - val_acc: 0.8911\n",
      "Epoch 554/1000\n",
      " - 0s - loss: 0.1977 - acc: 0.9394 - val_loss: 0.3152 - val_acc: 0.8848\n",
      "Epoch 555/1000\n",
      " - 0s - loss: 0.1975 - acc: 0.9399 - val_loss: 0.3095 - val_acc: 0.8934\n",
      "Epoch 556/1000\n",
      " - 0s - loss: 0.1971 - acc: 0.9383 - val_loss: 0.3164 - val_acc: 0.8832\n",
      "Epoch 557/1000\n",
      " - 0s - loss: 0.1972 - acc: 0.9420 - val_loss: 0.3147 - val_acc: 0.8848\n",
      "Epoch 558/1000\n",
      " - 0s - loss: 0.1970 - acc: 0.9394 - val_loss: 0.3171 - val_acc: 0.8832\n",
      "Epoch 559/1000\n",
      " - 0s - loss: 0.1966 - acc: 0.9404 - val_loss: 0.3092 - val_acc: 0.8887\n",
      "Epoch 560/1000\n",
      " - 0s - loss: 0.1967 - acc: 0.9389 - val_loss: 0.3158 - val_acc: 0.8848\n",
      "Epoch 561/1000\n",
      " - 0s - loss: 0.1966 - acc: 0.9404 - val_loss: 0.3136 - val_acc: 0.8879\n",
      "Epoch 562/1000\n",
      " - 0s - loss: 0.1964 - acc: 0.9425 - val_loss: 0.3125 - val_acc: 0.8864\n",
      "Epoch 563/1000\n",
      " - 0s - loss: 0.1953 - acc: 0.9425 - val_loss: 0.3183 - val_acc: 0.8895\n",
      "Epoch 564/1000\n",
      " - 0s - loss: 0.1955 - acc: 0.9441 - val_loss: 0.3098 - val_acc: 0.8864\n",
      "Epoch 565/1000\n",
      " - 0s - loss: 0.1952 - acc: 0.9352 - val_loss: 0.3186 - val_acc: 0.8840\n",
      "Epoch 566/1000\n",
      " - 0s - loss: 0.1947 - acc: 0.9431 - val_loss: 0.3118 - val_acc: 0.8887\n",
      "Epoch 567/1000\n",
      " - 0s - loss: 0.1941 - acc: 0.9451 - val_loss: 0.3156 - val_acc: 0.8824\n",
      "Epoch 568/1000\n",
      " - 0s - loss: 0.1948 - acc: 0.9415 - val_loss: 0.3164 - val_acc: 0.8856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569/1000\n",
      " - 0s - loss: 0.1945 - acc: 0.9404 - val_loss: 0.3179 - val_acc: 0.8809\n",
      "Epoch 570/1000\n",
      " - 0s - loss: 0.1940 - acc: 0.9441 - val_loss: 0.3178 - val_acc: 0.8817\n",
      "Epoch 571/1000\n",
      " - 0s - loss: 0.1933 - acc: 0.9462 - val_loss: 0.3098 - val_acc: 0.8895\n",
      "Epoch 572/1000\n",
      " - 0s - loss: 0.1926 - acc: 0.9415 - val_loss: 0.3138 - val_acc: 0.8832\n",
      "Epoch 573/1000\n",
      " - 0s - loss: 0.1924 - acc: 0.9420 - val_loss: 0.3114 - val_acc: 0.8871\n",
      "Epoch 574/1000\n",
      " - 0s - loss: 0.1926 - acc: 0.9431 - val_loss: 0.3297 - val_acc: 0.8715\n",
      "Epoch 575/1000\n",
      " - 0s - loss: 0.1936 - acc: 0.9431 - val_loss: 0.3098 - val_acc: 0.8911\n",
      "Epoch 576/1000\n",
      " - 0s - loss: 0.1927 - acc: 0.9404 - val_loss: 0.3129 - val_acc: 0.8824\n",
      "Epoch 577/1000\n",
      " - 0s - loss: 0.1918 - acc: 0.9446 - val_loss: 0.3281 - val_acc: 0.8785\n",
      "Epoch 578/1000\n",
      " - 0s - loss: 0.1921 - acc: 0.9415 - val_loss: 0.3090 - val_acc: 0.8911\n",
      "Epoch 579/1000\n",
      " - 0s - loss: 0.1910 - acc: 0.9451 - val_loss: 0.3126 - val_acc: 0.8856\n",
      "Epoch 580/1000\n",
      " - 0s - loss: 0.1915 - acc: 0.9446 - val_loss: 0.3080 - val_acc: 0.8895\n",
      "Epoch 581/1000\n",
      " - 0s - loss: 0.1914 - acc: 0.9462 - val_loss: 0.3129 - val_acc: 0.8824\n",
      "Epoch 582/1000\n",
      " - 0s - loss: 0.1903 - acc: 0.9467 - val_loss: 0.3079 - val_acc: 0.8879\n",
      "Epoch 583/1000\n",
      " - 0s - loss: 0.1900 - acc: 0.9441 - val_loss: 0.3086 - val_acc: 0.8911\n",
      "Epoch 584/1000\n",
      " - 0s - loss: 0.1900 - acc: 0.9441 - val_loss: 0.3175 - val_acc: 0.8793\n",
      "Epoch 585/1000\n",
      " - 0s - loss: 0.1899 - acc: 0.9467 - val_loss: 0.3155 - val_acc: 0.8817\n",
      "Epoch 586/1000\n",
      " - 0s - loss: 0.1889 - acc: 0.9457 - val_loss: 0.3206 - val_acc: 0.8801\n",
      "Epoch 587/1000\n",
      " - 0s - loss: 0.1895 - acc: 0.9446 - val_loss: 0.3103 - val_acc: 0.8903\n",
      "Epoch 588/1000\n",
      " - 0s - loss: 0.1886 - acc: 0.9431 - val_loss: 0.3108 - val_acc: 0.8911\n",
      "Epoch 589/1000\n",
      " - 0s - loss: 0.1883 - acc: 0.9457 - val_loss: 0.3255 - val_acc: 0.8824\n",
      "Epoch 590/1000\n",
      " - 0s - loss: 0.1891 - acc: 0.9472 - val_loss: 0.3109 - val_acc: 0.8864\n",
      "Epoch 591/1000\n",
      " - 0s - loss: 0.1886 - acc: 0.9441 - val_loss: 0.3077 - val_acc: 0.8926\n",
      "Epoch 592/1000\n",
      " - 0s - loss: 0.1875 - acc: 0.9451 - val_loss: 0.3116 - val_acc: 0.8903\n",
      "Epoch 593/1000\n",
      " - 0s - loss: 0.1877 - acc: 0.9436 - val_loss: 0.3086 - val_acc: 0.8918\n",
      "Epoch 594/1000\n",
      " - 0s - loss: 0.1873 - acc: 0.9457 - val_loss: 0.3107 - val_acc: 0.8879\n",
      "Epoch 595/1000\n",
      " - 0s - loss: 0.1872 - acc: 0.9478 - val_loss: 0.3183 - val_acc: 0.8824\n",
      "Epoch 596/1000\n",
      " - 0s - loss: 0.1875 - acc: 0.9457 - val_loss: 0.3113 - val_acc: 0.8832\n",
      "Epoch 597/1000\n",
      " - 0s - loss: 0.1869 - acc: 0.9472 - val_loss: 0.3083 - val_acc: 0.8903\n",
      "Epoch 598/1000\n",
      " - 0s - loss: 0.1867 - acc: 0.9462 - val_loss: 0.3145 - val_acc: 0.8895\n",
      "Epoch 599/1000\n",
      " - 0s - loss: 0.1864 - acc: 0.9462 - val_loss: 0.3075 - val_acc: 0.8918\n",
      "Epoch 600/1000\n",
      " - 0s - loss: 0.1859 - acc: 0.9457 - val_loss: 0.3089 - val_acc: 0.8887\n",
      "Epoch 601/1000\n",
      " - 0s - loss: 0.1854 - acc: 0.9493 - val_loss: 0.3117 - val_acc: 0.8848\n",
      "Epoch 602/1000\n",
      " - 0s - loss: 0.1860 - acc: 0.9462 - val_loss: 0.3093 - val_acc: 0.8887\n",
      "Epoch 603/1000\n",
      " - 0s - loss: 0.1857 - acc: 0.9441 - val_loss: 0.3096 - val_acc: 0.8879\n",
      "Epoch 604/1000\n",
      " - 0s - loss: 0.1850 - acc: 0.9478 - val_loss: 0.3094 - val_acc: 0.8864\n",
      "Epoch 605/1000\n",
      " - 0s - loss: 0.1846 - acc: 0.9478 - val_loss: 0.3069 - val_acc: 0.8918\n",
      "Epoch 606/1000\n",
      " - 0s - loss: 0.1847 - acc: 0.9483 - val_loss: 0.3082 - val_acc: 0.8911\n",
      "Epoch 607/1000\n",
      " - 0s - loss: 0.1839 - acc: 0.9472 - val_loss: 0.3079 - val_acc: 0.8864\n",
      "Epoch 608/1000\n",
      " - 0s - loss: 0.1847 - acc: 0.9478 - val_loss: 0.3159 - val_acc: 0.8824\n",
      "Epoch 609/1000\n",
      " - 0s - loss: 0.1837 - acc: 0.9451 - val_loss: 0.3113 - val_acc: 0.8864\n",
      "Epoch 610/1000\n",
      " - 0s - loss: 0.1834 - acc: 0.9478 - val_loss: 0.3085 - val_acc: 0.8911\n",
      "Epoch 611/1000\n",
      " - 0s - loss: 0.1829 - acc: 0.9488 - val_loss: 0.3075 - val_acc: 0.8926\n",
      "Epoch 612/1000\n",
      " - 0s - loss: 0.1823 - acc: 0.9472 - val_loss: 0.3071 - val_acc: 0.8926\n",
      "Epoch 613/1000\n",
      " - 0s - loss: 0.1825 - acc: 0.9478 - val_loss: 0.3146 - val_acc: 0.8840\n",
      "Epoch 614/1000\n",
      " - 0s - loss: 0.1832 - acc: 0.9488 - val_loss: 0.3203 - val_acc: 0.8840\n",
      "Epoch 615/1000\n",
      " - 0s - loss: 0.1829 - acc: 0.9478 - val_loss: 0.3195 - val_acc: 0.8785\n",
      "Epoch 616/1000\n",
      " - 0s - loss: 0.1830 - acc: 0.9514 - val_loss: 0.3121 - val_acc: 0.8832\n",
      "Epoch 617/1000\n",
      " - 0s - loss: 0.1816 - acc: 0.9509 - val_loss: 0.3066 - val_acc: 0.8911\n",
      "Epoch 618/1000\n",
      " - 0s - loss: 0.1811 - acc: 0.9514 - val_loss: 0.3068 - val_acc: 0.8918\n",
      "Epoch 619/1000\n",
      " - 0s - loss: 0.1808 - acc: 0.9488 - val_loss: 0.3079 - val_acc: 0.8895\n",
      "Epoch 620/1000\n",
      " - 0s - loss: 0.1809 - acc: 0.9488 - val_loss: 0.3092 - val_acc: 0.8895\n",
      "Epoch 621/1000\n",
      " - 0s - loss: 0.1807 - acc: 0.9493 - val_loss: 0.3103 - val_acc: 0.8864\n",
      "Epoch 622/1000\n",
      " - 0s - loss: 0.1801 - acc: 0.9514 - val_loss: 0.3101 - val_acc: 0.8864\n",
      "Epoch 623/1000\n",
      " - 0s - loss: 0.1800 - acc: 0.9498 - val_loss: 0.3073 - val_acc: 0.8911\n",
      "Epoch 624/1000\n",
      " - 0s - loss: 0.1795 - acc: 0.9488 - val_loss: 0.3134 - val_acc: 0.8840\n",
      "Epoch 625/1000\n",
      " - 0s - loss: 0.1796 - acc: 0.9509 - val_loss: 0.3071 - val_acc: 0.8895\n",
      "Epoch 626/1000\n",
      " - 0s - loss: 0.1786 - acc: 0.9509 - val_loss: 0.3066 - val_acc: 0.8911\n",
      "Epoch 627/1000\n",
      " - 0s - loss: 0.1783 - acc: 0.9498 - val_loss: 0.3133 - val_acc: 0.8856\n",
      "Epoch 628/1000\n",
      " - 0s - loss: 0.1790 - acc: 0.9504 - val_loss: 0.3066 - val_acc: 0.8895\n",
      "Epoch 629/1000\n",
      " - 0s - loss: 0.1787 - acc: 0.9488 - val_loss: 0.3104 - val_acc: 0.8871\n",
      "Epoch 630/1000\n",
      " - 0s - loss: 0.1786 - acc: 0.9535 - val_loss: 0.3127 - val_acc: 0.8879\n",
      "Epoch 631/1000\n",
      " - 0s - loss: 0.1782 - acc: 0.9514 - val_loss: 0.3130 - val_acc: 0.8840\n",
      "Epoch 632/1000\n",
      " - 0s - loss: 0.1776 - acc: 0.9519 - val_loss: 0.3137 - val_acc: 0.8864\n",
      "Epoch 633/1000\n",
      " - 0s - loss: 0.1785 - acc: 0.9504 - val_loss: 0.3090 - val_acc: 0.8887\n",
      "Epoch 634/1000\n",
      " - 0s - loss: 0.1778 - acc: 0.9478 - val_loss: 0.3097 - val_acc: 0.8895\n",
      "Epoch 635/1000\n",
      " - 0s - loss: 0.1770 - acc: 0.9525 - val_loss: 0.3064 - val_acc: 0.8911\n",
      "Epoch 636/1000\n",
      " - 0s - loss: 0.1769 - acc: 0.9519 - val_loss: 0.3092 - val_acc: 0.8911\n",
      "Epoch 637/1000\n",
      " - 0s - loss: 0.1768 - acc: 0.9519 - val_loss: 0.3072 - val_acc: 0.8918\n",
      "Epoch 638/1000\n",
      " - 0s - loss: 0.1760 - acc: 0.9509 - val_loss: 0.3106 - val_acc: 0.8832\n",
      "Epoch 639/1000\n",
      " - 0s - loss: 0.1763 - acc: 0.9530 - val_loss: 0.3095 - val_acc: 0.8887\n",
      "Epoch 640/1000\n",
      " - 0s - loss: 0.1757 - acc: 0.9525 - val_loss: 0.3119 - val_acc: 0.8871\n",
      "Epoch 641/1000\n",
      " - 0s - loss: 0.1759 - acc: 0.9525 - val_loss: 0.3074 - val_acc: 0.8864\n",
      "Epoch 642/1000\n",
      " - 0s - loss: 0.1753 - acc: 0.9525 - val_loss: 0.3160 - val_acc: 0.8840\n",
      "Epoch 643/1000\n",
      " - 0s - loss: 0.1753 - acc: 0.9514 - val_loss: 0.3099 - val_acc: 0.8856\n",
      "Epoch 644/1000\n",
      " - 0s - loss: 0.1751 - acc: 0.9540 - val_loss: 0.3094 - val_acc: 0.8879\n",
      "Epoch 645/1000\n",
      " - 0s - loss: 0.1748 - acc: 0.9530 - val_loss: 0.3078 - val_acc: 0.8887\n",
      "Epoch 646/1000\n",
      " - 0s - loss: 0.1739 - acc: 0.9540 - val_loss: 0.3064 - val_acc: 0.8911\n",
      "Epoch 647/1000\n",
      " - 0s - loss: 0.1749 - acc: 0.9514 - val_loss: 0.3061 - val_acc: 0.8926\n",
      "Epoch 648/1000\n",
      " - 0s - loss: 0.1737 - acc: 0.9530 - val_loss: 0.3070 - val_acc: 0.8895\n",
      "Epoch 649/1000\n",
      " - 0s - loss: 0.1736 - acc: 0.9551 - val_loss: 0.3127 - val_acc: 0.8848\n",
      "Epoch 650/1000\n",
      " - 0s - loss: 0.1731 - acc: 0.9566 - val_loss: 0.3066 - val_acc: 0.8903\n",
      "Epoch 651/1000\n",
      " - 0s - loss: 0.1734 - acc: 0.9551 - val_loss: 0.3139 - val_acc: 0.8848\n",
      "Epoch 652/1000\n",
      " - 0s - loss: 0.1748 - acc: 0.9519 - val_loss: 0.3127 - val_acc: 0.8864\n",
      "Epoch 653/1000\n",
      " - 0s - loss: 0.1732 - acc: 0.9525 - val_loss: 0.3075 - val_acc: 0.8911\n",
      "Epoch 654/1000\n",
      " - 0s - loss: 0.1725 - acc: 0.9540 - val_loss: 0.3088 - val_acc: 0.8856\n",
      "Epoch 655/1000\n",
      " - 0s - loss: 0.1723 - acc: 0.9556 - val_loss: 0.3097 - val_acc: 0.8879\n",
      "Epoch 656/1000\n",
      " - 0s - loss: 0.1717 - acc: 0.9540 - val_loss: 0.3101 - val_acc: 0.8903\n",
      "Epoch 657/1000\n",
      " - 0s - loss: 0.1725 - acc: 0.9525 - val_loss: 0.3083 - val_acc: 0.8871\n",
      "Epoch 658/1000\n",
      " - 0s - loss: 0.1712 - acc: 0.9577 - val_loss: 0.3058 - val_acc: 0.8918\n",
      "Epoch 659/1000\n",
      " - 0s - loss: 0.1712 - acc: 0.9535 - val_loss: 0.3053 - val_acc: 0.8895\n",
      "Epoch 660/1000\n",
      " - 0s - loss: 0.1711 - acc: 0.9530 - val_loss: 0.3062 - val_acc: 0.8942\n",
      "Epoch 661/1000\n",
      " - 0s - loss: 0.1711 - acc: 0.9545 - val_loss: 0.3125 - val_acc: 0.8856\n",
      "Epoch 662/1000\n",
      " - 0s - loss: 0.1709 - acc: 0.9551 - val_loss: 0.3074 - val_acc: 0.8879\n",
      "Epoch 663/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1703 - acc: 0.9540 - val_loss: 0.3065 - val_acc: 0.8918\n",
      "Epoch 664/1000\n",
      " - 0s - loss: 0.1697 - acc: 0.9535 - val_loss: 0.3114 - val_acc: 0.8848\n",
      "Epoch 665/1000\n",
      " - 0s - loss: 0.1706 - acc: 0.9545 - val_loss: 0.3060 - val_acc: 0.8911\n",
      "Epoch 666/1000\n",
      " - 0s - loss: 0.1698 - acc: 0.9551 - val_loss: 0.3170 - val_acc: 0.8809\n",
      "Epoch 667/1000\n",
      " - 0s - loss: 0.1702 - acc: 0.9556 - val_loss: 0.3167 - val_acc: 0.8801\n",
      "Epoch 668/1000\n",
      " - 0s - loss: 0.1699 - acc: 0.9545 - val_loss: 0.3078 - val_acc: 0.8848\n",
      "Epoch 669/1000\n",
      " - 0s - loss: 0.1692 - acc: 0.9551 - val_loss: 0.3170 - val_acc: 0.8832\n",
      "Epoch 670/1000\n",
      " - 0s - loss: 0.1693 - acc: 0.9561 - val_loss: 0.3075 - val_acc: 0.8864\n",
      "Epoch 671/1000\n",
      " - 0s - loss: 0.1692 - acc: 0.9519 - val_loss: 0.3050 - val_acc: 0.8903\n",
      "Epoch 672/1000\n",
      " - 0s - loss: 0.1679 - acc: 0.9545 - val_loss: 0.3085 - val_acc: 0.8895\n",
      "Epoch 673/1000\n",
      " - 0s - loss: 0.1683 - acc: 0.9545 - val_loss: 0.3050 - val_acc: 0.8911\n",
      "Epoch 674/1000\n",
      " - 0s - loss: 0.1681 - acc: 0.9545 - val_loss: 0.3050 - val_acc: 0.8911\n",
      "Epoch 675/1000\n",
      " - 0s - loss: 0.1682 - acc: 0.9519 - val_loss: 0.3056 - val_acc: 0.8887\n",
      "Epoch 676/1000\n",
      " - 0s - loss: 0.1671 - acc: 0.9577 - val_loss: 0.3210 - val_acc: 0.8770\n",
      "Epoch 677/1000\n",
      " - 0s - loss: 0.1678 - acc: 0.9577 - val_loss: 0.3049 - val_acc: 0.8887\n",
      "Epoch 678/1000\n",
      " - 0s - loss: 0.1670 - acc: 0.9561 - val_loss: 0.3068 - val_acc: 0.8879\n",
      "Epoch 679/1000\n",
      " - 0s - loss: 0.1669 - acc: 0.9561 - val_loss: 0.3045 - val_acc: 0.8903\n",
      "Epoch 680/1000\n",
      " - 0s - loss: 0.1661 - acc: 0.9566 - val_loss: 0.3230 - val_acc: 0.8770\n",
      "Epoch 681/1000\n",
      " - 0s - loss: 0.1671 - acc: 0.9572 - val_loss: 0.3091 - val_acc: 0.8856\n",
      "Epoch 682/1000\n",
      " - 0s - loss: 0.1655 - acc: 0.9561 - val_loss: 0.3068 - val_acc: 0.8871\n",
      "Epoch 683/1000\n",
      " - 0s - loss: 0.1651 - acc: 0.9566 - val_loss: 0.3067 - val_acc: 0.8848\n",
      "Epoch 684/1000\n",
      " - 0s - loss: 0.1653 - acc: 0.9561 - val_loss: 0.3142 - val_acc: 0.8840\n",
      "Epoch 685/1000\n",
      " - 0s - loss: 0.1653 - acc: 0.9582 - val_loss: 0.3062 - val_acc: 0.8903\n",
      "Epoch 686/1000\n",
      " - 0s - loss: 0.1650 - acc: 0.9566 - val_loss: 0.3046 - val_acc: 0.8895\n",
      "Epoch 687/1000\n",
      " - 0s - loss: 0.1653 - acc: 0.9572 - val_loss: 0.3052 - val_acc: 0.8918\n",
      "Epoch 688/1000\n",
      " - 0s - loss: 0.1639 - acc: 0.9566 - val_loss: 0.3101 - val_acc: 0.8848\n",
      "Epoch 689/1000\n",
      " - 0s - loss: 0.1642 - acc: 0.9577 - val_loss: 0.3053 - val_acc: 0.8887\n",
      "Epoch 690/1000\n",
      " - 0s - loss: 0.1638 - acc: 0.9582 - val_loss: 0.3047 - val_acc: 0.8903\n",
      "Epoch 691/1000\n",
      " - 0s - loss: 0.1634 - acc: 0.9598 - val_loss: 0.3116 - val_acc: 0.8809\n",
      "Epoch 692/1000\n",
      " - 0s - loss: 0.1636 - acc: 0.9566 - val_loss: 0.3079 - val_acc: 0.8895\n",
      "Epoch 693/1000\n",
      " - 0s - loss: 0.1629 - acc: 0.9582 - val_loss: 0.3040 - val_acc: 0.8903\n",
      "Epoch 694/1000\n",
      " - 0s - loss: 0.1626 - acc: 0.9577 - val_loss: 0.3040 - val_acc: 0.8903\n",
      "Epoch 695/1000\n",
      " - 0s - loss: 0.1629 - acc: 0.9556 - val_loss: 0.3067 - val_acc: 0.8918\n",
      "Epoch 696/1000\n",
      " - 0s - loss: 0.1628 - acc: 0.9598 - val_loss: 0.3046 - val_acc: 0.8895\n",
      "Epoch 697/1000\n",
      " - 0s - loss: 0.1624 - acc: 0.9556 - val_loss: 0.3063 - val_acc: 0.8895\n",
      "Epoch 698/1000\n",
      " - 0s - loss: 0.1619 - acc: 0.9577 - val_loss: 0.3077 - val_acc: 0.8848\n",
      "Epoch 699/1000\n",
      " - 0s - loss: 0.1622 - acc: 0.9566 - val_loss: 0.3040 - val_acc: 0.8895\n",
      "Epoch 700/1000\n",
      " - 0s - loss: 0.1616 - acc: 0.9556 - val_loss: 0.3098 - val_acc: 0.8895\n",
      "Epoch 701/1000\n",
      " - 0s - loss: 0.1615 - acc: 0.9577 - val_loss: 0.3042 - val_acc: 0.8918\n",
      "Epoch 702/1000\n",
      " - 0s - loss: 0.1613 - acc: 0.9572 - val_loss: 0.3071 - val_acc: 0.8856\n",
      "Epoch 703/1000\n",
      " - 0s - loss: 0.1607 - acc: 0.9598 - val_loss: 0.3100 - val_acc: 0.8887\n",
      "Epoch 704/1000\n",
      " - 0s - loss: 0.1607 - acc: 0.9582 - val_loss: 0.3046 - val_acc: 0.8895\n",
      "Epoch 705/1000\n",
      " - 0s - loss: 0.1604 - acc: 0.9577 - val_loss: 0.3057 - val_acc: 0.8903\n",
      "Epoch 706/1000\n",
      " - 0s - loss: 0.1604 - acc: 0.9587 - val_loss: 0.3064 - val_acc: 0.8856\n",
      "Epoch 707/1000\n",
      " - 0s - loss: 0.1599 - acc: 0.9603 - val_loss: 0.3049 - val_acc: 0.8887\n",
      "Epoch 708/1000\n",
      " - 0s - loss: 0.1597 - acc: 0.9582 - val_loss: 0.3067 - val_acc: 0.8887\n",
      "Epoch 709/1000\n",
      " - 0s - loss: 0.1592 - acc: 0.9598 - val_loss: 0.3089 - val_acc: 0.8832\n",
      "Epoch 710/1000\n",
      " - 0s - loss: 0.1590 - acc: 0.9592 - val_loss: 0.3047 - val_acc: 0.8879\n",
      "Epoch 711/1000\n",
      " - 0s - loss: 0.1589 - acc: 0.9582 - val_loss: 0.3044 - val_acc: 0.8903\n",
      "Epoch 712/1000\n",
      " - 0s - loss: 0.1588 - acc: 0.9587 - val_loss: 0.3046 - val_acc: 0.8918\n",
      "Epoch 713/1000\n",
      " - 0s - loss: 0.1593 - acc: 0.9592 - val_loss: 0.3105 - val_acc: 0.8817\n",
      "Epoch 714/1000\n",
      " - 0s - loss: 0.1589 - acc: 0.9592 - val_loss: 0.3122 - val_acc: 0.8824\n",
      "Epoch 715/1000\n",
      " - 0s - loss: 0.1583 - acc: 0.9598 - val_loss: 0.3053 - val_acc: 0.8879\n",
      "Epoch 716/1000\n",
      " - 0s - loss: 0.1577 - acc: 0.9603 - val_loss: 0.3036 - val_acc: 0.8918\n",
      "Epoch 717/1000\n",
      " - 0s - loss: 0.1579 - acc: 0.9592 - val_loss: 0.3053 - val_acc: 0.8887\n",
      "Epoch 718/1000\n",
      " - 0s - loss: 0.1574 - acc: 0.9587 - val_loss: 0.3066 - val_acc: 0.8848\n",
      "Epoch 719/1000\n",
      " - 0s - loss: 0.1572 - acc: 0.9629 - val_loss: 0.3056 - val_acc: 0.8864\n",
      "Epoch 720/1000\n",
      " - 0s - loss: 0.1567 - acc: 0.9592 - val_loss: 0.3054 - val_acc: 0.8871\n",
      "Epoch 721/1000\n",
      " - 0s - loss: 0.1568 - acc: 0.9582 - val_loss: 0.3050 - val_acc: 0.8871\n",
      "Epoch 722/1000\n",
      " - 0s - loss: 0.1566 - acc: 0.9598 - val_loss: 0.3051 - val_acc: 0.8864\n",
      "Epoch 723/1000\n",
      " - 0s - loss: 0.1564 - acc: 0.9592 - val_loss: 0.3057 - val_acc: 0.8856\n",
      "Epoch 724/1000\n",
      " - 0s - loss: 0.1562 - acc: 0.9619 - val_loss: 0.3092 - val_acc: 0.8824\n",
      "Epoch 725/1000\n",
      " - 0s - loss: 0.1557 - acc: 0.9598 - val_loss: 0.3047 - val_acc: 0.8856\n",
      "Epoch 726/1000\n",
      " - 0s - loss: 0.1556 - acc: 0.9613 - val_loss: 0.3106 - val_acc: 0.8832\n",
      "Epoch 727/1000\n",
      " - 0s - loss: 0.1552 - acc: 0.9608 - val_loss: 0.3040 - val_acc: 0.8887\n",
      "Epoch 728/1000\n",
      " - 0s - loss: 0.1554 - acc: 0.9624 - val_loss: 0.3033 - val_acc: 0.8918\n",
      "Epoch 729/1000\n",
      " - 0s - loss: 0.1552 - acc: 0.9598 - val_loss: 0.3042 - val_acc: 0.8887\n",
      "Epoch 730/1000\n",
      " - 0s - loss: 0.1547 - acc: 0.9608 - val_loss: 0.3078 - val_acc: 0.8856\n",
      "Epoch 731/1000\n",
      " - 0s - loss: 0.1544 - acc: 0.9603 - val_loss: 0.3070 - val_acc: 0.8856\n",
      "Epoch 732/1000\n",
      " - 0s - loss: 0.1544 - acc: 0.9624 - val_loss: 0.3080 - val_acc: 0.8824\n",
      "Epoch 733/1000\n",
      " - 0s - loss: 0.1540 - acc: 0.9639 - val_loss: 0.3034 - val_acc: 0.8911\n",
      "Epoch 734/1000\n",
      " - 0s - loss: 0.1540 - acc: 0.9619 - val_loss: 0.3043 - val_acc: 0.8871\n",
      "Epoch 735/1000\n",
      " - 0s - loss: 0.1539 - acc: 0.9629 - val_loss: 0.3049 - val_acc: 0.8887\n",
      "Epoch 736/1000\n",
      " - 0s - loss: 0.1537 - acc: 0.9598 - val_loss: 0.3103 - val_acc: 0.8809\n",
      "Epoch 737/1000\n",
      " - 0s - loss: 0.1536 - acc: 0.9598 - val_loss: 0.3073 - val_acc: 0.8895\n",
      "Epoch 738/1000\n",
      " - 0s - loss: 0.1532 - acc: 0.9582 - val_loss: 0.3042 - val_acc: 0.8895\n",
      "Epoch 739/1000\n",
      " - 0s - loss: 0.1531 - acc: 0.9608 - val_loss: 0.3058 - val_acc: 0.8879\n",
      "Epoch 740/1000\n",
      " - 0s - loss: 0.1524 - acc: 0.9634 - val_loss: 0.3088 - val_acc: 0.8832\n",
      "Epoch 741/1000\n",
      " - 0s - loss: 0.1524 - acc: 0.9613 - val_loss: 0.3041 - val_acc: 0.8895\n",
      "Epoch 742/1000\n",
      " - 0s - loss: 0.1521 - acc: 0.9608 - val_loss: 0.3051 - val_acc: 0.8848\n",
      "Epoch 743/1000\n",
      " - 0s - loss: 0.1523 - acc: 0.9613 - val_loss: 0.3046 - val_acc: 0.8934\n",
      "Epoch 744/1000\n",
      " - 0s - loss: 0.1528 - acc: 0.9598 - val_loss: 0.3053 - val_acc: 0.8840\n",
      "Epoch 745/1000\n",
      " - 0s - loss: 0.1515 - acc: 0.9619 - val_loss: 0.3182 - val_acc: 0.8754\n",
      "Epoch 746/1000\n",
      " - 0s - loss: 0.1520 - acc: 0.9639 - val_loss: 0.3035 - val_acc: 0.8918\n",
      "Epoch 747/1000\n",
      " - 0s - loss: 0.1508 - acc: 0.9639 - val_loss: 0.3108 - val_acc: 0.8809\n",
      "Epoch 748/1000\n",
      " - 0s - loss: 0.1513 - acc: 0.9639 - val_loss: 0.3073 - val_acc: 0.8840\n",
      "Epoch 749/1000\n",
      " - 0s - loss: 0.1506 - acc: 0.9639 - val_loss: 0.3093 - val_acc: 0.8832\n",
      "Epoch 750/1000\n",
      " - 0s - loss: 0.1508 - acc: 0.9687 - val_loss: 0.3058 - val_acc: 0.8864\n",
      "Epoch 751/1000\n",
      " - 0s - loss: 0.1505 - acc: 0.9650 - val_loss: 0.3099 - val_acc: 0.8824\n",
      "Epoch 752/1000\n",
      " - 0s - loss: 0.1505 - acc: 0.9639 - val_loss: 0.3051 - val_acc: 0.8887\n",
      "Epoch 753/1000\n",
      " - 0s - loss: 0.1494 - acc: 0.9634 - val_loss: 0.3047 - val_acc: 0.8864\n",
      "Epoch 754/1000\n",
      " - 0s - loss: 0.1496 - acc: 0.9645 - val_loss: 0.3074 - val_acc: 0.8856\n",
      "Epoch 755/1000\n",
      " - 0s - loss: 0.1496 - acc: 0.9639 - val_loss: 0.3090 - val_acc: 0.8832\n",
      "Epoch 756/1000\n",
      " - 0s - loss: 0.1495 - acc: 0.9655 - val_loss: 0.3059 - val_acc: 0.8856\n",
      "Epoch 757/1000\n",
      " - 0s - loss: 0.1486 - acc: 0.9681 - val_loss: 0.3072 - val_acc: 0.8879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 758/1000\n",
      " - 0s - loss: 0.1487 - acc: 0.9634 - val_loss: 0.3095 - val_acc: 0.8824\n",
      "Epoch 759/1000\n",
      " - 0s - loss: 0.1486 - acc: 0.9650 - val_loss: 0.3058 - val_acc: 0.8887\n",
      "Epoch 760/1000\n",
      " - 0s - loss: 0.1486 - acc: 0.9629 - val_loss: 0.3039 - val_acc: 0.8864\n",
      "Epoch 761/1000\n",
      " - 0s - loss: 0.1483 - acc: 0.9676 - val_loss: 0.3029 - val_acc: 0.8911\n",
      "Epoch 762/1000\n",
      " - 0s - loss: 0.1480 - acc: 0.9650 - val_loss: 0.3077 - val_acc: 0.8832\n",
      "Epoch 763/1000\n",
      " - 0s - loss: 0.1478 - acc: 0.9660 - val_loss: 0.3183 - val_acc: 0.8730\n",
      "Epoch 764/1000\n",
      " - 0s - loss: 0.1485 - acc: 0.9671 - val_loss: 0.3028 - val_acc: 0.8887\n",
      "Epoch 765/1000\n",
      " - 0s - loss: 0.1475 - acc: 0.9681 - val_loss: 0.3037 - val_acc: 0.8879\n",
      "Epoch 766/1000\n",
      " - 0s - loss: 0.1472 - acc: 0.9655 - val_loss: 0.3036 - val_acc: 0.8871\n",
      "Epoch 767/1000\n",
      " - 0s - loss: 0.1467 - acc: 0.9681 - val_loss: 0.3121 - val_acc: 0.8785\n",
      "Epoch 768/1000\n",
      " - 0s - loss: 0.1473 - acc: 0.9666 - val_loss: 0.3034 - val_acc: 0.8871\n",
      "Epoch 769/1000\n",
      " - 0s - loss: 0.1465 - acc: 0.9666 - val_loss: 0.3059 - val_acc: 0.8864\n",
      "Epoch 770/1000\n",
      " - 0s - loss: 0.1462 - acc: 0.9692 - val_loss: 0.3067 - val_acc: 0.8848\n",
      "Epoch 771/1000\n",
      " - 0s - loss: 0.1462 - acc: 0.9671 - val_loss: 0.3066 - val_acc: 0.8848\n",
      "Epoch 772/1000\n",
      " - 0s - loss: 0.1456 - acc: 0.9707 - val_loss: 0.3102 - val_acc: 0.8817\n",
      "Epoch 773/1000\n",
      " - 0s - loss: 0.1459 - acc: 0.9666 - val_loss: 0.3053 - val_acc: 0.8871\n",
      "Epoch 774/1000\n",
      " - 0s - loss: 0.1453 - acc: 0.9687 - val_loss: 0.3165 - val_acc: 0.8871\n",
      "Epoch 775/1000\n",
      " - 0s - loss: 0.1464 - acc: 0.9666 - val_loss: 0.3047 - val_acc: 0.8856\n",
      "Epoch 776/1000\n",
      " - 0s - loss: 0.1447 - acc: 0.9681 - val_loss: 0.3030 - val_acc: 0.8887\n",
      "Epoch 777/1000\n",
      " - 0s - loss: 0.1447 - acc: 0.9671 - val_loss: 0.3083 - val_acc: 0.8809\n",
      "Epoch 778/1000\n",
      " - 0s - loss: 0.1446 - acc: 0.9660 - val_loss: 0.3056 - val_acc: 0.8871\n",
      "Epoch 779/1000\n",
      " - 0s - loss: 0.1445 - acc: 0.9676 - val_loss: 0.3024 - val_acc: 0.8911\n",
      "Epoch 780/1000\n",
      " - 0s - loss: 0.1443 - acc: 0.9676 - val_loss: 0.3084 - val_acc: 0.8824\n",
      "Epoch 781/1000\n",
      " - 0s - loss: 0.1447 - acc: 0.9666 - val_loss: 0.3032 - val_acc: 0.8879\n",
      "Epoch 782/1000\n",
      " - 0s - loss: 0.1438 - acc: 0.9707 - val_loss: 0.3043 - val_acc: 0.8918\n",
      "Epoch 783/1000\n",
      " - 0s - loss: 0.1438 - acc: 0.9681 - val_loss: 0.3028 - val_acc: 0.8864\n",
      "Epoch 784/1000\n",
      " - 0s - loss: 0.1435 - acc: 0.9697 - val_loss: 0.3043 - val_acc: 0.8871\n",
      "Epoch 785/1000\n",
      " - 0s - loss: 0.1433 - acc: 0.9692 - val_loss: 0.3027 - val_acc: 0.8871\n",
      "Epoch 786/1000\n",
      " - 0s - loss: 0.1429 - acc: 0.9687 - val_loss: 0.3025 - val_acc: 0.8879\n",
      "Epoch 787/1000\n",
      " - 0s - loss: 0.1426 - acc: 0.9687 - val_loss: 0.3070 - val_acc: 0.8832\n",
      "Epoch 788/1000\n",
      " - 0s - loss: 0.1428 - acc: 0.9702 - val_loss: 0.3106 - val_acc: 0.8801\n",
      "Epoch 789/1000\n",
      " - 0s - loss: 0.1427 - acc: 0.9707 - val_loss: 0.3040 - val_acc: 0.8879\n",
      "Epoch 790/1000\n",
      " - 0s - loss: 0.1422 - acc: 0.9707 - val_loss: 0.3136 - val_acc: 0.8817\n",
      "Epoch 791/1000\n",
      " - 0s - loss: 0.1429 - acc: 0.9666 - val_loss: 0.3041 - val_acc: 0.8871\n",
      "Epoch 792/1000\n",
      " - 0s - loss: 0.1420 - acc: 0.9692 - val_loss: 0.3034 - val_acc: 0.8879\n",
      "Epoch 793/1000\n",
      " - 0s - loss: 0.1412 - acc: 0.9728 - val_loss: 0.3023 - val_acc: 0.8903\n",
      "Epoch 794/1000\n",
      " - 0s - loss: 0.1414 - acc: 0.9702 - val_loss: 0.3044 - val_acc: 0.8887\n",
      "Epoch 795/1000\n",
      " - 0s - loss: 0.1413 - acc: 0.9697 - val_loss: 0.3110 - val_acc: 0.8832\n",
      "Epoch 796/1000\n",
      " - 0s - loss: 0.1412 - acc: 0.9707 - val_loss: 0.3043 - val_acc: 0.8879\n",
      "Epoch 797/1000\n",
      " - 0s - loss: 0.1409 - acc: 0.9697 - val_loss: 0.3043 - val_acc: 0.8879\n",
      "Epoch 798/1000\n",
      " - 0s - loss: 0.1399 - acc: 0.9687 - val_loss: 0.3066 - val_acc: 0.8824\n",
      "Epoch 799/1000\n",
      " - 0s - loss: 0.1408 - acc: 0.9692 - val_loss: 0.3079 - val_acc: 0.8864\n",
      "Epoch 800/1000\n",
      " - 0s - loss: 0.1402 - acc: 0.9707 - val_loss: 0.3070 - val_acc: 0.8840\n",
      "Epoch 801/1000\n",
      " - 0s - loss: 0.1399 - acc: 0.9734 - val_loss: 0.3018 - val_acc: 0.8918\n",
      "Epoch 802/1000\n",
      " - 0s - loss: 0.1403 - acc: 0.9702 - val_loss: 0.3061 - val_acc: 0.8817\n",
      "Epoch 803/1000\n",
      " - 0s - loss: 0.1398 - acc: 0.9702 - val_loss: 0.3049 - val_acc: 0.8848\n",
      "Epoch 804/1000\n",
      " - 0s - loss: 0.1393 - acc: 0.9734 - val_loss: 0.3067 - val_acc: 0.8840\n",
      "Epoch 805/1000\n",
      " - 0s - loss: 0.1393 - acc: 0.9702 - val_loss: 0.3061 - val_acc: 0.8840\n",
      "Epoch 806/1000\n",
      " - 0s - loss: 0.1389 - acc: 0.9702 - val_loss: 0.3030 - val_acc: 0.8911\n",
      "Epoch 807/1000\n",
      " - 0s - loss: 0.1394 - acc: 0.9707 - val_loss: 0.3024 - val_acc: 0.8918\n",
      "Epoch 808/1000\n",
      " - 0s - loss: 0.1385 - acc: 0.9702 - val_loss: 0.3052 - val_acc: 0.8848\n",
      "Epoch 809/1000\n",
      " - 0s - loss: 0.1388 - acc: 0.9697 - val_loss: 0.3070 - val_acc: 0.8817\n",
      "Epoch 810/1000\n",
      " - 0s - loss: 0.1381 - acc: 0.9707 - val_loss: 0.3049 - val_acc: 0.8864\n",
      "Epoch 811/1000\n",
      " - 0s - loss: 0.1379 - acc: 0.9744 - val_loss: 0.3037 - val_acc: 0.8871\n",
      "Epoch 812/1000\n",
      " - 0s - loss: 0.1381 - acc: 0.9718 - val_loss: 0.3059 - val_acc: 0.8856\n",
      "Epoch 813/1000\n",
      " - 0s - loss: 0.1380 - acc: 0.9723 - val_loss: 0.3019 - val_acc: 0.8871\n",
      "Epoch 814/1000\n",
      " - 0s - loss: 0.1373 - acc: 0.9734 - val_loss: 0.3020 - val_acc: 0.8879\n",
      "Epoch 815/1000\n",
      " - 0s - loss: 0.1371 - acc: 0.9697 - val_loss: 0.3089 - val_acc: 0.8832\n",
      "Epoch 816/1000\n",
      " - 0s - loss: 0.1373 - acc: 0.9702 - val_loss: 0.3031 - val_acc: 0.8864\n",
      "Epoch 817/1000\n",
      " - 0s - loss: 0.1373 - acc: 0.9718 - val_loss: 0.3018 - val_acc: 0.8918\n",
      "Epoch 818/1000\n",
      " - 0s - loss: 0.1362 - acc: 0.9707 - val_loss: 0.3053 - val_acc: 0.8856\n",
      "Epoch 819/1000\n",
      " - 0s - loss: 0.1360 - acc: 0.9723 - val_loss: 0.3018 - val_acc: 0.8895\n",
      "Epoch 820/1000\n",
      " - 0s - loss: 0.1366 - acc: 0.9718 - val_loss: 0.3022 - val_acc: 0.8879\n",
      "Epoch 821/1000\n",
      " - 0s - loss: 0.1363 - acc: 0.9713 - val_loss: 0.3063 - val_acc: 0.8817\n",
      "Epoch 822/1000\n",
      " - 0s - loss: 0.1357 - acc: 0.9718 - val_loss: 0.3056 - val_acc: 0.8840\n",
      "Epoch 823/1000\n",
      " - 0s - loss: 0.1357 - acc: 0.9749 - val_loss: 0.3028 - val_acc: 0.8911\n",
      "Epoch 824/1000\n",
      " - 0s - loss: 0.1358 - acc: 0.9713 - val_loss: 0.3022 - val_acc: 0.8887\n",
      "Epoch 825/1000\n",
      " - 0s - loss: 0.1352 - acc: 0.9723 - val_loss: 0.3066 - val_acc: 0.8817\n",
      "Epoch 826/1000\n",
      " - 0s - loss: 0.1350 - acc: 0.9749 - val_loss: 0.3021 - val_acc: 0.8871\n",
      "Epoch 827/1000\n",
      " - 0s - loss: 0.1344 - acc: 0.9713 - val_loss: 0.3031 - val_acc: 0.8864\n",
      "Epoch 828/1000\n",
      " - 0s - loss: 0.1345 - acc: 0.9734 - val_loss: 0.3039 - val_acc: 0.8864\n",
      "Epoch 829/1000\n",
      " - 0s - loss: 0.1346 - acc: 0.9723 - val_loss: 0.3050 - val_acc: 0.8848\n",
      "Epoch 830/1000\n",
      " - 0s - loss: 0.1343 - acc: 0.9723 - val_loss: 0.3022 - val_acc: 0.8879\n",
      "Epoch 831/1000\n",
      " - 0s - loss: 0.1340 - acc: 0.9728 - val_loss: 0.3025 - val_acc: 0.8887\n",
      "Epoch 832/1000\n",
      " - 0s - loss: 0.1338 - acc: 0.9749 - val_loss: 0.3054 - val_acc: 0.8856\n",
      "Epoch 833/1000\n",
      " - 0s - loss: 0.1336 - acc: 0.9734 - val_loss: 0.3044 - val_acc: 0.8864\n",
      "Epoch 834/1000\n",
      " - 0s - loss: 0.1335 - acc: 0.9754 - val_loss: 0.3026 - val_acc: 0.8895\n",
      "Epoch 835/1000\n",
      " - 0s - loss: 0.1335 - acc: 0.9734 - val_loss: 0.3025 - val_acc: 0.8879\n",
      "Epoch 836/1000\n",
      " - 0s - loss: 0.1329 - acc: 0.9744 - val_loss: 0.3044 - val_acc: 0.8864\n",
      "Epoch 837/1000\n",
      " - 0s - loss: 0.1335 - acc: 0.9734 - val_loss: 0.3018 - val_acc: 0.8887\n",
      "Epoch 838/1000\n",
      " - 0s - loss: 0.1328 - acc: 0.9734 - val_loss: 0.3055 - val_acc: 0.8848\n",
      "Epoch 839/1000\n",
      " - 0s - loss: 0.1327 - acc: 0.9760 - val_loss: 0.3054 - val_acc: 0.8879\n",
      "Epoch 840/1000\n",
      " - 0s - loss: 0.1328 - acc: 0.9723 - val_loss: 0.3030 - val_acc: 0.8895\n",
      "Epoch 841/1000\n",
      " - 0s - loss: 0.1323 - acc: 0.9734 - val_loss: 0.3027 - val_acc: 0.8871\n",
      "Epoch 842/1000\n",
      " - 0s - loss: 0.1320 - acc: 0.9739 - val_loss: 0.3016 - val_acc: 0.8879\n",
      "Epoch 843/1000\n",
      " - 0s - loss: 0.1325 - acc: 0.9728 - val_loss: 0.3042 - val_acc: 0.8864\n",
      "Epoch 844/1000\n",
      " - 0s - loss: 0.1317 - acc: 0.9760 - val_loss: 0.3042 - val_acc: 0.8903\n",
      "Epoch 845/1000\n",
      " - 0s - loss: 0.1320 - acc: 0.9728 - val_loss: 0.3048 - val_acc: 0.8848\n",
      "Epoch 846/1000\n",
      " - 0s - loss: 0.1311 - acc: 0.9760 - val_loss: 0.3012 - val_acc: 0.8879\n",
      "Epoch 847/1000\n",
      " - 0s - loss: 0.1311 - acc: 0.9734 - val_loss: 0.3015 - val_acc: 0.8879\n",
      "Epoch 848/1000\n",
      " - 0s - loss: 0.1312 - acc: 0.9749 - val_loss: 0.3074 - val_acc: 0.8832\n",
      "Epoch 849/1000\n",
      " - 0s - loss: 0.1311 - acc: 0.9760 - val_loss: 0.3031 - val_acc: 0.8871\n",
      "Epoch 850/1000\n",
      " - 0s - loss: 0.1308 - acc: 0.9749 - val_loss: 0.3058 - val_acc: 0.8871\n",
      "Epoch 851/1000\n",
      " - 0s - loss: 0.1306 - acc: 0.9744 - val_loss: 0.3053 - val_acc: 0.8871\n",
      "Epoch 852/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1300 - acc: 0.9760 - val_loss: 0.3021 - val_acc: 0.8879\n",
      "Epoch 853/1000\n",
      " - 0s - loss: 0.1299 - acc: 0.9760 - val_loss: 0.3031 - val_acc: 0.8879\n",
      "Epoch 854/1000\n",
      " - 0s - loss: 0.1300 - acc: 0.9744 - val_loss: 0.3199 - val_acc: 0.8762\n",
      "Epoch 855/1000\n",
      " - 0s - loss: 0.1303 - acc: 0.9754 - val_loss: 0.3013 - val_acc: 0.8887\n",
      "Epoch 856/1000\n",
      " - 0s - loss: 0.1298 - acc: 0.9749 - val_loss: 0.3024 - val_acc: 0.8903\n",
      "Epoch 857/1000\n",
      " - 0s - loss: 0.1294 - acc: 0.9760 - val_loss: 0.3029 - val_acc: 0.8903\n",
      "Epoch 858/1000\n",
      " - 0s - loss: 0.1299 - acc: 0.9760 - val_loss: 0.3014 - val_acc: 0.8918\n",
      "Epoch 859/1000\n",
      " - 0s - loss: 0.1293 - acc: 0.9749 - val_loss: 0.3029 - val_acc: 0.8871\n",
      "Epoch 860/1000\n",
      " - 0s - loss: 0.1290 - acc: 0.9765 - val_loss: 0.3099 - val_acc: 0.8887\n",
      "Epoch 861/1000\n",
      " - 0s - loss: 0.1295 - acc: 0.9760 - val_loss: 0.3017 - val_acc: 0.8871\n",
      "Epoch 862/1000\n",
      " - 0s - loss: 0.1285 - acc: 0.9739 - val_loss: 0.3112 - val_acc: 0.8824\n",
      "Epoch 863/1000\n",
      " - 0s - loss: 0.1285 - acc: 0.9770 - val_loss: 0.3017 - val_acc: 0.8903\n",
      "Epoch 864/1000\n",
      " - 0s - loss: 0.1278 - acc: 0.9775 - val_loss: 0.3048 - val_acc: 0.8864\n",
      "Epoch 865/1000\n",
      " - 0s - loss: 0.1275 - acc: 0.9770 - val_loss: 0.3029 - val_acc: 0.8887\n",
      "Epoch 866/1000\n",
      " - 0s - loss: 0.1274 - acc: 0.9754 - val_loss: 0.3011 - val_acc: 0.8895\n",
      "Epoch 867/1000\n",
      " - 0s - loss: 0.1279 - acc: 0.9765 - val_loss: 0.3035 - val_acc: 0.8871\n",
      "Epoch 868/1000\n",
      " - 0s - loss: 0.1270 - acc: 0.9781 - val_loss: 0.3027 - val_acc: 0.8903\n",
      "Epoch 869/1000\n",
      " - 0s - loss: 0.1271 - acc: 0.9754 - val_loss: 0.3103 - val_acc: 0.8793\n",
      "Epoch 870/1000\n",
      " - 0s - loss: 0.1274 - acc: 0.9781 - val_loss: 0.3059 - val_acc: 0.8809\n",
      "Epoch 871/1000\n",
      " - 0s - loss: 0.1270 - acc: 0.9775 - val_loss: 0.3019 - val_acc: 0.8895\n",
      "Epoch 872/1000\n",
      " - 0s - loss: 0.1264 - acc: 0.9770 - val_loss: 0.3033 - val_acc: 0.8856\n",
      "Epoch 873/1000\n",
      " - 0s - loss: 0.1263 - acc: 0.9765 - val_loss: 0.3068 - val_acc: 0.8809\n",
      "Epoch 874/1000\n",
      " - 0s - loss: 0.1265 - acc: 0.9781 - val_loss: 0.3042 - val_acc: 0.8871\n",
      "Epoch 875/1000\n",
      " - 0s - loss: 0.1255 - acc: 0.9781 - val_loss: 0.3203 - val_acc: 0.8754\n",
      "Epoch 876/1000\n",
      " - 0s - loss: 0.1270 - acc: 0.9807 - val_loss: 0.3079 - val_acc: 0.8824\n",
      "Epoch 877/1000\n",
      " - 0s - loss: 0.1263 - acc: 0.9739 - val_loss: 0.3030 - val_acc: 0.8864\n",
      "Epoch 878/1000\n",
      " - 0s - loss: 0.1249 - acc: 0.9775 - val_loss: 0.3034 - val_acc: 0.8848\n",
      "Epoch 879/1000\n",
      " - 0s - loss: 0.1250 - acc: 0.9775 - val_loss: 0.3032 - val_acc: 0.8856\n",
      "Epoch 880/1000\n",
      " - 0s - loss: 0.1248 - acc: 0.9781 - val_loss: 0.3037 - val_acc: 0.8856\n",
      "Epoch 881/1000\n",
      " - 0s - loss: 0.1252 - acc: 0.9775 - val_loss: 0.3086 - val_acc: 0.8793\n",
      "Epoch 882/1000\n",
      " - 0s - loss: 0.1254 - acc: 0.9781 - val_loss: 0.3040 - val_acc: 0.8817\n",
      "Epoch 883/1000\n",
      " - 0s - loss: 0.1248 - acc: 0.9791 - val_loss: 0.3010 - val_acc: 0.8887\n",
      "Epoch 884/1000\n",
      " - 0s - loss: 0.1244 - acc: 0.9765 - val_loss: 0.3015 - val_acc: 0.8903\n",
      "Epoch 885/1000\n",
      " - 0s - loss: 0.1240 - acc: 0.9765 - val_loss: 0.3009 - val_acc: 0.8879\n",
      "Epoch 886/1000\n",
      " - 0s - loss: 0.1240 - acc: 0.9770 - val_loss: 0.3022 - val_acc: 0.8879\n",
      "Epoch 887/1000\n",
      " - 0s - loss: 0.1240 - acc: 0.9765 - val_loss: 0.3016 - val_acc: 0.8911\n",
      "Epoch 888/1000\n",
      " - 0s - loss: 0.1237 - acc: 0.9765 - val_loss: 0.3088 - val_acc: 0.8809\n",
      "Epoch 889/1000\n",
      " - 0s - loss: 0.1237 - acc: 0.9786 - val_loss: 0.3020 - val_acc: 0.8887\n",
      "Epoch 890/1000\n",
      " - 0s - loss: 0.1231 - acc: 0.9786 - val_loss: 0.3048 - val_acc: 0.8887\n",
      "Epoch 891/1000\n",
      " - 0s - loss: 0.1243 - acc: 0.9760 - val_loss: 0.3045 - val_acc: 0.8856\n",
      "Epoch 892/1000\n",
      " - 0s - loss: 0.1228 - acc: 0.9807 - val_loss: 0.3027 - val_acc: 0.8887\n",
      "Epoch 893/1000\n",
      " - 0s - loss: 0.1227 - acc: 0.9786 - val_loss: 0.3068 - val_acc: 0.8817\n",
      "Epoch 894/1000\n",
      " - 0s - loss: 0.1227 - acc: 0.9786 - val_loss: 0.3086 - val_acc: 0.8793\n",
      "Epoch 895/1000\n",
      " - 0s - loss: 0.1228 - acc: 0.9770 - val_loss: 0.3039 - val_acc: 0.8840\n",
      "Epoch 896/1000\n",
      " - 0s - loss: 0.1223 - acc: 0.9775 - val_loss: 0.3018 - val_acc: 0.8895\n",
      "Epoch 897/1000\n",
      " - 0s - loss: 0.1219 - acc: 0.9781 - val_loss: 0.3036 - val_acc: 0.8871\n",
      "Epoch 898/1000\n",
      " - 0s - loss: 0.1220 - acc: 0.9775 - val_loss: 0.3034 - val_acc: 0.8856\n",
      "Epoch 899/1000\n",
      " - 0s - loss: 0.1218 - acc: 0.9791 - val_loss: 0.3018 - val_acc: 0.8895\n",
      "Epoch 900/1000\n",
      " - 0s - loss: 0.1220 - acc: 0.9775 - val_loss: 0.3014 - val_acc: 0.8879\n",
      "Epoch 901/1000\n",
      " - 0s - loss: 0.1217 - acc: 0.9781 - val_loss: 0.3017 - val_acc: 0.8895\n",
      "Epoch 902/1000\n",
      " - 0s - loss: 0.1211 - acc: 0.9770 - val_loss: 0.3011 - val_acc: 0.8887\n",
      "Epoch 903/1000\n",
      " - 0s - loss: 0.1212 - acc: 0.9786 - val_loss: 0.3025 - val_acc: 0.8911\n",
      "Epoch 904/1000\n",
      " - 0s - loss: 0.1209 - acc: 0.9807 - val_loss: 0.3028 - val_acc: 0.8895\n",
      "Epoch 905/1000\n",
      " - 0s - loss: 0.1208 - acc: 0.9786 - val_loss: 0.3017 - val_acc: 0.8903\n",
      "Epoch 906/1000\n",
      " - 0s - loss: 0.1205 - acc: 0.9791 - val_loss: 0.3021 - val_acc: 0.8864\n",
      "Epoch 907/1000\n",
      " - 0s - loss: 0.1206 - acc: 0.9791 - val_loss: 0.3046 - val_acc: 0.8856\n",
      "Epoch 908/1000\n",
      " - 0s - loss: 0.1204 - acc: 0.9781 - val_loss: 0.3031 - val_acc: 0.8879\n",
      "Epoch 909/1000\n",
      " - 0s - loss: 0.1201 - acc: 0.9791 - val_loss: 0.3029 - val_acc: 0.8856\n",
      "Epoch 910/1000\n",
      " - 0s - loss: 0.1203 - acc: 0.9791 - val_loss: 0.3037 - val_acc: 0.8864\n",
      "Epoch 911/1000\n",
      " - 0s - loss: 0.1195 - acc: 0.9781 - val_loss: 0.3019 - val_acc: 0.8887\n",
      "Epoch 912/1000\n",
      " - 0s - loss: 0.1194 - acc: 0.9781 - val_loss: 0.3013 - val_acc: 0.8903\n",
      "Epoch 913/1000\n",
      " - 0s - loss: 0.1194 - acc: 0.9791 - val_loss: 0.3014 - val_acc: 0.8887\n",
      "Epoch 914/1000\n",
      " - 0s - loss: 0.1189 - acc: 0.9781 - val_loss: 0.3022 - val_acc: 0.8895\n",
      "Epoch 915/1000\n",
      " - 0s - loss: 0.1191 - acc: 0.9796 - val_loss: 0.3022 - val_acc: 0.8879\n",
      "Epoch 916/1000\n",
      " - 0s - loss: 0.1187 - acc: 0.9791 - val_loss: 0.3012 - val_acc: 0.8871\n",
      "Epoch 917/1000\n",
      " - 0s - loss: 0.1185 - acc: 0.9796 - val_loss: 0.3033 - val_acc: 0.8887\n",
      "Epoch 918/1000\n",
      " - 0s - loss: 0.1184 - acc: 0.9796 - val_loss: 0.3005 - val_acc: 0.8879\n",
      "Epoch 919/1000\n",
      " - 0s - loss: 0.1186 - acc: 0.9796 - val_loss: 0.3012 - val_acc: 0.8911\n",
      "Epoch 920/1000\n",
      " - 0s - loss: 0.1180 - acc: 0.9786 - val_loss: 0.3069 - val_acc: 0.8770\n",
      "Epoch 921/1000\n",
      " - 0s - loss: 0.1183 - acc: 0.9817 - val_loss: 0.3043 - val_acc: 0.8879\n",
      "Epoch 922/1000\n",
      " - 0s - loss: 0.1180 - acc: 0.9796 - val_loss: 0.3019 - val_acc: 0.8895\n",
      "Epoch 923/1000\n",
      " - 0s - loss: 0.1178 - acc: 0.9796 - val_loss: 0.3049 - val_acc: 0.8824\n",
      "Epoch 924/1000\n",
      " - 0s - loss: 0.1177 - acc: 0.9796 - val_loss: 0.3012 - val_acc: 0.8903\n",
      "Epoch 925/1000\n",
      " - 0s - loss: 0.1173 - acc: 0.9807 - val_loss: 0.3019 - val_acc: 0.8895\n",
      "Epoch 926/1000\n",
      " - 0s - loss: 0.1170 - acc: 0.9807 - val_loss: 0.3017 - val_acc: 0.8911\n",
      "Epoch 927/1000\n",
      " - 0s - loss: 0.1171 - acc: 0.9801 - val_loss: 0.3040 - val_acc: 0.8879\n",
      "Epoch 928/1000\n",
      " - 0s - loss: 0.1168 - acc: 0.9801 - val_loss: 0.3017 - val_acc: 0.8871\n",
      "Epoch 929/1000\n",
      " - 0s - loss: 0.1168 - acc: 0.9801 - val_loss: 0.3006 - val_acc: 0.8879\n",
      "Epoch 930/1000\n",
      " - 0s - loss: 0.1164 - acc: 0.9791 - val_loss: 0.3055 - val_acc: 0.8879\n",
      "Epoch 931/1000\n",
      " - 0s - loss: 0.1166 - acc: 0.9822 - val_loss: 0.3008 - val_acc: 0.8879\n",
      "Epoch 932/1000\n",
      " - 0s - loss: 0.1156 - acc: 0.9796 - val_loss: 0.3020 - val_acc: 0.8879\n",
      "Epoch 933/1000\n",
      " - 0s - loss: 0.1160 - acc: 0.9812 - val_loss: 0.3048 - val_acc: 0.8801\n",
      "Epoch 934/1000\n",
      " - 0s - loss: 0.1158 - acc: 0.9791 - val_loss: 0.3012 - val_acc: 0.8871\n",
      "Epoch 935/1000\n",
      " - 0s - loss: 0.1156 - acc: 0.9807 - val_loss: 0.3016 - val_acc: 0.8903\n",
      "Epoch 936/1000\n",
      " - 0s - loss: 0.1153 - acc: 0.9817 - val_loss: 0.3021 - val_acc: 0.8887\n",
      "Epoch 937/1000\n",
      " - 0s - loss: 0.1151 - acc: 0.9812 - val_loss: 0.3010 - val_acc: 0.8918\n",
      "Epoch 938/1000\n",
      " - 0s - loss: 0.1152 - acc: 0.9791 - val_loss: 0.3009 - val_acc: 0.8895\n",
      "Epoch 939/1000\n",
      " - 0s - loss: 0.1151 - acc: 0.9796 - val_loss: 0.3012 - val_acc: 0.8895\n",
      "Epoch 940/1000\n",
      " - 0s - loss: 0.1146 - acc: 0.9796 - val_loss: 0.3015 - val_acc: 0.8903\n",
      "Epoch 941/1000\n",
      " - 0s - loss: 0.1149 - acc: 0.9801 - val_loss: 0.3010 - val_acc: 0.8864\n",
      "Epoch 942/1000\n",
      " - 0s - loss: 0.1144 - acc: 0.9796 - val_loss: 0.3014 - val_acc: 0.8856\n",
      "Epoch 943/1000\n",
      " - 0s - loss: 0.1146 - acc: 0.9796 - val_loss: 0.3023 - val_acc: 0.8887\n",
      "Epoch 944/1000\n",
      " - 0s - loss: 0.1136 - acc: 0.9822 - val_loss: 0.3089 - val_acc: 0.8770\n",
      "Epoch 945/1000\n",
      " - 0s - loss: 0.1143 - acc: 0.9828 - val_loss: 0.3040 - val_acc: 0.8887\n",
      "Epoch 946/1000\n",
      " - 0s - loss: 0.1140 - acc: 0.9822 - val_loss: 0.3017 - val_acc: 0.8903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      " - 0s - loss: 0.1138 - acc: 0.9807 - val_loss: 0.3032 - val_acc: 0.8903\n",
      "Epoch 948/1000\n",
      " - 0s - loss: 0.1137 - acc: 0.9786 - val_loss: 0.3055 - val_acc: 0.8824\n",
      "Epoch 949/1000\n",
      " - 0s - loss: 0.1137 - acc: 0.9807 - val_loss: 0.3051 - val_acc: 0.8848\n",
      "Epoch 950/1000\n",
      " - 0s - loss: 0.1134 - acc: 0.9817 - val_loss: 0.3034 - val_acc: 0.8871\n",
      "Epoch 951/1000\n",
      " - 0s - loss: 0.1130 - acc: 0.9812 - val_loss: 0.3044 - val_acc: 0.8824\n",
      "Epoch 952/1000\n",
      " - 0s - loss: 0.1128 - acc: 0.9822 - val_loss: 0.3053 - val_acc: 0.8809\n",
      "Epoch 953/1000\n",
      " - 0s - loss: 0.1131 - acc: 0.9812 - val_loss: 0.3043 - val_acc: 0.8864\n",
      "Epoch 954/1000\n",
      " - 0s - loss: 0.1125 - acc: 0.9796 - val_loss: 0.3019 - val_acc: 0.8840\n",
      "Epoch 955/1000\n",
      " - 0s - loss: 0.1123 - acc: 0.9822 - val_loss: 0.3016 - val_acc: 0.8864\n",
      "Epoch 956/1000\n",
      " - 0s - loss: 0.1121 - acc: 0.9807 - val_loss: 0.3009 - val_acc: 0.8864\n",
      "Epoch 957/1000\n",
      " - 0s - loss: 0.1123 - acc: 0.9786 - val_loss: 0.3024 - val_acc: 0.8856\n",
      "Epoch 958/1000\n",
      " - 0s - loss: 0.1120 - acc: 0.9807 - val_loss: 0.3020 - val_acc: 0.8918\n",
      "Epoch 959/1000\n",
      " - 0s - loss: 0.1118 - acc: 0.9817 - val_loss: 0.3015 - val_acc: 0.8911\n",
      "Epoch 960/1000\n",
      " - 0s - loss: 0.1120 - acc: 0.9796 - val_loss: 0.3044 - val_acc: 0.8848\n",
      "Epoch 961/1000\n",
      " - 0s - loss: 0.1113 - acc: 0.9812 - val_loss: 0.3048 - val_acc: 0.8856\n",
      "Epoch 962/1000\n",
      " - 0s - loss: 0.1117 - acc: 0.9812 - val_loss: 0.3041 - val_acc: 0.8864\n",
      "Epoch 963/1000\n",
      " - 0s - loss: 0.1111 - acc: 0.9838 - val_loss: 0.3029 - val_acc: 0.8903\n",
      "Epoch 964/1000\n",
      " - 0s - loss: 0.1111 - acc: 0.9812 - val_loss: 0.3028 - val_acc: 0.8879\n",
      "Epoch 965/1000\n",
      " - 0s - loss: 0.1106 - acc: 0.9801 - val_loss: 0.3045 - val_acc: 0.8864\n",
      "Epoch 966/1000\n",
      " - 0s - loss: 0.1107 - acc: 0.9812 - val_loss: 0.3022 - val_acc: 0.8840\n",
      "Epoch 967/1000\n",
      " - 0s - loss: 0.1102 - acc: 0.9828 - val_loss: 0.3066 - val_acc: 0.8817\n",
      "Epoch 968/1000\n",
      " - 0s - loss: 0.1106 - acc: 0.9822 - val_loss: 0.3030 - val_acc: 0.8817\n",
      "Epoch 969/1000\n",
      " - 0s - loss: 0.1105 - acc: 0.9822 - val_loss: 0.3042 - val_acc: 0.8824\n",
      "Epoch 970/1000\n",
      " - 0s - loss: 0.1103 - acc: 0.9817 - val_loss: 0.3052 - val_acc: 0.8840\n",
      "Epoch 971/1000\n",
      " - 0s - loss: 0.1100 - acc: 0.9812 - val_loss: 0.3047 - val_acc: 0.8871\n",
      "Epoch 972/1000\n",
      " - 0s - loss: 0.1103 - acc: 0.9817 - val_loss: 0.3059 - val_acc: 0.8871\n",
      "Epoch 973/1000\n",
      " - 0s - loss: 0.1095 - acc: 0.9833 - val_loss: 0.3048 - val_acc: 0.8832\n",
      "Epoch 974/1000\n",
      " - 0s - loss: 0.1095 - acc: 0.9828 - val_loss: 0.3013 - val_acc: 0.8864\n",
      "Epoch 975/1000\n",
      " - 0s - loss: 0.1093 - acc: 0.9817 - val_loss: 0.3031 - val_acc: 0.8864\n",
      "Epoch 976/1000\n",
      " - 0s - loss: 0.1090 - acc: 0.9817 - val_loss: 0.3033 - val_acc: 0.8879\n",
      "Epoch 977/1000\n",
      " - 0s - loss: 0.1094 - acc: 0.9812 - val_loss: 0.3028 - val_acc: 0.8864\n",
      "Epoch 978/1000\n",
      " - 0s - loss: 0.1089 - acc: 0.9812 - val_loss: 0.3049 - val_acc: 0.8871\n",
      "Epoch 979/1000\n",
      " - 0s - loss: 0.1090 - acc: 0.9817 - val_loss: 0.3020 - val_acc: 0.8895\n",
      "Epoch 980/1000\n",
      " - 0s - loss: 0.1085 - acc: 0.9812 - val_loss: 0.3038 - val_acc: 0.8879\n",
      "Epoch 981/1000\n",
      " - 0s - loss: 0.1083 - acc: 0.9812 - val_loss: 0.3068 - val_acc: 0.8824\n",
      "Epoch 982/1000\n",
      " - 0s - loss: 0.1082 - acc: 0.9838 - val_loss: 0.3035 - val_acc: 0.8879\n",
      "Epoch 983/1000\n",
      " - 0s - loss: 0.1083 - acc: 0.9822 - val_loss: 0.3039 - val_acc: 0.8879\n",
      "Epoch 984/1000\n",
      " - 0s - loss: 0.1081 - acc: 0.9812 - val_loss: 0.3035 - val_acc: 0.8871\n",
      "Epoch 985/1000\n",
      " - 0s - loss: 0.1076 - acc: 0.9828 - val_loss: 0.3048 - val_acc: 0.8817\n",
      "Epoch 986/1000\n",
      " - 0s - loss: 0.1078 - acc: 0.9838 - val_loss: 0.3053 - val_acc: 0.8832\n",
      "Epoch 987/1000\n",
      " - 0s - loss: 0.1079 - acc: 0.9828 - val_loss: 0.3022 - val_acc: 0.8856\n",
      "Epoch 988/1000\n",
      " - 0s - loss: 0.1072 - acc: 0.9838 - val_loss: 0.3020 - val_acc: 0.8832\n",
      "Epoch 989/1000\n",
      " - 0s - loss: 0.1070 - acc: 0.9822 - val_loss: 0.3022 - val_acc: 0.8864\n",
      "Epoch 990/1000\n",
      " - 0s - loss: 0.1073 - acc: 0.9822 - val_loss: 0.3019 - val_acc: 0.8840\n",
      "Epoch 991/1000\n",
      " - 0s - loss: 0.1068 - acc: 0.9822 - val_loss: 0.3018 - val_acc: 0.8911\n",
      "Epoch 992/1000\n",
      " - 0s - loss: 0.1070 - acc: 0.9822 - val_loss: 0.3027 - val_acc: 0.8848\n",
      "Epoch 993/1000\n",
      " - 0s - loss: 0.1065 - acc: 0.9838 - val_loss: 0.3071 - val_acc: 0.8809\n",
      "Epoch 994/1000\n",
      " - 0s - loss: 0.1071 - acc: 0.9833 - val_loss: 0.3053 - val_acc: 0.8848\n",
      "Epoch 995/1000\n",
      " - 0s - loss: 0.1068 - acc: 0.9833 - val_loss: 0.3017 - val_acc: 0.8887\n",
      "Epoch 996/1000\n",
      " - 0s - loss: 0.1061 - acc: 0.9817 - val_loss: 0.3013 - val_acc: 0.8879\n",
      "Epoch 997/1000\n",
      " - 0s - loss: 0.1060 - acc: 0.9828 - val_loss: 0.3026 - val_acc: 0.8887\n",
      "Epoch 998/1000\n",
      " - 0s - loss: 0.1060 - acc: 0.9848 - val_loss: 0.3132 - val_acc: 0.8801\n",
      "Epoch 999/1000\n",
      " - 0s - loss: 0.1063 - acc: 0.9848 - val_loss: 0.3056 - val_acc: 0.8817\n",
      "Epoch 1000/1000\n",
      " - 0s - loss: 0.1060 - acc: 0.9828 - val_loss: 0.3061 - val_acc: 0.8848\n",
      "CPU times: user 48.8 s, sys: 9.96 s, total: 58.7 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist2 = model2.fit(X_train, y_train, epochs=1000, batch_size=100,\n",
    "                   validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.88      0.81      0.84       320\n",
      "         IE       0.79      0.94      0.86       327\n",
      "          N       0.95      0.90      0.92       629\n",
      "\n",
      "avg / total       0.89      0.88      0.89      1276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dnn_model_classification_report(model2, dnn_preprocessed, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_preprocessed = DataPreprocess(df, is_vectorize=True, vectorizer=CountVectorizer)\n",
    "tf_preprocessed  = DataPreprocess(df, is_vectorize=True, vectorizer=TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "dnn_model_cnt = DNNModel(sess, \"dnn_model_cnt\", cnt_preprocessed)\n",
    "dnn_model_tf = DNNModel(sess, \"dnn_model_tf\", tf_preprocessed)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoch = 1000\n",
    "\n",
    "for step in range(epoch):\n",
    "    c, _ = dnn_model.train()\n",
    "    if step % 100 == 0:\n",
    "        print(step, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model_classification_report(dnn_model_cnt, cnt_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoch = 1000\n",
    "\n",
    "for step in range(epoch):\n",
    "    c, _ = dnn_model_cnt.train()\n",
    "    if step % 100 == 0:\n",
    "        print(step, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model_classification_report(dnn_model_tf, tf_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoch = 1000\n",
    "\n",
    "for step in range(epoch):\n",
    "    c, _ = dnn_model_tf.train()\n",
    "    if step % 100 == 0:\n",
    "        print(step, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cnt_preprocessed.get_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(np.hstack([x_train, y_train.reshape(1914, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_0 = tmp[tmp[8] == \"N\"]\n",
    "tmp_1 = tmp[tmp[8] == \"IE\"]\n",
    "tmp_2 = tmp[tmp[8] == \"EI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tmp.columns:\n",
    "    if i != len(tmp.columns) - 1:\n",
    "        tmp[i] = tmp[i].astype(int)\n",
    "tmp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(data=tmp_0.values[:,:-1])\n",
    "plt.title(\"N\")\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(data=tmp_1.values[:,:-1])\n",
    "plt.title(\"IE\")\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=tmp_2.values[:,:-1])\n",
    "plt.title(\"EI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_preprocessed = DataPreprocess(df, is_vectorize=True, vectorizer=CountVectorizer)\n",
    "tf_preprocessed  = DataPreprocess(df, is_vectorize=True, vectorizer=TfidfVectorizer)\n",
    "\n",
    "cnt_model_train = ClassicModelTrain(cnt_preprocessed, is_run_random=False, is_run_extra_random=False)\n",
    "tf_model_train = ClassicModelTrain(tf_preprocessed, is_run_random=False, is_run_extra_random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### shape of X train and X_test : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1914, 8) (1276, 8)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Run logistic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Logistic Regression classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.56      0.16      0.25       320\n",
      "         IE       0.55      0.24      0.33       327\n",
      "          N       0.52      0.86      0.65       629\n",
      "\n",
      "avg / total       0.54      0.53      0.47      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Logistic Regression <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.51      0.19      0.28       236\n",
      "         IE       0.55      0.24      0.34       246\n",
      "          N       0.54      0.84      0.66       518\n",
      "\n",
      "avg / total       0.54      0.54      0.49      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Ten-fold cross-validation accuracy score : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### : 53.90000000000001"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Run multinomialNB"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### MultinomialNB classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.47      0.43      0.45       320\n",
      "         IE       0.50      0.49      0.50       327\n",
      "          N       0.56      0.59      0.58       629\n",
      "\n",
      "avg / total       0.52      0.53      0.52      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### MultinomialNB <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.44      0.46      0.45       223\n",
      "         IE       0.48      0.51      0.50       241\n",
      "          N       0.63      0.59      0.61       536\n",
      "\n",
      "avg / total       0.55      0.54      0.54      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Ten-fold cross-validation accuracy score : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### : 53.269999999999996"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Kernel Support Vector Machine classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.52      0.26      0.35       320\n",
      "         IE       0.55      0.35      0.43       327\n",
      "          N       0.54      0.78      0.64       629\n",
      "\n",
      "avg / total       0.54      0.54      0.51      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Kernel Support Vector Machine <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.66      0.38      0.48       250\n",
      "         IE       0.64      0.39      0.48       235\n",
      "          N       0.62      0.86      0.72       515\n",
      "\n",
      "avg / total       0.63      0.63      0.60      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Ten-fold cross-validation accuracy score : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### : 63.24999999999999"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Run xgboost"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### XGBoost classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.50      0.24      0.32       320\n",
      "         IE       0.53      0.42      0.47       327\n",
      "          N       0.56      0.76      0.64       629\n",
      "\n",
      "avg / total       0.54      0.54      0.52      1276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### XGBoost <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.64      0.31      0.42       240\n",
      "         IE       0.59      0.50      0.54       236\n",
      "          N       0.62      0.81      0.70       524\n",
      "\n",
      "avg / total       0.62      0.62      0.60      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Ten-fold cross-validation accuracy score : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### : 62.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 58.3 ms, total: 4.1 s\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnt_model_train.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_model_train.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = DataPreprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_train = ClassicModelTrain(preprocessed, is_run_extra_random=False, is_run_random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### shape of X train and X_test : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1914, 60) (1276, 60)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Run logistic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Logistic Regression classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.80      0.81      0.81       320\n",
      "         IE       0.82      0.85      0.83       327\n",
      "          N       0.93      0.91      0.92       629\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Logistic Regression <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.79      0.82      0.80       232\n",
      "         IE       0.79      0.86      0.82       234\n",
      "          N       0.96      0.90      0.93       534\n",
      "\n",
      "avg / total       0.88      0.87      0.87      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Run multinomialNB"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### MultinomialNB classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.79      0.85      0.82       320\n",
      "         IE       0.80      0.85      0.82       327\n",
      "          N       0.95      0.88      0.91       629\n",
      "\n",
      "avg / total       0.87      0.87      0.87      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### MultinomialNB <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.78      0.86      0.82       232\n",
      "         IE       0.81      0.88      0.84       246\n",
      "          N       0.95      0.87      0.91       522\n",
      "\n",
      "avg / total       0.88      0.87      0.87      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Kernel Support Vector Machine classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.97      0.28      0.43       320\n",
      "         IE       0.96      0.42      0.58       327\n",
      "          N       0.60      1.00      0.75       629\n",
      "\n",
      "avg / total       0.78      0.67      0.63      1276\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Kernel Support Vector Machine <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       1.00      0.71      0.83       233\n",
      "         IE       0.99      0.73      0.84       218\n",
      "          N       0.81      1.00      0.90       549\n",
      "\n",
      "avg / total       0.90      0.87      0.87      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Run xgboost"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08596\teval-mlogloss:1.08607\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:1.07351\teval-mlogloss:1.07371\n",
      "[2]\ttrain-mlogloss:1.06129\teval-mlogloss:1.06158\n",
      "[3]\ttrain-mlogloss:1.04926\teval-mlogloss:1.04966\n",
      "[4]\ttrain-mlogloss:1.03745\teval-mlogloss:1.03802\n",
      "[5]\ttrain-mlogloss:1.02584\teval-mlogloss:1.02651\n",
      "[6]\ttrain-mlogloss:1.0144\teval-mlogloss:1.01519\n",
      "[7]\ttrain-mlogloss:1.00317\teval-mlogloss:1.00413\n",
      "[8]\ttrain-mlogloss:0.99212\teval-mlogloss:0.993225\n",
      "[9]\ttrain-mlogloss:0.981233\teval-mlogloss:0.982478\n",
      "[10]\ttrain-mlogloss:0.970544\teval-mlogloss:0.971914\n",
      "[11]\ttrain-mlogloss:0.960019\teval-mlogloss:0.961562\n",
      "[12]\ttrain-mlogloss:0.949648\teval-mlogloss:0.951327\n",
      "[13]\ttrain-mlogloss:0.939462\teval-mlogloss:0.941296\n",
      "[14]\ttrain-mlogloss:0.929428\teval-mlogloss:0.931402\n",
      "[15]\ttrain-mlogloss:0.919547\teval-mlogloss:0.921646\n",
      "[16]\ttrain-mlogloss:0.909831\teval-mlogloss:0.912137\n",
      "[17]\ttrain-mlogloss:0.900268\teval-mlogloss:0.902814\n",
      "[18]\ttrain-mlogloss:0.890845\teval-mlogloss:0.893535\n",
      "[19]\ttrain-mlogloss:0.881572\teval-mlogloss:0.884459\n",
      "[20]\ttrain-mlogloss:0.872443\teval-mlogloss:0.875473\n",
      "[21]\ttrain-mlogloss:0.863431\teval-mlogloss:0.866739\n",
      "[22]\ttrain-mlogloss:0.854563\teval-mlogloss:0.858114\n",
      "[23]\ttrain-mlogloss:0.845831\teval-mlogloss:0.849621\n",
      "[24]\ttrain-mlogloss:0.837213\teval-mlogloss:0.841259\n",
      "[25]\ttrain-mlogloss:0.82873\teval-mlogloss:0.833027\n",
      "[26]\ttrain-mlogloss:0.820368\teval-mlogloss:0.824926\n",
      "[27]\ttrain-mlogloss:0.81212\teval-mlogloss:0.816924\n",
      "[28]\ttrain-mlogloss:0.804\teval-mlogloss:0.809065\n",
      "[29]\ttrain-mlogloss:0.795999\teval-mlogloss:0.801281\n",
      "[30]\ttrain-mlogloss:0.78811\teval-mlogloss:0.793594\n",
      "[31]\ttrain-mlogloss:0.780319\teval-mlogloss:0.786028\n",
      "[32]\ttrain-mlogloss:0.77265\teval-mlogloss:0.77861\n",
      "[33]\ttrain-mlogloss:0.765087\teval-mlogloss:0.771279\n",
      "[34]\ttrain-mlogloss:0.757625\teval-mlogloss:0.764015\n",
      "[35]\ttrain-mlogloss:0.750251\teval-mlogloss:0.756911\n",
      "[36]\ttrain-mlogloss:0.742979\teval-mlogloss:0.749899\n",
      "[37]\ttrain-mlogloss:0.735821\teval-mlogloss:0.742964\n",
      "[38]\ttrain-mlogloss:0.728736\teval-mlogloss:0.736131\n",
      "[39]\ttrain-mlogloss:0.721762\teval-mlogloss:0.729396\n",
      "[40]\ttrain-mlogloss:0.714889\teval-mlogloss:0.722727\n",
      "[41]\ttrain-mlogloss:0.708094\teval-mlogloss:0.716189\n",
      "[42]\ttrain-mlogloss:0.70139\teval-mlogloss:0.709723\n",
      "[43]\ttrain-mlogloss:0.694786\teval-mlogloss:0.703308\n",
      "[44]\ttrain-mlogloss:0.688258\teval-mlogloss:0.696953\n",
      "[45]\ttrain-mlogloss:0.681816\teval-mlogloss:0.690764\n",
      "[46]\ttrain-mlogloss:0.675458\teval-mlogloss:0.684684\n",
      "[47]\ttrain-mlogloss:0.669196\teval-mlogloss:0.67862\n",
      "[48]\ttrain-mlogloss:0.663\teval-mlogloss:0.672646\n",
      "[49]\ttrain-mlogloss:0.656886\teval-mlogloss:0.6667\n",
      "[50]\ttrain-mlogloss:0.650863\teval-mlogloss:0.660884\n",
      "[51]\ttrain-mlogloss:0.644906\teval-mlogloss:0.655161\n",
      "[52]\ttrain-mlogloss:0.639029\teval-mlogloss:0.649496\n",
      "[53]\ttrain-mlogloss:0.633235\teval-mlogloss:0.643913\n",
      "[54]\ttrain-mlogloss:0.627501\teval-mlogloss:0.638355\n",
      "[55]\ttrain-mlogloss:0.621801\teval-mlogloss:0.632893\n",
      "[56]\ttrain-mlogloss:0.616177\teval-mlogloss:0.627522\n",
      "[57]\ttrain-mlogloss:0.610641\teval-mlogloss:0.622192\n",
      "[58]\ttrain-mlogloss:0.605159\teval-mlogloss:0.616955\n",
      "[59]\ttrain-mlogloss:0.599747\teval-mlogloss:0.611774\n",
      "[60]\ttrain-mlogloss:0.594414\teval-mlogloss:0.606654\n",
      "[61]\ttrain-mlogloss:0.589134\teval-mlogloss:0.601594\n",
      "[62]\ttrain-mlogloss:0.583919\teval-mlogloss:0.59662\n",
      "[63]\ttrain-mlogloss:0.578777\teval-mlogloss:0.591732\n",
      "[64]\ttrain-mlogloss:0.573702\teval-mlogloss:0.58686\n",
      "[65]\ttrain-mlogloss:0.568676\teval-mlogloss:0.58207\n",
      "[66]\ttrain-mlogloss:0.563734\teval-mlogloss:0.577371\n",
      "[67]\ttrain-mlogloss:0.558852\teval-mlogloss:0.572716\n",
      "[68]\ttrain-mlogloss:0.554025\teval-mlogloss:0.568106\n",
      "[69]\ttrain-mlogloss:0.54926\teval-mlogloss:0.563568\n",
      "[70]\ttrain-mlogloss:0.544561\teval-mlogloss:0.559055\n",
      "[71]\ttrain-mlogloss:0.53991\teval-mlogloss:0.554623\n",
      "[72]\ttrain-mlogloss:0.535309\teval-mlogloss:0.550234\n",
      "[73]\ttrain-mlogloss:0.530767\teval-mlogloss:0.545905\n",
      "[74]\ttrain-mlogloss:0.526268\teval-mlogloss:0.541619\n",
      "[75]\ttrain-mlogloss:0.521825\teval-mlogloss:0.537386\n",
      "[76]\ttrain-mlogloss:0.517438\teval-mlogloss:0.53322\n",
      "[77]\ttrain-mlogloss:0.513107\teval-mlogloss:0.52909\n",
      "[78]\ttrain-mlogloss:0.508819\teval-mlogloss:0.525007\n",
      "[79]\ttrain-mlogloss:0.504588\teval-mlogloss:0.520984\n",
      "[80]\ttrain-mlogloss:0.500413\teval-mlogloss:0.516991\n",
      "[81]\ttrain-mlogloss:0.496271\teval-mlogloss:0.513037\n",
      "[82]\ttrain-mlogloss:0.492195\teval-mlogloss:0.509132\n",
      "[83]\ttrain-mlogloss:0.488174\teval-mlogloss:0.505263\n",
      "[84]\ttrain-mlogloss:0.484192\teval-mlogloss:0.501413\n",
      "[85]\ttrain-mlogloss:0.480261\teval-mlogloss:0.497643\n",
      "[86]\ttrain-mlogloss:0.476373\teval-mlogloss:0.49392\n",
      "[87]\ttrain-mlogloss:0.472539\teval-mlogloss:0.490208\n",
      "[88]\ttrain-mlogloss:0.468736\teval-mlogloss:0.486537\n",
      "[89]\ttrain-mlogloss:0.464977\teval-mlogloss:0.482954\n",
      "[90]\ttrain-mlogloss:0.461259\teval-mlogloss:0.479365\n",
      "[91]\ttrain-mlogloss:0.457593\teval-mlogloss:0.475845\n",
      "[92]\ttrain-mlogloss:0.453956\teval-mlogloss:0.472346\n",
      "[93]\ttrain-mlogloss:0.450368\teval-mlogloss:0.46892\n",
      "[94]\ttrain-mlogloss:0.446817\teval-mlogloss:0.465516\n",
      "[95]\ttrain-mlogloss:0.443294\teval-mlogloss:0.462162\n",
      "[96]\ttrain-mlogloss:0.439805\teval-mlogloss:0.458827\n",
      "[97]\ttrain-mlogloss:0.436353\teval-mlogloss:0.455522\n",
      "[98]\ttrain-mlogloss:0.432927\teval-mlogloss:0.452262\n",
      "[99]\ttrain-mlogloss:0.429546\teval-mlogloss:0.449067\n",
      "[100]\ttrain-mlogloss:0.426187\teval-mlogloss:0.445849\n",
      "[101]\ttrain-mlogloss:0.422877\teval-mlogloss:0.442735\n",
      "[102]\ttrain-mlogloss:0.419591\teval-mlogloss:0.439592\n",
      "[103]\ttrain-mlogloss:0.416361\teval-mlogloss:0.436539\n",
      "[104]\ttrain-mlogloss:0.413138\teval-mlogloss:0.433496\n",
      "[105]\ttrain-mlogloss:0.40997\teval-mlogloss:0.430481\n",
      "[106]\ttrain-mlogloss:0.406845\teval-mlogloss:0.427509\n",
      "[107]\ttrain-mlogloss:0.403726\teval-mlogloss:0.424564\n",
      "[108]\ttrain-mlogloss:0.400667\teval-mlogloss:0.421649\n",
      "[109]\ttrain-mlogloss:0.397631\teval-mlogloss:0.418766\n",
      "[110]\ttrain-mlogloss:0.394612\teval-mlogloss:0.415909\n",
      "[111]\ttrain-mlogloss:0.391651\teval-mlogloss:0.413121\n",
      "[112]\ttrain-mlogloss:0.388711\teval-mlogloss:0.41033\n",
      "[113]\ttrain-mlogloss:0.385788\teval-mlogloss:0.407569\n",
      "[114]\ttrain-mlogloss:0.382907\teval-mlogloss:0.404844\n",
      "[115]\ttrain-mlogloss:0.380049\teval-mlogloss:0.402145\n",
      "[116]\ttrain-mlogloss:0.377229\teval-mlogloss:0.399472\n",
      "[117]\ttrain-mlogloss:0.374432\teval-mlogloss:0.396813\n",
      "[118]\ttrain-mlogloss:0.371667\teval-mlogloss:0.394227\n",
      "[119]\ttrain-mlogloss:0.36895\teval-mlogloss:0.391665\n",
      "[120]\ttrain-mlogloss:0.366239\teval-mlogloss:0.389106\n",
      "[121]\ttrain-mlogloss:0.36355\teval-mlogloss:0.386569\n",
      "[122]\ttrain-mlogloss:0.3609\teval-mlogloss:0.384065\n",
      "[123]\ttrain-mlogloss:0.35828\teval-mlogloss:0.381608\n",
      "[124]\ttrain-mlogloss:0.355684\teval-mlogloss:0.37919\n",
      "[125]\ttrain-mlogloss:0.353126\teval-mlogloss:0.376766\n",
      "[126]\ttrain-mlogloss:0.350574\teval-mlogloss:0.374395\n",
      "[127]\ttrain-mlogloss:0.34806\teval-mlogloss:0.372047\n",
      "[128]\ttrain-mlogloss:0.345574\teval-mlogloss:0.369718\n",
      "[129]\ttrain-mlogloss:0.343107\teval-mlogloss:0.367401\n",
      "[130]\ttrain-mlogloss:0.340667\teval-mlogloss:0.365125\n",
      "[131]\ttrain-mlogloss:0.338265\teval-mlogloss:0.362854\n",
      "[132]\ttrain-mlogloss:0.335868\teval-mlogloss:0.360621\n",
      "[133]\ttrain-mlogloss:0.333505\teval-mlogloss:0.358414\n",
      "[134]\ttrain-mlogloss:0.331148\teval-mlogloss:0.356216\n",
      "[135]\ttrain-mlogloss:0.328841\teval-mlogloss:0.354079\n",
      "[136]\ttrain-mlogloss:0.326544\teval-mlogloss:0.351941\n",
      "[137]\ttrain-mlogloss:0.324284\teval-mlogloss:0.349833\n",
      "[138]\ttrain-mlogloss:0.321998\teval-mlogloss:0.347697\n",
      "[139]\ttrain-mlogloss:0.319769\teval-mlogloss:0.345624\n",
      "[140]\ttrain-mlogloss:0.317537\teval-mlogloss:0.343566\n",
      "[141]\ttrain-mlogloss:0.315368\teval-mlogloss:0.341536\n",
      "[142]\ttrain-mlogloss:0.313115\teval-mlogloss:0.339479\n",
      "[143]\ttrain-mlogloss:0.310884\teval-mlogloss:0.337448\n",
      "[144]\ttrain-mlogloss:0.308742\teval-mlogloss:0.335461\n",
      "[145]\ttrain-mlogloss:0.306587\teval-mlogloss:0.33346\n",
      "[146]\ttrain-mlogloss:0.304421\teval-mlogloss:0.331453\n",
      "[147]\ttrain-mlogloss:0.302351\teval-mlogloss:0.329481\n",
      "[148]\ttrain-mlogloss:0.300227\teval-mlogloss:0.327514\n",
      "[149]\ttrain-mlogloss:0.298141\teval-mlogloss:0.32557\n",
      "[150]\ttrain-mlogloss:0.296118\teval-mlogloss:0.32368\n",
      "[151]\ttrain-mlogloss:0.294071\teval-mlogloss:0.321797\n",
      "[152]\ttrain-mlogloss:0.292104\teval-mlogloss:0.319933\n",
      "[153]\ttrain-mlogloss:0.290086\teval-mlogloss:0.31809\n",
      "[154]\ttrain-mlogloss:0.288122\teval-mlogloss:0.316292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155]\ttrain-mlogloss:0.28621\teval-mlogloss:0.314469\n",
      "[156]\ttrain-mlogloss:0.284253\teval-mlogloss:0.312675\n",
      "[157]\ttrain-mlogloss:0.282377\teval-mlogloss:0.310936\n",
      "[158]\ttrain-mlogloss:0.28047\teval-mlogloss:0.309158\n",
      "[159]\ttrain-mlogloss:0.27857\teval-mlogloss:0.30742\n",
      "[160]\ttrain-mlogloss:0.276754\teval-mlogloss:0.305711\n",
      "[161]\ttrain-mlogloss:0.274921\teval-mlogloss:0.304034\n",
      "[162]\ttrain-mlogloss:0.273076\teval-mlogloss:0.30235\n",
      "[163]\ttrain-mlogloss:0.271264\teval-mlogloss:0.300682\n",
      "[164]\ttrain-mlogloss:0.269457\teval-mlogloss:0.29905\n",
      "[165]\ttrain-mlogloss:0.267737\teval-mlogloss:0.297427\n",
      "[166]\ttrain-mlogloss:0.265967\teval-mlogloss:0.295794\n",
      "[167]\ttrain-mlogloss:0.264205\teval-mlogloss:0.294189\n",
      "[168]\ttrain-mlogloss:0.262512\teval-mlogloss:0.292611\n",
      "[169]\ttrain-mlogloss:0.260803\teval-mlogloss:0.291044\n",
      "[170]\ttrain-mlogloss:0.259136\teval-mlogloss:0.289504\n",
      "[171]\ttrain-mlogloss:0.257463\teval-mlogloss:0.28798\n",
      "[172]\ttrain-mlogloss:0.255781\teval-mlogloss:0.286459\n",
      "[173]\ttrain-mlogloss:0.254178\teval-mlogloss:0.284941\n",
      "[174]\ttrain-mlogloss:0.252546\teval-mlogloss:0.283443\n",
      "[175]\ttrain-mlogloss:0.250962\teval-mlogloss:0.282002\n",
      "[176]\ttrain-mlogloss:0.249347\teval-mlogloss:0.280573\n",
      "[177]\ttrain-mlogloss:0.247769\teval-mlogloss:0.279125\n",
      "[178]\ttrain-mlogloss:0.246231\teval-mlogloss:0.277707\n",
      "[179]\ttrain-mlogloss:0.244651\teval-mlogloss:0.276348\n",
      "[180]\ttrain-mlogloss:0.243109\teval-mlogloss:0.27494\n",
      "[181]\ttrain-mlogloss:0.241601\teval-mlogloss:0.273519\n",
      "[182]\ttrain-mlogloss:0.240078\teval-mlogloss:0.272171\n",
      "[183]\ttrain-mlogloss:0.238568\teval-mlogloss:0.270882\n",
      "[184]\ttrain-mlogloss:0.237067\teval-mlogloss:0.269533\n",
      "[185]\ttrain-mlogloss:0.235618\teval-mlogloss:0.268176\n",
      "[186]\ttrain-mlogloss:0.234139\teval-mlogloss:0.266871\n",
      "[187]\ttrain-mlogloss:0.232718\teval-mlogloss:0.265549\n",
      "[188]\ttrain-mlogloss:0.231268\teval-mlogloss:0.264317\n",
      "[189]\ttrain-mlogloss:0.229891\teval-mlogloss:0.263039\n",
      "[190]\ttrain-mlogloss:0.228469\teval-mlogloss:0.261753\n",
      "[191]\ttrain-mlogloss:0.227113\teval-mlogloss:0.260479\n",
      "[192]\ttrain-mlogloss:0.225736\teval-mlogloss:0.259266\n",
      "[193]\ttrain-mlogloss:0.224342\teval-mlogloss:0.258053\n",
      "[194]\ttrain-mlogloss:0.223026\teval-mlogloss:0.25684\n",
      "[195]\ttrain-mlogloss:0.221658\teval-mlogloss:0.255657\n",
      "[196]\ttrain-mlogloss:0.220314\teval-mlogloss:0.254421\n",
      "[197]\ttrain-mlogloss:0.219015\teval-mlogloss:0.253226\n",
      "[198]\ttrain-mlogloss:0.217686\teval-mlogloss:0.252115\n",
      "[199]\ttrain-mlogloss:0.216392\teval-mlogloss:0.25093\n",
      "[200]\ttrain-mlogloss:0.215059\teval-mlogloss:0.249765\n",
      "[201]\ttrain-mlogloss:0.213754\teval-mlogloss:0.248636\n",
      "[202]\ttrain-mlogloss:0.212508\teval-mlogloss:0.247509\n",
      "[203]\ttrain-mlogloss:0.211235\teval-mlogloss:0.246435\n",
      "[204]\ttrain-mlogloss:0.210013\teval-mlogloss:0.245325\n",
      "[205]\ttrain-mlogloss:0.208743\teval-mlogloss:0.244248\n",
      "[206]\ttrain-mlogloss:0.207489\teval-mlogloss:0.243234\n",
      "[207]\ttrain-mlogloss:0.206258\teval-mlogloss:0.242163\n",
      "[208]\ttrain-mlogloss:0.205075\teval-mlogloss:0.241082\n",
      "[209]\ttrain-mlogloss:0.203855\teval-mlogloss:0.240104\n",
      "[210]\ttrain-mlogloss:0.202643\teval-mlogloss:0.239025\n",
      "[211]\ttrain-mlogloss:0.20144\teval-mlogloss:0.238043\n",
      "[212]\ttrain-mlogloss:0.200293\teval-mlogloss:0.236985\n",
      "[213]\ttrain-mlogloss:0.199119\teval-mlogloss:0.23595\n",
      "[214]\ttrain-mlogloss:0.197948\teval-mlogloss:0.235004\n",
      "[215]\ttrain-mlogloss:0.1968\teval-mlogloss:0.234014\n",
      "[216]\ttrain-mlogloss:0.195649\teval-mlogloss:0.233022\n",
      "[217]\ttrain-mlogloss:0.194542\teval-mlogloss:0.232006\n",
      "[218]\ttrain-mlogloss:0.193396\teval-mlogloss:0.231073\n",
      "[219]\ttrain-mlogloss:0.192253\teval-mlogloss:0.230082\n",
      "[220]\ttrain-mlogloss:0.191157\teval-mlogloss:0.229138\n",
      "[221]\ttrain-mlogloss:0.190037\teval-mlogloss:0.228211\n",
      "[222]\ttrain-mlogloss:0.18895\teval-mlogloss:0.227285\n",
      "[223]\ttrain-mlogloss:0.187867\teval-mlogloss:0.226329\n",
      "[224]\ttrain-mlogloss:0.186775\teval-mlogloss:0.225447\n",
      "[225]\ttrain-mlogloss:0.185726\teval-mlogloss:0.224554\n",
      "[226]\ttrain-mlogloss:0.18468\teval-mlogloss:0.223641\n",
      "[227]\ttrain-mlogloss:0.183628\teval-mlogloss:0.22275\n",
      "[228]\ttrain-mlogloss:0.18258\teval-mlogloss:0.221877\n",
      "[229]\ttrain-mlogloss:0.181562\teval-mlogloss:0.221026\n",
      "[230]\ttrain-mlogloss:0.180556\teval-mlogloss:0.220151\n",
      "[231]\ttrain-mlogloss:0.179519\teval-mlogloss:0.219293\n",
      "[232]\ttrain-mlogloss:0.178532\teval-mlogloss:0.218467\n",
      "[233]\ttrain-mlogloss:0.177511\teval-mlogloss:0.217613\n",
      "[234]\ttrain-mlogloss:0.176512\teval-mlogloss:0.216793\n",
      "[235]\ttrain-mlogloss:0.175545\teval-mlogloss:0.215977\n",
      "[236]\ttrain-mlogloss:0.174558\teval-mlogloss:0.215146\n",
      "[237]\ttrain-mlogloss:0.173621\teval-mlogloss:0.214329\n",
      "[238]\ttrain-mlogloss:0.172685\teval-mlogloss:0.213548\n",
      "[239]\ttrain-mlogloss:0.171732\teval-mlogloss:0.21275\n",
      "[240]\ttrain-mlogloss:0.170805\teval-mlogloss:0.211938\n",
      "[241]\ttrain-mlogloss:0.169854\teval-mlogloss:0.211165\n",
      "[242]\ttrain-mlogloss:0.168904\teval-mlogloss:0.21034\n",
      "[243]\ttrain-mlogloss:0.168002\teval-mlogloss:0.209545\n",
      "[244]\ttrain-mlogloss:0.167083\teval-mlogloss:0.208799\n",
      "[245]\ttrain-mlogloss:0.166183\teval-mlogloss:0.207986\n",
      "[246]\ttrain-mlogloss:0.165302\teval-mlogloss:0.207192\n",
      "[247]\ttrain-mlogloss:0.164397\teval-mlogloss:0.206467\n",
      "[248]\ttrain-mlogloss:0.163494\teval-mlogloss:0.205701\n",
      "[249]\ttrain-mlogloss:0.162648\teval-mlogloss:0.204951\n",
      "[250]\ttrain-mlogloss:0.161761\teval-mlogloss:0.204186\n",
      "[251]\ttrain-mlogloss:0.160917\teval-mlogloss:0.203493\n",
      "[252]\ttrain-mlogloss:0.160055\teval-mlogloss:0.202784\n",
      "[253]\ttrain-mlogloss:0.159227\teval-mlogloss:0.202066\n",
      "[254]\ttrain-mlogloss:0.158376\teval-mlogloss:0.201395\n",
      "[255]\ttrain-mlogloss:0.157548\teval-mlogloss:0.200703\n",
      "[256]\ttrain-mlogloss:0.156729\teval-mlogloss:0.199998\n",
      "[257]\ttrain-mlogloss:0.155905\teval-mlogloss:0.199338\n",
      "[258]\ttrain-mlogloss:0.155116\teval-mlogloss:0.19864\n",
      "[259]\ttrain-mlogloss:0.154321\teval-mlogloss:0.197937\n",
      "[260]\ttrain-mlogloss:0.15351\teval-mlogloss:0.197264\n",
      "[261]\ttrain-mlogloss:0.152705\teval-mlogloss:0.196617\n",
      "[262]\ttrain-mlogloss:0.151932\teval-mlogloss:0.195916\n",
      "[263]\ttrain-mlogloss:0.151137\teval-mlogloss:0.195276\n",
      "[264]\ttrain-mlogloss:0.150375\teval-mlogloss:0.194654\n",
      "[265]\ttrain-mlogloss:0.149603\teval-mlogloss:0.193981\n",
      "[266]\ttrain-mlogloss:0.148853\teval-mlogloss:0.193331\n",
      "[267]\ttrain-mlogloss:0.148083\teval-mlogloss:0.192696\n",
      "[268]\ttrain-mlogloss:0.147345\teval-mlogloss:0.19209\n",
      "[269]\ttrain-mlogloss:0.146595\teval-mlogloss:0.191449\n",
      "[270]\ttrain-mlogloss:0.145849\teval-mlogloss:0.190824\n",
      "[271]\ttrain-mlogloss:0.145132\teval-mlogloss:0.190228\n",
      "[272]\ttrain-mlogloss:0.144383\teval-mlogloss:0.189644\n",
      "[273]\ttrain-mlogloss:0.143671\teval-mlogloss:0.189031\n",
      "[274]\ttrain-mlogloss:0.142946\teval-mlogloss:0.188439\n",
      "[275]\ttrain-mlogloss:0.142253\teval-mlogloss:0.187844\n",
      "[276]\ttrain-mlogloss:0.141555\teval-mlogloss:0.187243\n",
      "[277]\ttrain-mlogloss:0.140833\teval-mlogloss:0.186679\n",
      "[278]\ttrain-mlogloss:0.140144\teval-mlogloss:0.18612\n",
      "[279]\ttrain-mlogloss:0.139471\teval-mlogloss:0.18555\n",
      "[280]\ttrain-mlogloss:0.138758\teval-mlogloss:0.184967\n",
      "[281]\ttrain-mlogloss:0.138065\teval-mlogloss:0.184426\n",
      "[282]\ttrain-mlogloss:0.13741\teval-mlogloss:0.183862\n",
      "[283]\ttrain-mlogloss:0.136748\teval-mlogloss:0.183287\n",
      "[284]\ttrain-mlogloss:0.136062\teval-mlogloss:0.182773\n",
      "[285]\ttrain-mlogloss:0.135414\teval-mlogloss:0.182229\n",
      "[286]\ttrain-mlogloss:0.134769\teval-mlogloss:0.181692\n",
      "[287]\ttrain-mlogloss:0.134133\teval-mlogloss:0.181131\n",
      "[288]\ttrain-mlogloss:0.133461\teval-mlogloss:0.180623\n",
      "[289]\ttrain-mlogloss:0.132804\teval-mlogloss:0.180065\n",
      "[290]\ttrain-mlogloss:0.132185\teval-mlogloss:0.17955\n",
      "[291]\ttrain-mlogloss:0.13157\teval-mlogloss:0.179052\n",
      "[292]\ttrain-mlogloss:0.130914\teval-mlogloss:0.178522\n",
      "[293]\ttrain-mlogloss:0.130281\teval-mlogloss:0.178037\n",
      "[294]\ttrain-mlogloss:0.129676\teval-mlogloss:0.177515\n",
      "[295]\ttrain-mlogloss:0.129072\teval-mlogloss:0.177018\n",
      "[296]\ttrain-mlogloss:0.128435\teval-mlogloss:0.176535\n",
      "[297]\ttrain-mlogloss:0.127849\teval-mlogloss:0.176079\n",
      "[298]\ttrain-mlogloss:0.127266\teval-mlogloss:0.175596\n",
      "[299]\ttrain-mlogloss:0.126669\teval-mlogloss:0.175104\n",
      "[300]\ttrain-mlogloss:0.126094\teval-mlogloss:0.174612\n",
      "[301]\ttrain-mlogloss:0.125494\teval-mlogloss:0.174144\n",
      "[302]\ttrain-mlogloss:0.124921\teval-mlogloss:0.173704\n",
      "[303]\ttrain-mlogloss:0.124328\teval-mlogloss:0.173257\n",
      "[304]\ttrain-mlogloss:0.123758\teval-mlogloss:0.172761\n",
      "[305]\ttrain-mlogloss:0.123178\teval-mlogloss:0.172323\n",
      "[306]\ttrain-mlogloss:0.122604\teval-mlogloss:0.171865\n",
      "[307]\ttrain-mlogloss:0.122054\teval-mlogloss:0.171381\n",
      "[308]\ttrain-mlogloss:0.121474\teval-mlogloss:0.170957\n",
      "[309]\ttrain-mlogloss:0.12093\teval-mlogloss:0.170504\n",
      "[310]\ttrain-mlogloss:0.120368\teval-mlogloss:0.170077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[311]\ttrain-mlogloss:0.119838\teval-mlogloss:0.16964\n",
      "[312]\ttrain-mlogloss:0.119265\teval-mlogloss:0.169214\n",
      "[313]\ttrain-mlogloss:0.118717\teval-mlogloss:0.168802\n",
      "[314]\ttrain-mlogloss:0.118183\teval-mlogloss:0.16834\n",
      "[315]\ttrain-mlogloss:0.117635\teval-mlogloss:0.167877\n",
      "[316]\ttrain-mlogloss:0.117117\teval-mlogloss:0.167442\n",
      "[317]\ttrain-mlogloss:0.116571\teval-mlogloss:0.167039\n",
      "[318]\ttrain-mlogloss:0.116059\teval-mlogloss:0.166592\n",
      "[319]\ttrain-mlogloss:0.115524\teval-mlogloss:0.16618\n",
      "[320]\ttrain-mlogloss:0.115018\teval-mlogloss:0.165723\n",
      "[321]\ttrain-mlogloss:0.11448\teval-mlogloss:0.165331\n",
      "[322]\ttrain-mlogloss:0.113979\teval-mlogloss:0.164909\n",
      "[323]\ttrain-mlogloss:0.113453\teval-mlogloss:0.164503\n",
      "[324]\ttrain-mlogloss:0.11296\teval-mlogloss:0.164064\n",
      "[325]\ttrain-mlogloss:0.11244\teval-mlogloss:0.163664\n",
      "[326]\ttrain-mlogloss:0.11195\teval-mlogloss:0.163294\n",
      "[327]\ttrain-mlogloss:0.111471\teval-mlogloss:0.162899\n",
      "[328]\ttrain-mlogloss:0.11095\teval-mlogloss:0.162516\n",
      "[329]\ttrain-mlogloss:0.110465\teval-mlogloss:0.162121\n",
      "[330]\ttrain-mlogloss:0.109961\teval-mlogloss:0.161711\n",
      "[331]\ttrain-mlogloss:0.109496\teval-mlogloss:0.161318\n",
      "[332]\ttrain-mlogloss:0.109034\teval-mlogloss:0.160939\n",
      "[333]\ttrain-mlogloss:0.108528\teval-mlogloss:0.160598\n",
      "[334]\ttrain-mlogloss:0.108035\teval-mlogloss:0.160236\n",
      "[335]\ttrain-mlogloss:0.107561\teval-mlogloss:0.159878\n",
      "[336]\ttrain-mlogloss:0.107055\teval-mlogloss:0.159515\n",
      "[337]\ttrain-mlogloss:0.106596\teval-mlogloss:0.159156\n",
      "[338]\ttrain-mlogloss:0.106142\teval-mlogloss:0.158797\n",
      "[339]\ttrain-mlogloss:0.105671\teval-mlogloss:0.158443\n",
      "[340]\ttrain-mlogloss:0.105192\teval-mlogloss:0.158072\n",
      "[341]\ttrain-mlogloss:0.104702\teval-mlogloss:0.157727\n",
      "[342]\ttrain-mlogloss:0.104254\teval-mlogloss:0.157348\n",
      "[343]\ttrain-mlogloss:0.103765\teval-mlogloss:0.157031\n",
      "[344]\ttrain-mlogloss:0.103325\teval-mlogloss:0.156689\n",
      "[345]\ttrain-mlogloss:0.102892\teval-mlogloss:0.156325\n",
      "[346]\ttrain-mlogloss:0.102429\teval-mlogloss:0.156\n",
      "[347]\ttrain-mlogloss:0.101996\teval-mlogloss:0.155647\n",
      "[348]\ttrain-mlogloss:0.101529\teval-mlogloss:0.155305\n",
      "[349]\ttrain-mlogloss:0.101105\teval-mlogloss:0.154961\n",
      "[350]\ttrain-mlogloss:0.100627\teval-mlogloss:0.154621\n",
      "[351]\ttrain-mlogloss:0.100194\teval-mlogloss:0.154284\n",
      "[352]\ttrain-mlogloss:0.099751\teval-mlogloss:0.153937\n",
      "[353]\ttrain-mlogloss:0.099319\teval-mlogloss:0.153588\n",
      "[354]\ttrain-mlogloss:0.098872\teval-mlogloss:0.153257\n",
      "[355]\ttrain-mlogloss:0.098463\teval-mlogloss:0.15297\n",
      "[356]\ttrain-mlogloss:0.09804\teval-mlogloss:0.152634\n",
      "[357]\ttrain-mlogloss:0.0976\teval-mlogloss:0.152308\n",
      "[358]\ttrain-mlogloss:0.097154\teval-mlogloss:0.152028\n",
      "[359]\ttrain-mlogloss:0.096755\teval-mlogloss:0.151707\n",
      "[360]\ttrain-mlogloss:0.096316\teval-mlogloss:0.151398\n",
      "[361]\ttrain-mlogloss:0.095927\teval-mlogloss:0.15109\n",
      "[362]\ttrain-mlogloss:0.095523\teval-mlogloss:0.150793\n",
      "[363]\ttrain-mlogloss:0.095096\teval-mlogloss:0.15051\n",
      "[364]\ttrain-mlogloss:0.094692\teval-mlogloss:0.150199\n",
      "[365]\ttrain-mlogloss:0.094275\teval-mlogloss:0.149922\n",
      "[366]\ttrain-mlogloss:0.093892\teval-mlogloss:0.149629\n",
      "[367]\ttrain-mlogloss:0.093494\teval-mlogloss:0.149322\n",
      "[368]\ttrain-mlogloss:0.093089\teval-mlogloss:0.149043\n",
      "[369]\ttrain-mlogloss:0.092698\teval-mlogloss:0.148742\n",
      "[370]\ttrain-mlogloss:0.092318\teval-mlogloss:0.148474\n",
      "[371]\ttrain-mlogloss:0.09192\teval-mlogloss:0.148196\n",
      "[372]\ttrain-mlogloss:0.09153\teval-mlogloss:0.147927\n",
      "[373]\ttrain-mlogloss:0.091125\teval-mlogloss:0.147643\n",
      "[374]\ttrain-mlogloss:0.090771\teval-mlogloss:0.14737\n",
      "[375]\ttrain-mlogloss:0.090367\teval-mlogloss:0.147129\n",
      "[376]\ttrain-mlogloss:0.089999\teval-mlogloss:0.146876\n",
      "[377]\ttrain-mlogloss:0.089608\teval-mlogloss:0.146624\n",
      "[378]\ttrain-mlogloss:0.089237\teval-mlogloss:0.146384\n",
      "[379]\ttrain-mlogloss:0.088857\teval-mlogloss:0.146135\n",
      "[380]\ttrain-mlogloss:0.088493\teval-mlogloss:0.145857\n",
      "[381]\ttrain-mlogloss:0.088114\teval-mlogloss:0.14561\n",
      "[382]\ttrain-mlogloss:0.087759\teval-mlogloss:0.145356\n",
      "[383]\ttrain-mlogloss:0.08737\teval-mlogloss:0.145094\n",
      "[384]\ttrain-mlogloss:0.087035\teval-mlogloss:0.144853\n",
      "[385]\ttrain-mlogloss:0.08666\teval-mlogloss:0.144636\n",
      "[386]\ttrain-mlogloss:0.086299\teval-mlogloss:0.144374\n",
      "[387]\ttrain-mlogloss:0.085967\teval-mlogloss:0.144145\n",
      "[388]\ttrain-mlogloss:0.085604\teval-mlogloss:0.143912\n",
      "[389]\ttrain-mlogloss:0.085243\teval-mlogloss:0.143676\n",
      "[390]\ttrain-mlogloss:0.084888\teval-mlogloss:0.143458\n",
      "[391]\ttrain-mlogloss:0.084521\teval-mlogloss:0.143234\n",
      "[392]\ttrain-mlogloss:0.084186\teval-mlogloss:0.142994\n",
      "[393]\ttrain-mlogloss:0.083836\teval-mlogloss:0.142786\n",
      "[394]\ttrain-mlogloss:0.083503\teval-mlogloss:0.142548\n",
      "[395]\ttrain-mlogloss:0.083153\teval-mlogloss:0.142326\n",
      "[396]\ttrain-mlogloss:0.08283\teval-mlogloss:0.142101\n",
      "[397]\ttrain-mlogloss:0.082475\teval-mlogloss:0.141859\n",
      "[398]\ttrain-mlogloss:0.082162\teval-mlogloss:0.141637\n",
      "[399]\ttrain-mlogloss:0.081807\teval-mlogloss:0.141428\n",
      "[400]\ttrain-mlogloss:0.081505\teval-mlogloss:0.141212\n",
      "[401]\ttrain-mlogloss:0.081171\teval-mlogloss:0.140979\n",
      "[402]\ttrain-mlogloss:0.080825\teval-mlogloss:0.140743\n",
      "[403]\ttrain-mlogloss:0.080501\teval-mlogloss:0.140553\n",
      "[404]\ttrain-mlogloss:0.080206\teval-mlogloss:0.140337\n",
      "[405]\ttrain-mlogloss:0.079859\teval-mlogloss:0.140099\n",
      "[406]\ttrain-mlogloss:0.07953\teval-mlogloss:0.139917\n",
      "[407]\ttrain-mlogloss:0.079244\teval-mlogloss:0.139695\n",
      "[408]\ttrain-mlogloss:0.078914\teval-mlogloss:0.139466\n",
      "[409]\ttrain-mlogloss:0.078593\teval-mlogloss:0.139239\n",
      "[410]\ttrain-mlogloss:0.078259\teval-mlogloss:0.139028\n",
      "[411]\ttrain-mlogloss:0.077973\teval-mlogloss:0.138829\n",
      "[412]\ttrain-mlogloss:0.07768\teval-mlogloss:0.138628\n",
      "[413]\ttrain-mlogloss:0.077329\teval-mlogloss:0.138407\n",
      "[414]\ttrain-mlogloss:0.077005\teval-mlogloss:0.138201\n",
      "[415]\ttrain-mlogloss:0.076678\teval-mlogloss:0.137985\n",
      "[416]\ttrain-mlogloss:0.076375\teval-mlogloss:0.137789\n",
      "[417]\ttrain-mlogloss:0.076062\teval-mlogloss:0.137567\n",
      "[418]\ttrain-mlogloss:0.075741\teval-mlogloss:0.137362\n",
      "[419]\ttrain-mlogloss:0.075437\teval-mlogloss:0.13718\n",
      "[420]\ttrain-mlogloss:0.075129\teval-mlogloss:0.136982\n",
      "[421]\ttrain-mlogloss:0.074822\teval-mlogloss:0.136772\n",
      "[422]\ttrain-mlogloss:0.074525\teval-mlogloss:0.136593\n",
      "[423]\ttrain-mlogloss:0.074256\teval-mlogloss:0.136401\n",
      "[424]\ttrain-mlogloss:0.073927\teval-mlogloss:0.136206\n",
      "[425]\ttrain-mlogloss:0.073623\teval-mlogloss:0.136005\n",
      "[426]\ttrain-mlogloss:0.073332\teval-mlogloss:0.1358\n",
      "[427]\ttrain-mlogloss:0.073035\teval-mlogloss:0.135636\n",
      "[428]\ttrain-mlogloss:0.072757\teval-mlogloss:0.135455\n",
      "[429]\ttrain-mlogloss:0.072447\teval-mlogloss:0.135286\n",
      "[430]\ttrain-mlogloss:0.072189\teval-mlogloss:0.135088\n",
      "[431]\ttrain-mlogloss:0.071905\teval-mlogloss:0.134885\n",
      "[432]\ttrain-mlogloss:0.071583\teval-mlogloss:0.134698\n",
      "[433]\ttrain-mlogloss:0.071331\teval-mlogloss:0.134521\n",
      "[434]\ttrain-mlogloss:0.071026\teval-mlogloss:0.134345\n",
      "[435]\ttrain-mlogloss:0.070765\teval-mlogloss:0.134174\n",
      "[436]\ttrain-mlogloss:0.070483\teval-mlogloss:0.133971\n",
      "[437]\ttrain-mlogloss:0.070202\teval-mlogloss:0.133781\n",
      "[438]\ttrain-mlogloss:0.069922\teval-mlogloss:0.13361\n",
      "[439]\ttrain-mlogloss:0.069618\teval-mlogloss:0.133451\n",
      "[440]\ttrain-mlogloss:0.069388\teval-mlogloss:0.133267\n",
      "[441]\ttrain-mlogloss:0.069084\teval-mlogloss:0.133097\n",
      "[442]\ttrain-mlogloss:0.068823\teval-mlogloss:0.132927\n",
      "[443]\ttrain-mlogloss:0.068537\teval-mlogloss:0.132781\n",
      "[444]\ttrain-mlogloss:0.068273\teval-mlogloss:0.132585\n",
      "[445]\ttrain-mlogloss:0.068028\teval-mlogloss:0.132414\n",
      "[446]\ttrain-mlogloss:0.067762\teval-mlogloss:0.132264\n",
      "[447]\ttrain-mlogloss:0.067466\teval-mlogloss:0.132115\n",
      "[448]\ttrain-mlogloss:0.06722\teval-mlogloss:0.131967\n",
      "[449]\ttrain-mlogloss:0.066954\teval-mlogloss:0.131784\n",
      "[450]\ttrain-mlogloss:0.066691\teval-mlogloss:0.131645\n",
      "[451]\ttrain-mlogloss:0.066418\teval-mlogloss:0.13148\n",
      "[452]\ttrain-mlogloss:0.066163\teval-mlogloss:0.131318\n",
      "[453]\ttrain-mlogloss:0.065888\teval-mlogloss:0.131157\n",
      "[454]\ttrain-mlogloss:0.065625\teval-mlogloss:0.131009\n",
      "[455]\ttrain-mlogloss:0.065386\teval-mlogloss:0.13085\n",
      "[456]\ttrain-mlogloss:0.065098\teval-mlogloss:0.130673\n",
      "[457]\ttrain-mlogloss:0.064848\teval-mlogloss:0.130546\n",
      "[458]\ttrain-mlogloss:0.064629\teval-mlogloss:0.130372\n",
      "[459]\ttrain-mlogloss:0.064372\teval-mlogloss:0.130229\n",
      "[460]\ttrain-mlogloss:0.064113\teval-mlogloss:0.130091\n",
      "[461]\ttrain-mlogloss:0.063876\teval-mlogloss:0.1299\n",
      "[462]\ttrain-mlogloss:0.063639\teval-mlogloss:0.129758\n",
      "[463]\ttrain-mlogloss:0.063361\teval-mlogloss:0.129597\n",
      "[464]\ttrain-mlogloss:0.063135\teval-mlogloss:0.129466\n",
      "[465]\ttrain-mlogloss:0.062902\teval-mlogloss:0.129306\n",
      "[466]\ttrain-mlogloss:0.062655\teval-mlogloss:0.129163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[467]\ttrain-mlogloss:0.062409\teval-mlogloss:0.129018\n",
      "[468]\ttrain-mlogloss:0.062152\teval-mlogloss:0.128894\n",
      "[469]\ttrain-mlogloss:0.061922\teval-mlogloss:0.128714\n",
      "[470]\ttrain-mlogloss:0.061679\teval-mlogloss:0.128585\n",
      "[471]\ttrain-mlogloss:0.061443\teval-mlogloss:0.128439\n",
      "[472]\ttrain-mlogloss:0.061224\teval-mlogloss:0.128333\n",
      "[473]\ttrain-mlogloss:0.060984\teval-mlogloss:0.128178\n",
      "[474]\ttrain-mlogloss:0.060767\teval-mlogloss:0.128044\n",
      "[475]\ttrain-mlogloss:0.060542\teval-mlogloss:0.127896\n",
      "[476]\ttrain-mlogloss:0.060304\teval-mlogloss:0.127775\n",
      "[477]\ttrain-mlogloss:0.060046\teval-mlogloss:0.127651\n",
      "[478]\ttrain-mlogloss:0.059843\teval-mlogloss:0.127505\n",
      "[479]\ttrain-mlogloss:0.059612\teval-mlogloss:0.127384\n",
      "[480]\ttrain-mlogloss:0.059373\teval-mlogloss:0.12723\n",
      "[481]\ttrain-mlogloss:0.059147\teval-mlogloss:0.127106\n",
      "[482]\ttrain-mlogloss:0.05892\teval-mlogloss:0.126966\n",
      "[483]\ttrain-mlogloss:0.058692\teval-mlogloss:0.126832\n",
      "[484]\ttrain-mlogloss:0.058463\teval-mlogloss:0.126686\n",
      "[485]\ttrain-mlogloss:0.058244\teval-mlogloss:0.126569\n",
      "[486]\ttrain-mlogloss:0.058052\teval-mlogloss:0.126462\n",
      "[487]\ttrain-mlogloss:0.057851\teval-mlogloss:0.126312\n",
      "[488]\ttrain-mlogloss:0.057629\teval-mlogloss:0.126186\n",
      "[489]\ttrain-mlogloss:0.057411\teval-mlogloss:0.126054\n",
      "[490]\ttrain-mlogloss:0.057185\teval-mlogloss:0.125931\n",
      "[491]\ttrain-mlogloss:0.056987\teval-mlogloss:0.125804\n",
      "[492]\ttrain-mlogloss:0.056791\teval-mlogloss:0.125673\n",
      "[493]\ttrain-mlogloss:0.056579\teval-mlogloss:0.125534\n",
      "[494]\ttrain-mlogloss:0.056373\teval-mlogloss:0.125384\n",
      "[495]\ttrain-mlogloss:0.056147\teval-mlogloss:0.125284\n",
      "[496]\ttrain-mlogloss:0.055939\teval-mlogloss:0.12514\n",
      "[497]\ttrain-mlogloss:0.055731\teval-mlogloss:0.125024\n",
      "[498]\ttrain-mlogloss:0.055523\teval-mlogloss:0.124917\n",
      "[499]\ttrain-mlogloss:0.055344\teval-mlogloss:0.124807\n",
      "[500]\ttrain-mlogloss:0.055147\teval-mlogloss:0.124707\n",
      "[501]\ttrain-mlogloss:0.054954\teval-mlogloss:0.124574\n",
      "[502]\ttrain-mlogloss:0.054753\teval-mlogloss:0.124433\n",
      "[503]\ttrain-mlogloss:0.054549\teval-mlogloss:0.124325\n",
      "[504]\ttrain-mlogloss:0.05434\teval-mlogloss:0.124189\n",
      "[505]\ttrain-mlogloss:0.054139\teval-mlogloss:0.12408\n",
      "[506]\ttrain-mlogloss:0.053949\teval-mlogloss:0.123951\n",
      "[507]\ttrain-mlogloss:0.053746\teval-mlogloss:0.123848\n",
      "[508]\ttrain-mlogloss:0.053563\teval-mlogloss:0.123706\n",
      "[509]\ttrain-mlogloss:0.053384\teval-mlogloss:0.123585\n",
      "[510]\ttrain-mlogloss:0.053175\teval-mlogloss:0.123489\n",
      "[511]\ttrain-mlogloss:0.052984\teval-mlogloss:0.123351\n",
      "[512]\ttrain-mlogloss:0.052798\teval-mlogloss:0.123223\n",
      "[513]\ttrain-mlogloss:0.052625\teval-mlogloss:0.12307\n",
      "[514]\ttrain-mlogloss:0.052411\teval-mlogloss:0.122952\n",
      "[515]\ttrain-mlogloss:0.052243\teval-mlogloss:0.122813\n",
      "[516]\ttrain-mlogloss:0.052055\teval-mlogloss:0.122707\n",
      "[517]\ttrain-mlogloss:0.051881\teval-mlogloss:0.122621\n",
      "[518]\ttrain-mlogloss:0.051677\teval-mlogloss:0.122502\n",
      "[519]\ttrain-mlogloss:0.051491\teval-mlogloss:0.122369\n",
      "[520]\ttrain-mlogloss:0.05131\teval-mlogloss:0.122254\n",
      "[521]\ttrain-mlogloss:0.051147\teval-mlogloss:0.122124\n",
      "[522]\ttrain-mlogloss:0.05094\teval-mlogloss:0.122009\n",
      "[523]\ttrain-mlogloss:0.050771\teval-mlogloss:0.121907\n",
      "[524]\ttrain-mlogloss:0.050589\teval-mlogloss:0.121804\n",
      "[525]\ttrain-mlogloss:0.050434\teval-mlogloss:0.1217\n",
      "[526]\ttrain-mlogloss:0.050264\teval-mlogloss:0.121586\n",
      "[527]\ttrain-mlogloss:0.050083\teval-mlogloss:0.121477\n",
      "[528]\ttrain-mlogloss:0.049909\teval-mlogloss:0.121388\n",
      "[529]\ttrain-mlogloss:0.049718\teval-mlogloss:0.121311\n",
      "[530]\ttrain-mlogloss:0.049561\teval-mlogloss:0.121182\n",
      "[531]\ttrain-mlogloss:0.049387\teval-mlogloss:0.121084\n",
      "[532]\ttrain-mlogloss:0.049206\teval-mlogloss:0.120988\n",
      "[533]\ttrain-mlogloss:0.049032\teval-mlogloss:0.12085\n",
      "[534]\ttrain-mlogloss:0.048842\teval-mlogloss:0.120763\n",
      "[535]\ttrain-mlogloss:0.04869\teval-mlogloss:0.120705\n",
      "[536]\ttrain-mlogloss:0.048526\teval-mlogloss:0.120596\n",
      "[537]\ttrain-mlogloss:0.048379\teval-mlogloss:0.120488\n",
      "[538]\ttrain-mlogloss:0.048199\teval-mlogloss:0.120371\n",
      "[539]\ttrain-mlogloss:0.048037\teval-mlogloss:0.120275\n",
      "[540]\ttrain-mlogloss:0.047871\teval-mlogloss:0.120186\n",
      "[541]\ttrain-mlogloss:0.047706\teval-mlogloss:0.120101\n",
      "[542]\ttrain-mlogloss:0.04756\teval-mlogloss:0.12002\n",
      "[543]\ttrain-mlogloss:0.0474\teval-mlogloss:0.119943\n",
      "[544]\ttrain-mlogloss:0.047227\teval-mlogloss:0.119844\n",
      "[545]\ttrain-mlogloss:0.047072\teval-mlogloss:0.119754\n",
      "[546]\ttrain-mlogloss:0.046917\teval-mlogloss:0.11966\n",
      "[547]\ttrain-mlogloss:0.046754\teval-mlogloss:0.119544\n",
      "[548]\ttrain-mlogloss:0.046612\teval-mlogloss:0.119456\n",
      "[549]\ttrain-mlogloss:0.046437\teval-mlogloss:0.119393\n",
      "[550]\ttrain-mlogloss:0.046287\teval-mlogloss:0.119291\n",
      "[551]\ttrain-mlogloss:0.046128\teval-mlogloss:0.119195\n",
      "[552]\ttrain-mlogloss:0.045956\teval-mlogloss:0.119109\n",
      "[553]\ttrain-mlogloss:0.045804\teval-mlogloss:0.119005\n",
      "[554]\ttrain-mlogloss:0.04565\teval-mlogloss:0.118944\n",
      "[555]\ttrain-mlogloss:0.045479\teval-mlogloss:0.118849\n",
      "[556]\ttrain-mlogloss:0.045323\teval-mlogloss:0.118736\n",
      "[557]\ttrain-mlogloss:0.045173\teval-mlogloss:0.118677\n",
      "[558]\ttrain-mlogloss:0.045009\teval-mlogloss:0.118571\n",
      "[559]\ttrain-mlogloss:0.04486\teval-mlogloss:0.118496\n",
      "[560]\ttrain-mlogloss:0.044714\teval-mlogloss:0.11843\n",
      "[561]\ttrain-mlogloss:0.044564\teval-mlogloss:0.118334\n",
      "[562]\ttrain-mlogloss:0.0444\teval-mlogloss:0.118236\n",
      "[563]\ttrain-mlogloss:0.044253\teval-mlogloss:0.118188\n",
      "[564]\ttrain-mlogloss:0.044113\teval-mlogloss:0.118088\n",
      "[565]\ttrain-mlogloss:0.043949\teval-mlogloss:0.118018\n",
      "[566]\ttrain-mlogloss:0.043812\teval-mlogloss:0.117899\n",
      "[567]\ttrain-mlogloss:0.043672\teval-mlogloss:0.117782\n",
      "[568]\ttrain-mlogloss:0.043534\teval-mlogloss:0.117696\n",
      "[569]\ttrain-mlogloss:0.043372\teval-mlogloss:0.117615\n",
      "[570]\ttrain-mlogloss:0.043237\teval-mlogloss:0.117535\n",
      "[571]\ttrain-mlogloss:0.043094\teval-mlogloss:0.117444\n",
      "[572]\ttrain-mlogloss:0.042939\teval-mlogloss:0.117335\n",
      "[573]\ttrain-mlogloss:0.042806\teval-mlogloss:0.117247\n",
      "[574]\ttrain-mlogloss:0.042655\teval-mlogloss:0.117149\n",
      "[575]\ttrain-mlogloss:0.042527\teval-mlogloss:0.117075\n",
      "[576]\ttrain-mlogloss:0.042388\teval-mlogloss:0.117033\n",
      "[577]\ttrain-mlogloss:0.042248\teval-mlogloss:0.116944\n",
      "[578]\ttrain-mlogloss:0.042094\teval-mlogloss:0.116849\n",
      "[579]\ttrain-mlogloss:0.041963\teval-mlogloss:0.116735\n",
      "[580]\ttrain-mlogloss:0.041826\teval-mlogloss:0.116672\n",
      "[581]\ttrain-mlogloss:0.04168\teval-mlogloss:0.116591\n",
      "[582]\ttrain-mlogloss:0.041538\teval-mlogloss:0.116512\n",
      "[583]\ttrain-mlogloss:0.041419\teval-mlogloss:0.11642\n",
      "[584]\ttrain-mlogloss:0.041268\teval-mlogloss:0.11634\n",
      "[585]\ttrain-mlogloss:0.041138\teval-mlogloss:0.116274\n",
      "[586]\ttrain-mlogloss:0.041005\teval-mlogloss:0.116211\n",
      "[587]\ttrain-mlogloss:0.040864\teval-mlogloss:0.116151\n",
      "[588]\ttrain-mlogloss:0.040718\teval-mlogloss:0.116056\n",
      "[589]\ttrain-mlogloss:0.040586\teval-mlogloss:0.115991\n",
      "[590]\ttrain-mlogloss:0.040452\teval-mlogloss:0.115911\n",
      "[591]\ttrain-mlogloss:0.040301\teval-mlogloss:0.115822\n",
      "[592]\ttrain-mlogloss:0.040157\teval-mlogloss:0.115747\n",
      "[593]\ttrain-mlogloss:0.040031\teval-mlogloss:0.115655\n",
      "[594]\ttrain-mlogloss:0.039887\teval-mlogloss:0.115573\n",
      "[595]\ttrain-mlogloss:0.039747\teval-mlogloss:0.115513\n",
      "[596]\ttrain-mlogloss:0.039638\teval-mlogloss:0.115461\n",
      "[597]\ttrain-mlogloss:0.0395\teval-mlogloss:0.115376\n",
      "[598]\ttrain-mlogloss:0.039362\teval-mlogloss:0.11532\n",
      "[599]\ttrain-mlogloss:0.039216\teval-mlogloss:0.115248\n",
      "[600]\ttrain-mlogloss:0.039079\teval-mlogloss:0.115176\n",
      "[601]\ttrain-mlogloss:0.038939\teval-mlogloss:0.115099\n",
      "[602]\ttrain-mlogloss:0.038811\teval-mlogloss:0.115036\n",
      "[603]\ttrain-mlogloss:0.038689\teval-mlogloss:0.114965\n",
      "[604]\ttrain-mlogloss:0.038575\teval-mlogloss:0.114892\n",
      "[605]\ttrain-mlogloss:0.038452\teval-mlogloss:0.114835\n",
      "[606]\ttrain-mlogloss:0.038334\teval-mlogloss:0.114765\n",
      "[607]\ttrain-mlogloss:0.038203\teval-mlogloss:0.114713\n",
      "[608]\ttrain-mlogloss:0.038089\teval-mlogloss:0.114643\n",
      "[609]\ttrain-mlogloss:0.037955\teval-mlogloss:0.114595\n",
      "[610]\ttrain-mlogloss:0.037843\teval-mlogloss:0.114531\n",
      "[611]\ttrain-mlogloss:0.037731\teval-mlogloss:0.114443\n",
      "[612]\ttrain-mlogloss:0.037615\teval-mlogloss:0.114401\n",
      "[613]\ttrain-mlogloss:0.037484\teval-mlogloss:0.114343\n",
      "[614]\ttrain-mlogloss:0.037369\teval-mlogloss:0.114293\n",
      "[615]\ttrain-mlogloss:0.037259\teval-mlogloss:0.11423\n",
      "[616]\ttrain-mlogloss:0.037144\teval-mlogloss:0.114183\n",
      "[617]\ttrain-mlogloss:0.037011\teval-mlogloss:0.114103\n",
      "[618]\ttrain-mlogloss:0.036907\teval-mlogloss:0.114051\n",
      "[619]\ttrain-mlogloss:0.036789\teval-mlogloss:0.11398\n",
      "[620]\ttrain-mlogloss:0.036681\teval-mlogloss:0.11391\n",
      "[621]\ttrain-mlogloss:0.036565\teval-mlogloss:0.113842\n",
      "[622]\ttrain-mlogloss:0.036463\teval-mlogloss:0.113786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[623]\ttrain-mlogloss:0.036344\teval-mlogloss:0.113741\n",
      "[624]\ttrain-mlogloss:0.036228\teval-mlogloss:0.113702\n",
      "[625]\ttrain-mlogloss:0.03612\teval-mlogloss:0.113633\n",
      "[626]\ttrain-mlogloss:0.036002\teval-mlogloss:0.113594\n",
      "[627]\ttrain-mlogloss:0.035901\teval-mlogloss:0.113565\n",
      "[628]\ttrain-mlogloss:0.035797\teval-mlogloss:0.113524\n",
      "[629]\ttrain-mlogloss:0.035683\teval-mlogloss:0.113462\n",
      "[630]\ttrain-mlogloss:0.035584\teval-mlogloss:0.113385\n",
      "[631]\ttrain-mlogloss:0.035468\teval-mlogloss:0.113336\n",
      "[632]\ttrain-mlogloss:0.035371\teval-mlogloss:0.113291\n",
      "[633]\ttrain-mlogloss:0.035259\teval-mlogloss:0.113227\n",
      "[634]\ttrain-mlogloss:0.035159\teval-mlogloss:0.113163\n",
      "[635]\ttrain-mlogloss:0.035044\teval-mlogloss:0.113135\n",
      "[636]\ttrain-mlogloss:0.034945\teval-mlogloss:0.113062\n",
      "[637]\ttrain-mlogloss:0.034835\teval-mlogloss:0.113027\n",
      "[638]\ttrain-mlogloss:0.034729\teval-mlogloss:0.112989\n",
      "[639]\ttrain-mlogloss:0.03463\teval-mlogloss:0.112927\n",
      "[640]\ttrain-mlogloss:0.034516\teval-mlogloss:0.112891\n",
      "[641]\ttrain-mlogloss:0.034422\teval-mlogloss:0.112851\n",
      "[642]\ttrain-mlogloss:0.034311\teval-mlogloss:0.112811\n",
      "[643]\ttrain-mlogloss:0.034209\teval-mlogloss:0.112756\n",
      "[644]\ttrain-mlogloss:0.034092\teval-mlogloss:0.112715\n",
      "[645]\ttrain-mlogloss:0.03399\teval-mlogloss:0.112681\n",
      "[646]\ttrain-mlogloss:0.033895\teval-mlogloss:0.112626\n",
      "[647]\ttrain-mlogloss:0.033792\teval-mlogloss:0.112577\n",
      "[648]\ttrain-mlogloss:0.033676\teval-mlogloss:0.112518\n",
      "[649]\ttrain-mlogloss:0.033573\teval-mlogloss:0.112471\n",
      "[650]\ttrain-mlogloss:0.033469\teval-mlogloss:0.112417\n",
      "[651]\ttrain-mlogloss:0.033358\teval-mlogloss:0.112363\n",
      "[652]\ttrain-mlogloss:0.03327\teval-mlogloss:0.112328\n",
      "[653]\ttrain-mlogloss:0.033173\teval-mlogloss:0.112264\n",
      "[654]\ttrain-mlogloss:0.033066\teval-mlogloss:0.112219\n",
      "[655]\ttrain-mlogloss:0.032958\teval-mlogloss:0.112198\n",
      "[656]\ttrain-mlogloss:0.032869\teval-mlogloss:0.112157\n",
      "[657]\ttrain-mlogloss:0.032769\teval-mlogloss:0.112119\n",
      "[658]\ttrain-mlogloss:0.032669\teval-mlogloss:0.11204\n",
      "[659]\ttrain-mlogloss:0.032563\teval-mlogloss:0.111987\n",
      "[660]\ttrain-mlogloss:0.032467\teval-mlogloss:0.111943\n",
      "[661]\ttrain-mlogloss:0.03237\teval-mlogloss:0.111905\n",
      "[662]\ttrain-mlogloss:0.032271\teval-mlogloss:0.11187\n",
      "[663]\ttrain-mlogloss:0.032167\teval-mlogloss:0.111813\n",
      "[664]\ttrain-mlogloss:0.032063\teval-mlogloss:0.111761\n",
      "[665]\ttrain-mlogloss:0.031967\teval-mlogloss:0.111713\n",
      "[666]\ttrain-mlogloss:0.031869\teval-mlogloss:0.111677\n",
      "[667]\ttrain-mlogloss:0.031778\teval-mlogloss:0.111641\n",
      "[668]\ttrain-mlogloss:0.03169\teval-mlogloss:0.11159\n",
      "[669]\ttrain-mlogloss:0.031593\teval-mlogloss:0.111546\n",
      "[670]\ttrain-mlogloss:0.031498\teval-mlogloss:0.111482\n",
      "[671]\ttrain-mlogloss:0.031398\teval-mlogloss:0.111425\n",
      "[672]\ttrain-mlogloss:0.031301\teval-mlogloss:0.111384\n",
      "[673]\ttrain-mlogloss:0.031219\teval-mlogloss:0.111337\n",
      "[674]\ttrain-mlogloss:0.031126\teval-mlogloss:0.11131\n",
      "[675]\ttrain-mlogloss:0.031038\teval-mlogloss:0.111258\n",
      "[676]\ttrain-mlogloss:0.030946\teval-mlogloss:0.111206\n",
      "[677]\ttrain-mlogloss:0.030851\teval-mlogloss:0.111178\n",
      "[678]\ttrain-mlogloss:0.030771\teval-mlogloss:0.111133\n",
      "[679]\ttrain-mlogloss:0.030677\teval-mlogloss:0.111088\n",
      "[680]\ttrain-mlogloss:0.030574\teval-mlogloss:0.111046\n",
      "[681]\ttrain-mlogloss:0.030494\teval-mlogloss:0.111011\n",
      "[682]\ttrain-mlogloss:0.030402\teval-mlogloss:0.110984\n",
      "[683]\ttrain-mlogloss:0.030319\teval-mlogloss:0.110933\n",
      "[684]\ttrain-mlogloss:0.030223\teval-mlogloss:0.110885\n",
      "[685]\ttrain-mlogloss:0.030136\teval-mlogloss:0.110835\n",
      "[686]\ttrain-mlogloss:0.030042\teval-mlogloss:0.110788\n",
      "[687]\ttrain-mlogloss:0.029956\teval-mlogloss:0.110767\n",
      "[688]\ttrain-mlogloss:0.029872\teval-mlogloss:0.110724\n",
      "[689]\ttrain-mlogloss:0.029781\teval-mlogloss:0.110686\n",
      "[690]\ttrain-mlogloss:0.029698\teval-mlogloss:0.11064\n",
      "[691]\ttrain-mlogloss:0.029607\teval-mlogloss:0.110598\n",
      "[692]\ttrain-mlogloss:0.029526\teval-mlogloss:0.110564\n",
      "[693]\ttrain-mlogloss:0.029444\teval-mlogloss:0.11054\n",
      "[694]\ttrain-mlogloss:0.029364\teval-mlogloss:0.110492\n",
      "[695]\ttrain-mlogloss:0.029278\teval-mlogloss:0.110455\n",
      "[696]\ttrain-mlogloss:0.029204\teval-mlogloss:0.110418\n",
      "[697]\ttrain-mlogloss:0.029123\teval-mlogloss:0.110387\n",
      "[698]\ttrain-mlogloss:0.029041\teval-mlogloss:0.110333\n",
      "[699]\ttrain-mlogloss:0.028963\teval-mlogloss:0.110286\n",
      "[700]\ttrain-mlogloss:0.028872\teval-mlogloss:0.110254\n",
      "[701]\ttrain-mlogloss:0.028789\teval-mlogloss:0.110213\n",
      "[702]\ttrain-mlogloss:0.028703\teval-mlogloss:0.110178\n",
      "[703]\ttrain-mlogloss:0.028631\teval-mlogloss:0.110133\n",
      "[704]\ttrain-mlogloss:0.028544\teval-mlogloss:0.110097\n",
      "[705]\ttrain-mlogloss:0.028471\teval-mlogloss:0.110064\n",
      "[706]\ttrain-mlogloss:0.028388\teval-mlogloss:0.110033\n",
      "[707]\ttrain-mlogloss:0.028309\teval-mlogloss:0.109994\n",
      "[708]\ttrain-mlogloss:0.028228\teval-mlogloss:0.109938\n",
      "[709]\ttrain-mlogloss:0.028141\teval-mlogloss:0.109903\n",
      "[710]\ttrain-mlogloss:0.028069\teval-mlogloss:0.109864\n",
      "[711]\ttrain-mlogloss:0.02798\teval-mlogloss:0.109835\n",
      "[712]\ttrain-mlogloss:0.027909\teval-mlogloss:0.109804\n",
      "[713]\ttrain-mlogloss:0.027832\teval-mlogloss:0.109757\n",
      "[714]\ttrain-mlogloss:0.027757\teval-mlogloss:0.109719\n",
      "[715]\ttrain-mlogloss:0.027679\teval-mlogloss:0.109676\n",
      "[716]\ttrain-mlogloss:0.027599\teval-mlogloss:0.109645\n",
      "[717]\ttrain-mlogloss:0.027519\teval-mlogloss:0.10964\n",
      "[718]\ttrain-mlogloss:0.027446\teval-mlogloss:0.109614\n",
      "[719]\ttrain-mlogloss:0.02736\teval-mlogloss:0.109573\n",
      "[720]\ttrain-mlogloss:0.027293\teval-mlogloss:0.109535\n",
      "[721]\ttrain-mlogloss:0.02722\teval-mlogloss:0.109517\n",
      "[722]\ttrain-mlogloss:0.027153\teval-mlogloss:0.109473\n",
      "[723]\ttrain-mlogloss:0.027075\teval-mlogloss:0.10945\n",
      "[724]\ttrain-mlogloss:0.026998\teval-mlogloss:0.109434\n",
      "[725]\ttrain-mlogloss:0.026934\teval-mlogloss:0.109394\n",
      "[726]\ttrain-mlogloss:0.02685\teval-mlogloss:0.109375\n",
      "[727]\ttrain-mlogloss:0.026784\teval-mlogloss:0.109338\n",
      "[728]\ttrain-mlogloss:0.026708\teval-mlogloss:0.109312\n",
      "[729]\ttrain-mlogloss:0.026637\teval-mlogloss:0.109299\n",
      "[730]\ttrain-mlogloss:0.026555\teval-mlogloss:0.109242\n",
      "[731]\ttrain-mlogloss:0.02648\teval-mlogloss:0.109222\n",
      "[732]\ttrain-mlogloss:0.026414\teval-mlogloss:0.109178\n",
      "[733]\ttrain-mlogloss:0.026337\teval-mlogloss:0.109163\n",
      "[734]\ttrain-mlogloss:0.026264\teval-mlogloss:0.109116\n",
      "[735]\ttrain-mlogloss:0.026199\teval-mlogloss:0.109072\n",
      "[736]\ttrain-mlogloss:0.026123\teval-mlogloss:0.109006\n",
      "[737]\ttrain-mlogloss:0.026045\teval-mlogloss:0.108994\n",
      "[738]\ttrain-mlogloss:0.025976\teval-mlogloss:0.108969\n",
      "[739]\ttrain-mlogloss:0.025903\teval-mlogloss:0.108972\n",
      "[740]\ttrain-mlogloss:0.025837\teval-mlogloss:0.108943\n",
      "[741]\ttrain-mlogloss:0.025761\teval-mlogloss:0.108933\n",
      "[742]\ttrain-mlogloss:0.025693\teval-mlogloss:0.108887\n",
      "[743]\ttrain-mlogloss:0.025622\teval-mlogloss:0.108832\n",
      "[744]\ttrain-mlogloss:0.025548\teval-mlogloss:0.108817\n",
      "[745]\ttrain-mlogloss:0.025474\teval-mlogloss:0.108767\n",
      "[746]\ttrain-mlogloss:0.0254\teval-mlogloss:0.108751\n",
      "[747]\ttrain-mlogloss:0.025332\teval-mlogloss:0.108731\n",
      "[748]\ttrain-mlogloss:0.025262\teval-mlogloss:0.108685\n",
      "[749]\ttrain-mlogloss:0.025188\teval-mlogloss:0.108653\n",
      "[750]\ttrain-mlogloss:0.025127\teval-mlogloss:0.10861\n",
      "[751]\ttrain-mlogloss:0.025048\teval-mlogloss:0.108611\n",
      "[752]\ttrain-mlogloss:0.024983\teval-mlogloss:0.108583\n",
      "[753]\ttrain-mlogloss:0.024914\teval-mlogloss:0.10858\n",
      "[754]\ttrain-mlogloss:0.024846\teval-mlogloss:0.108551\n",
      "[755]\ttrain-mlogloss:0.024784\teval-mlogloss:0.108524\n",
      "[756]\ttrain-mlogloss:0.024713\teval-mlogloss:0.108457\n",
      "[757]\ttrain-mlogloss:0.024648\teval-mlogloss:0.108405\n",
      "[758]\ttrain-mlogloss:0.024588\teval-mlogloss:0.108402\n",
      "[759]\ttrain-mlogloss:0.024519\teval-mlogloss:0.108351\n",
      "[760]\ttrain-mlogloss:0.024458\teval-mlogloss:0.108313\n",
      "[761]\ttrain-mlogloss:0.024387\teval-mlogloss:0.108294\n",
      "[762]\ttrain-mlogloss:0.024318\teval-mlogloss:0.108266\n",
      "[763]\ttrain-mlogloss:0.024252\teval-mlogloss:0.108212\n",
      "[764]\ttrain-mlogloss:0.024189\teval-mlogloss:0.108204\n",
      "[765]\ttrain-mlogloss:0.024119\teval-mlogloss:0.10819\n",
      "[766]\ttrain-mlogloss:0.024052\teval-mlogloss:0.108149\n",
      "[767]\ttrain-mlogloss:0.023984\teval-mlogloss:0.108124\n",
      "[768]\ttrain-mlogloss:0.023916\teval-mlogloss:0.108103\n",
      "[769]\ttrain-mlogloss:0.023855\teval-mlogloss:0.108073\n",
      "[770]\ttrain-mlogloss:0.023785\teval-mlogloss:0.108069\n",
      "[771]\ttrain-mlogloss:0.023719\teval-mlogloss:0.108008\n",
      "[772]\ttrain-mlogloss:0.023655\teval-mlogloss:0.107987\n",
      "[773]\ttrain-mlogloss:0.023601\teval-mlogloss:0.10794\n",
      "[774]\ttrain-mlogloss:0.023539\teval-mlogloss:0.107875\n",
      "[775]\ttrain-mlogloss:0.023483\teval-mlogloss:0.107855\n",
      "[776]\ttrain-mlogloss:0.023416\teval-mlogloss:0.107837\n",
      "[777]\ttrain-mlogloss:0.023351\teval-mlogloss:0.107795\n",
      "[778]\ttrain-mlogloss:0.023296\teval-mlogloss:0.107771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[779]\ttrain-mlogloss:0.023236\teval-mlogloss:0.107747\n",
      "[780]\ttrain-mlogloss:0.023172\teval-mlogloss:0.107731\n",
      "[781]\ttrain-mlogloss:0.02311\teval-mlogloss:0.107681\n",
      "[782]\ttrain-mlogloss:0.023049\teval-mlogloss:0.107676\n",
      "[783]\ttrain-mlogloss:0.02299\teval-mlogloss:0.10763\n",
      "[784]\ttrain-mlogloss:0.022935\teval-mlogloss:0.107613\n",
      "[785]\ttrain-mlogloss:0.022873\teval-mlogloss:0.107591\n",
      "[786]\ttrain-mlogloss:0.022819\teval-mlogloss:0.107578\n",
      "[787]\ttrain-mlogloss:0.022759\teval-mlogloss:0.107525\n",
      "[788]\ttrain-mlogloss:0.022695\teval-mlogloss:0.10752\n",
      "[789]\ttrain-mlogloss:0.022642\teval-mlogloss:0.10748\n",
      "[790]\ttrain-mlogloss:0.022581\teval-mlogloss:0.107447\n",
      "[791]\ttrain-mlogloss:0.022526\teval-mlogloss:0.107428\n",
      "[792]\ttrain-mlogloss:0.022471\teval-mlogloss:0.107426\n",
      "[793]\ttrain-mlogloss:0.022417\teval-mlogloss:0.107416\n",
      "[794]\ttrain-mlogloss:0.022357\teval-mlogloss:0.107394\n",
      "[795]\ttrain-mlogloss:0.022306\teval-mlogloss:0.107377\n",
      "[796]\ttrain-mlogloss:0.022246\teval-mlogloss:0.107369\n",
      "[797]\ttrain-mlogloss:0.022189\teval-mlogloss:0.107318\n",
      "[798]\ttrain-mlogloss:0.022139\teval-mlogloss:0.107288\n",
      "[799]\ttrain-mlogloss:0.022081\teval-mlogloss:0.107259\n",
      "[800]\ttrain-mlogloss:0.022023\teval-mlogloss:0.107241\n",
      "[801]\ttrain-mlogloss:0.021962\teval-mlogloss:0.107222\n",
      "[802]\ttrain-mlogloss:0.021905\teval-mlogloss:0.107208\n",
      "[803]\ttrain-mlogloss:0.021857\teval-mlogloss:0.107193\n",
      "[804]\ttrain-mlogloss:0.021799\teval-mlogloss:0.107174\n",
      "[805]\ttrain-mlogloss:0.021742\teval-mlogloss:0.107137\n",
      "[806]\ttrain-mlogloss:0.021683\teval-mlogloss:0.107116\n",
      "[807]\ttrain-mlogloss:0.021636\teval-mlogloss:0.107098\n",
      "[808]\ttrain-mlogloss:0.02158\teval-mlogloss:0.107069\n",
      "[809]\ttrain-mlogloss:0.021519\teval-mlogloss:0.107051\n",
      "[810]\ttrain-mlogloss:0.021465\teval-mlogloss:0.10705\n",
      "[811]\ttrain-mlogloss:0.021416\teval-mlogloss:0.107018\n",
      "[812]\ttrain-mlogloss:0.021361\teval-mlogloss:0.106975\n",
      "[813]\ttrain-mlogloss:0.021312\teval-mlogloss:0.106959\n",
      "[814]\ttrain-mlogloss:0.021253\teval-mlogloss:0.106944\n",
      "[815]\ttrain-mlogloss:0.021198\teval-mlogloss:0.106932\n",
      "[816]\ttrain-mlogloss:0.021153\teval-mlogloss:0.106916\n",
      "[817]\ttrain-mlogloss:0.021095\teval-mlogloss:0.10689\n",
      "[818]\ttrain-mlogloss:0.021039\teval-mlogloss:0.106876\n",
      "[819]\ttrain-mlogloss:0.020993\teval-mlogloss:0.106852\n",
      "[820]\ttrain-mlogloss:0.020937\teval-mlogloss:0.106844\n",
      "[821]\ttrain-mlogloss:0.020883\teval-mlogloss:0.106819\n",
      "[822]\ttrain-mlogloss:0.02083\teval-mlogloss:0.106775\n",
      "[823]\ttrain-mlogloss:0.020781\teval-mlogloss:0.106765\n",
      "[824]\ttrain-mlogloss:0.020731\teval-mlogloss:0.106751\n",
      "[825]\ttrain-mlogloss:0.020683\teval-mlogloss:0.106738\n",
      "[826]\ttrain-mlogloss:0.020628\teval-mlogloss:0.106688\n",
      "[827]\ttrain-mlogloss:0.020577\teval-mlogloss:0.106673\n",
      "[828]\ttrain-mlogloss:0.02053\teval-mlogloss:0.106658\n",
      "[829]\ttrain-mlogloss:0.020479\teval-mlogloss:0.1066\n",
      "[830]\ttrain-mlogloss:0.020429\teval-mlogloss:0.106584\n",
      "[831]\ttrain-mlogloss:0.020383\teval-mlogloss:0.106563\n",
      "[832]\ttrain-mlogloss:0.020333\teval-mlogloss:0.106547\n",
      "[833]\ttrain-mlogloss:0.020286\teval-mlogloss:0.106544\n",
      "[834]\ttrain-mlogloss:0.020235\teval-mlogloss:0.106504\n",
      "[835]\ttrain-mlogloss:0.020189\teval-mlogloss:0.106491\n",
      "[836]\ttrain-mlogloss:0.020138\teval-mlogloss:0.106459\n",
      "[837]\ttrain-mlogloss:0.020088\teval-mlogloss:0.106453\n",
      "[838]\ttrain-mlogloss:0.020042\teval-mlogloss:0.106426\n",
      "[839]\ttrain-mlogloss:0.019994\teval-mlogloss:0.106389\n",
      "[840]\ttrain-mlogloss:0.019943\teval-mlogloss:0.106392\n",
      "[841]\ttrain-mlogloss:0.0199\teval-mlogloss:0.106356\n",
      "[842]\ttrain-mlogloss:0.019851\teval-mlogloss:0.106319\n",
      "[843]\ttrain-mlogloss:0.019804\teval-mlogloss:0.106297\n",
      "[844]\ttrain-mlogloss:0.019755\teval-mlogloss:0.106281\n",
      "[845]\ttrain-mlogloss:0.019708\teval-mlogloss:0.106234\n",
      "[846]\ttrain-mlogloss:0.019659\teval-mlogloss:0.106195\n",
      "[847]\ttrain-mlogloss:0.019612\teval-mlogloss:0.106192\n",
      "[848]\ttrain-mlogloss:0.019563\teval-mlogloss:0.10617\n",
      "[849]\ttrain-mlogloss:0.019517\teval-mlogloss:0.106155\n",
      "[850]\ttrain-mlogloss:0.019473\teval-mlogloss:0.106126\n",
      "[851]\ttrain-mlogloss:0.019427\teval-mlogloss:0.10612\n",
      "[852]\ttrain-mlogloss:0.019382\teval-mlogloss:0.106085\n",
      "[853]\ttrain-mlogloss:0.01934\teval-mlogloss:0.106072\n",
      "[854]\ttrain-mlogloss:0.019292\teval-mlogloss:0.106039\n",
      "[855]\ttrain-mlogloss:0.019252\teval-mlogloss:0.106006\n",
      "[856]\ttrain-mlogloss:0.019206\teval-mlogloss:0.105993\n",
      "[857]\ttrain-mlogloss:0.019159\teval-mlogloss:0.105969\n",
      "[858]\ttrain-mlogloss:0.019116\teval-mlogloss:0.105967\n",
      "[859]\ttrain-mlogloss:0.019069\teval-mlogloss:0.105917\n",
      "[860]\ttrain-mlogloss:0.019022\teval-mlogloss:0.105874\n",
      "[861]\ttrain-mlogloss:0.018978\teval-mlogloss:0.105845\n",
      "[862]\ttrain-mlogloss:0.018928\teval-mlogloss:0.105827\n",
      "[863]\ttrain-mlogloss:0.018887\teval-mlogloss:0.105791\n",
      "[864]\ttrain-mlogloss:0.018843\teval-mlogloss:0.105755\n",
      "[865]\ttrain-mlogloss:0.0188\teval-mlogloss:0.105725\n",
      "[866]\ttrain-mlogloss:0.018756\teval-mlogloss:0.105698\n",
      "[867]\ttrain-mlogloss:0.018715\teval-mlogloss:0.105671\n",
      "[868]\ttrain-mlogloss:0.018672\teval-mlogloss:0.105655\n",
      "[869]\ttrain-mlogloss:0.018624\teval-mlogloss:0.105619\n",
      "[870]\ttrain-mlogloss:0.01858\teval-mlogloss:0.105601\n",
      "[871]\ttrain-mlogloss:0.018537\teval-mlogloss:0.10557\n",
      "[872]\ttrain-mlogloss:0.018492\teval-mlogloss:0.105533\n",
      "[873]\ttrain-mlogloss:0.018451\teval-mlogloss:0.105489\n",
      "[874]\ttrain-mlogloss:0.018412\teval-mlogloss:0.105451\n",
      "[875]\ttrain-mlogloss:0.018376\teval-mlogloss:0.105436\n",
      "[876]\ttrain-mlogloss:0.018334\teval-mlogloss:0.105411\n",
      "[877]\ttrain-mlogloss:0.018287\teval-mlogloss:0.105381\n",
      "[878]\ttrain-mlogloss:0.018244\teval-mlogloss:0.105355\n",
      "[879]\ttrain-mlogloss:0.018202\teval-mlogloss:0.105323\n",
      "[880]\ttrain-mlogloss:0.018161\teval-mlogloss:0.105312\n",
      "[881]\ttrain-mlogloss:0.01812\teval-mlogloss:0.105275\n",
      "[882]\ttrain-mlogloss:0.01808\teval-mlogloss:0.10525\n",
      "[883]\ttrain-mlogloss:0.018037\teval-mlogloss:0.105226\n",
      "[884]\ttrain-mlogloss:0.017996\teval-mlogloss:0.105188\n",
      "[885]\ttrain-mlogloss:0.017959\teval-mlogloss:0.10517\n",
      "[886]\ttrain-mlogloss:0.017919\teval-mlogloss:0.105137\n",
      "[887]\ttrain-mlogloss:0.017878\teval-mlogloss:0.105112\n",
      "[888]\ttrain-mlogloss:0.01784\teval-mlogloss:0.105096\n",
      "[889]\ttrain-mlogloss:0.017798\teval-mlogloss:0.105069\n",
      "[890]\ttrain-mlogloss:0.01776\teval-mlogloss:0.105047\n",
      "[891]\ttrain-mlogloss:0.017723\teval-mlogloss:0.105031\n",
      "[892]\ttrain-mlogloss:0.017686\teval-mlogloss:0.105019\n",
      "[893]\ttrain-mlogloss:0.017648\teval-mlogloss:0.104983\n",
      "[894]\ttrain-mlogloss:0.01761\teval-mlogloss:0.104946\n",
      "[895]\ttrain-mlogloss:0.017572\teval-mlogloss:0.104922\n",
      "[896]\ttrain-mlogloss:0.01753\teval-mlogloss:0.104909\n",
      "[897]\ttrain-mlogloss:0.017495\teval-mlogloss:0.104902\n",
      "[898]\ttrain-mlogloss:0.01746\teval-mlogloss:0.104872\n",
      "[899]\ttrain-mlogloss:0.017421\teval-mlogloss:0.104837\n",
      "[900]\ttrain-mlogloss:0.017387\teval-mlogloss:0.10482\n",
      "[901]\ttrain-mlogloss:0.017345\teval-mlogloss:0.104801\n",
      "[902]\ttrain-mlogloss:0.017308\teval-mlogloss:0.104781\n",
      "[903]\ttrain-mlogloss:0.017268\teval-mlogloss:0.104745\n",
      "[904]\ttrain-mlogloss:0.017231\teval-mlogloss:0.104716\n",
      "[905]\ttrain-mlogloss:0.017197\teval-mlogloss:0.104707\n",
      "[906]\ttrain-mlogloss:0.01716\teval-mlogloss:0.104686\n",
      "[907]\ttrain-mlogloss:0.017123\teval-mlogloss:0.10466\n",
      "[908]\ttrain-mlogloss:0.017087\teval-mlogloss:0.104644\n",
      "[909]\ttrain-mlogloss:0.017053\teval-mlogloss:0.104626\n",
      "[910]\ttrain-mlogloss:0.017018\teval-mlogloss:0.104604\n",
      "[911]\ttrain-mlogloss:0.016982\teval-mlogloss:0.104572\n",
      "[912]\ttrain-mlogloss:0.016948\teval-mlogloss:0.104532\n",
      "[913]\ttrain-mlogloss:0.016914\teval-mlogloss:0.104497\n",
      "[914]\ttrain-mlogloss:0.016878\teval-mlogloss:0.104488\n",
      "[915]\ttrain-mlogloss:0.016844\teval-mlogloss:0.104477\n",
      "[916]\ttrain-mlogloss:0.016812\teval-mlogloss:0.104479\n",
      "[917]\ttrain-mlogloss:0.016774\teval-mlogloss:0.104461\n",
      "[918]\ttrain-mlogloss:0.01674\teval-mlogloss:0.104418\n",
      "[919]\ttrain-mlogloss:0.016708\teval-mlogloss:0.104405\n",
      "[920]\ttrain-mlogloss:0.016673\teval-mlogloss:0.10439\n",
      "[921]\ttrain-mlogloss:0.016637\teval-mlogloss:0.10437\n",
      "[922]\ttrain-mlogloss:0.016603\teval-mlogloss:0.104351\n",
      "[923]\ttrain-mlogloss:0.01657\teval-mlogloss:0.104337\n",
      "[924]\ttrain-mlogloss:0.016539\teval-mlogloss:0.104318\n",
      "[925]\ttrain-mlogloss:0.016502\teval-mlogloss:0.104298\n",
      "[926]\ttrain-mlogloss:0.01647\teval-mlogloss:0.104268\n",
      "[927]\ttrain-mlogloss:0.016437\teval-mlogloss:0.104241\n",
      "[928]\ttrain-mlogloss:0.016404\teval-mlogloss:0.104218\n",
      "[929]\ttrain-mlogloss:0.016373\teval-mlogloss:0.104205\n",
      "[930]\ttrain-mlogloss:0.016341\teval-mlogloss:0.104187\n",
      "[931]\ttrain-mlogloss:0.016306\teval-mlogloss:0.104153\n",
      "[932]\ttrain-mlogloss:0.016276\teval-mlogloss:0.104135\n",
      "[933]\ttrain-mlogloss:0.016245\teval-mlogloss:0.104122\n",
      "[934]\ttrain-mlogloss:0.016214\teval-mlogloss:0.104091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[935]\ttrain-mlogloss:0.016176\teval-mlogloss:0.104064\n",
      "[936]\ttrain-mlogloss:0.016143\teval-mlogloss:0.104039\n",
      "[937]\ttrain-mlogloss:0.016111\teval-mlogloss:0.104006\n",
      "[938]\ttrain-mlogloss:0.01608\teval-mlogloss:0.103986\n",
      "[939]\ttrain-mlogloss:0.01605\teval-mlogloss:0.103969\n",
      "[940]\ttrain-mlogloss:0.016019\teval-mlogloss:0.103951\n",
      "[941]\ttrain-mlogloss:0.015988\teval-mlogloss:0.103931\n",
      "[942]\ttrain-mlogloss:0.015955\teval-mlogloss:0.103903\n",
      "[943]\ttrain-mlogloss:0.015925\teval-mlogloss:0.103879\n",
      "[944]\ttrain-mlogloss:0.015896\teval-mlogloss:0.103871\n",
      "[945]\ttrain-mlogloss:0.015861\teval-mlogloss:0.103833\n",
      "[946]\ttrain-mlogloss:0.015829\teval-mlogloss:0.103817\n",
      "[947]\ttrain-mlogloss:0.015799\teval-mlogloss:0.10379\n",
      "[948]\ttrain-mlogloss:0.015768\teval-mlogloss:0.103771\n",
      "[949]\ttrain-mlogloss:0.015739\teval-mlogloss:0.103764\n",
      "[950]\ttrain-mlogloss:0.015711\teval-mlogloss:0.103742\n",
      "[951]\ttrain-mlogloss:0.015677\teval-mlogloss:0.103739\n",
      "[952]\ttrain-mlogloss:0.015647\teval-mlogloss:0.103728\n",
      "[953]\ttrain-mlogloss:0.015616\teval-mlogloss:0.103705\n",
      "[954]\ttrain-mlogloss:0.015584\teval-mlogloss:0.103679\n",
      "[955]\ttrain-mlogloss:0.015555\teval-mlogloss:0.103633\n",
      "[956]\ttrain-mlogloss:0.015526\teval-mlogloss:0.103628\n",
      "[957]\ttrain-mlogloss:0.015495\teval-mlogloss:0.103608\n",
      "[958]\ttrain-mlogloss:0.015466\teval-mlogloss:0.103617\n",
      "[959]\ttrain-mlogloss:0.015434\teval-mlogloss:0.103576\n",
      "[960]\ttrain-mlogloss:0.015405\teval-mlogloss:0.103574\n",
      "[961]\ttrain-mlogloss:0.015376\teval-mlogloss:0.103556\n",
      "[962]\ttrain-mlogloss:0.015346\teval-mlogloss:0.103522\n",
      "[963]\ttrain-mlogloss:0.015318\teval-mlogloss:0.103514\n",
      "[964]\ttrain-mlogloss:0.015287\teval-mlogloss:0.103499\n",
      "[965]\ttrain-mlogloss:0.015259\teval-mlogloss:0.103485\n",
      "[966]\ttrain-mlogloss:0.01523\teval-mlogloss:0.10345\n",
      "[967]\ttrain-mlogloss:0.015202\teval-mlogloss:0.103445\n",
      "[968]\ttrain-mlogloss:0.015174\teval-mlogloss:0.103448\n",
      "[969]\ttrain-mlogloss:0.015145\teval-mlogloss:0.103425\n",
      "[970]\ttrain-mlogloss:0.015116\teval-mlogloss:0.103412\n",
      "[971]\ttrain-mlogloss:0.015089\teval-mlogloss:0.103375\n",
      "[972]\ttrain-mlogloss:0.015057\teval-mlogloss:0.103366\n",
      "[973]\ttrain-mlogloss:0.015029\teval-mlogloss:0.103341\n",
      "[974]\ttrain-mlogloss:0.014999\teval-mlogloss:0.103318\n",
      "[975]\ttrain-mlogloss:0.014971\teval-mlogloss:0.103315\n",
      "[976]\ttrain-mlogloss:0.014944\teval-mlogloss:0.103297\n",
      "[977]\ttrain-mlogloss:0.014916\teval-mlogloss:0.103279\n",
      "[978]\ttrain-mlogloss:0.014887\teval-mlogloss:0.103259\n",
      "[979]\ttrain-mlogloss:0.014859\teval-mlogloss:0.103247\n",
      "[980]\ttrain-mlogloss:0.014833\teval-mlogloss:0.103236\n",
      "[981]\ttrain-mlogloss:0.014805\teval-mlogloss:0.103235\n",
      "[982]\ttrain-mlogloss:0.014775\teval-mlogloss:0.103201\n",
      "[983]\ttrain-mlogloss:0.01475\teval-mlogloss:0.10318\n",
      "[984]\ttrain-mlogloss:0.014721\teval-mlogloss:0.103152\n",
      "[985]\ttrain-mlogloss:0.014695\teval-mlogloss:0.103142\n",
      "[986]\ttrain-mlogloss:0.014668\teval-mlogloss:0.103113\n",
      "[987]\ttrain-mlogloss:0.01464\teval-mlogloss:0.1031\n",
      "[988]\ttrain-mlogloss:0.014613\teval-mlogloss:0.103068\n",
      "[989]\ttrain-mlogloss:0.014584\teval-mlogloss:0.103042\n",
      "[990]\ttrain-mlogloss:0.014553\teval-mlogloss:0.103023\n",
      "[991]\ttrain-mlogloss:0.014527\teval-mlogloss:0.103001\n",
      "[992]\ttrain-mlogloss:0.0145\teval-mlogloss:0.102969\n",
      "[993]\ttrain-mlogloss:0.014474\teval-mlogloss:0.102955\n",
      "[994]\ttrain-mlogloss:0.014447\teval-mlogloss:0.102931\n",
      "[995]\ttrain-mlogloss:0.014421\teval-mlogloss:0.102922\n",
      "[996]\ttrain-mlogloss:0.014396\teval-mlogloss:0.102918\n",
      "[997]\ttrain-mlogloss:0.014369\teval-mlogloss:0.102903\n",
      "[998]\ttrain-mlogloss:0.014344\teval-mlogloss:0.102867\n",
      "[999]\ttrain-mlogloss:0.014313\teval-mlogloss:0.102851\n",
      "[1000]\ttrain-mlogloss:0.014286\teval-mlogloss:0.102834\n",
      "[1001]\ttrain-mlogloss:0.014259\teval-mlogloss:0.102808\n",
      "[1002]\ttrain-mlogloss:0.014233\teval-mlogloss:0.102787\n",
      "[1003]\ttrain-mlogloss:0.014208\teval-mlogloss:0.102758\n",
      "[1004]\ttrain-mlogloss:0.014182\teval-mlogloss:0.102742\n",
      "[1005]\ttrain-mlogloss:0.014158\teval-mlogloss:0.10275\n",
      "[1006]\ttrain-mlogloss:0.01413\teval-mlogloss:0.102713\n",
      "[1007]\ttrain-mlogloss:0.014105\teval-mlogloss:0.102688\n",
      "[1008]\ttrain-mlogloss:0.014079\teval-mlogloss:0.102694\n",
      "[1009]\ttrain-mlogloss:0.014051\teval-mlogloss:0.102669\n",
      "[1010]\ttrain-mlogloss:0.014026\teval-mlogloss:0.102636\n",
      "[1011]\ttrain-mlogloss:0.014001\teval-mlogloss:0.102619\n",
      "[1012]\ttrain-mlogloss:0.013976\teval-mlogloss:0.102602\n",
      "[1013]\ttrain-mlogloss:0.013951\teval-mlogloss:0.102581\n",
      "[1014]\ttrain-mlogloss:0.013924\teval-mlogloss:0.102575\n",
      "[1015]\ttrain-mlogloss:0.0139\teval-mlogloss:0.10256\n",
      "[1016]\ttrain-mlogloss:0.013874\teval-mlogloss:0.102536\n",
      "[1017]\ttrain-mlogloss:0.013849\teval-mlogloss:0.102519\n",
      "[1018]\ttrain-mlogloss:0.013825\teval-mlogloss:0.102499\n",
      "[1019]\ttrain-mlogloss:0.013801\teval-mlogloss:0.102469\n",
      "[1020]\ttrain-mlogloss:0.013774\teval-mlogloss:0.102468\n",
      "[1021]\ttrain-mlogloss:0.013747\teval-mlogloss:0.102442\n",
      "[1022]\ttrain-mlogloss:0.013724\teval-mlogloss:0.102413\n",
      "[1023]\ttrain-mlogloss:0.0137\teval-mlogloss:0.102406\n",
      "[1024]\ttrain-mlogloss:0.013676\teval-mlogloss:0.102381\n",
      "[1025]\ttrain-mlogloss:0.013654\teval-mlogloss:0.102345\n",
      "[1026]\ttrain-mlogloss:0.013628\teval-mlogloss:0.102321\n",
      "[1027]\ttrain-mlogloss:0.013603\teval-mlogloss:0.1023\n",
      "[1028]\ttrain-mlogloss:0.013579\teval-mlogloss:0.10229\n",
      "[1029]\ttrain-mlogloss:0.013557\teval-mlogloss:0.102282\n",
      "[1030]\ttrain-mlogloss:0.013534\teval-mlogloss:0.102261\n",
      "[1031]\ttrain-mlogloss:0.013511\teval-mlogloss:0.102248\n",
      "[1032]\ttrain-mlogloss:0.013486\teval-mlogloss:0.102224\n",
      "[1033]\ttrain-mlogloss:0.01346\teval-mlogloss:0.10219\n",
      "[1034]\ttrain-mlogloss:0.013438\teval-mlogloss:0.102168\n",
      "[1035]\ttrain-mlogloss:0.013414\teval-mlogloss:0.102136\n",
      "[1036]\ttrain-mlogloss:0.013391\teval-mlogloss:0.10212\n",
      "[1037]\ttrain-mlogloss:0.013368\teval-mlogloss:0.102097\n",
      "[1038]\ttrain-mlogloss:0.013345\teval-mlogloss:0.102085\n",
      "[1039]\ttrain-mlogloss:0.013319\teval-mlogloss:0.102053\n",
      "[1040]\ttrain-mlogloss:0.013294\teval-mlogloss:0.102041\n",
      "[1041]\ttrain-mlogloss:0.013271\teval-mlogloss:0.102031\n",
      "[1042]\ttrain-mlogloss:0.013248\teval-mlogloss:0.102019\n",
      "[1043]\ttrain-mlogloss:0.013225\teval-mlogloss:0.102013\n",
      "[1044]\ttrain-mlogloss:0.013203\teval-mlogloss:0.101981\n",
      "[1045]\ttrain-mlogloss:0.013179\teval-mlogloss:0.101961\n",
      "[1046]\ttrain-mlogloss:0.013158\teval-mlogloss:0.101948\n",
      "[1047]\ttrain-mlogloss:0.013136\teval-mlogloss:0.101916\n",
      "[1048]\ttrain-mlogloss:0.013113\teval-mlogloss:0.1019\n",
      "[1049]\ttrain-mlogloss:0.013092\teval-mlogloss:0.101881\n",
      "[1050]\ttrain-mlogloss:0.013069\teval-mlogloss:0.101855\n",
      "[1051]\ttrain-mlogloss:0.013047\teval-mlogloss:0.101836\n",
      "[1052]\ttrain-mlogloss:0.013025\teval-mlogloss:0.10182\n",
      "[1053]\ttrain-mlogloss:0.013005\teval-mlogloss:0.101801\n",
      "[1054]\ttrain-mlogloss:0.012979\teval-mlogloss:0.101776\n",
      "[1055]\ttrain-mlogloss:0.012954\teval-mlogloss:0.101758\n",
      "[1056]\ttrain-mlogloss:0.012932\teval-mlogloss:0.101755\n",
      "[1057]\ttrain-mlogloss:0.01291\teval-mlogloss:0.101731\n",
      "[1058]\ttrain-mlogloss:0.012889\teval-mlogloss:0.10171\n",
      "[1059]\ttrain-mlogloss:0.012868\teval-mlogloss:0.101702\n",
      "[1060]\ttrain-mlogloss:0.012846\teval-mlogloss:0.101675\n",
      "[1061]\ttrain-mlogloss:0.012824\teval-mlogloss:0.101659\n",
      "[1062]\ttrain-mlogloss:0.012802\teval-mlogloss:0.101645\n",
      "[1063]\ttrain-mlogloss:0.012778\teval-mlogloss:0.101624\n",
      "[1064]\ttrain-mlogloss:0.012757\teval-mlogloss:0.101615\n",
      "[1065]\ttrain-mlogloss:0.012736\teval-mlogloss:0.101598\n",
      "[1066]\ttrain-mlogloss:0.012713\teval-mlogloss:0.101587\n",
      "[1067]\ttrain-mlogloss:0.012691\teval-mlogloss:0.101556\n",
      "[1068]\ttrain-mlogloss:0.012672\teval-mlogloss:0.101536\n",
      "[1069]\ttrain-mlogloss:0.012648\teval-mlogloss:0.101518\n",
      "[1070]\ttrain-mlogloss:0.012627\teval-mlogloss:0.101508\n",
      "[1071]\ttrain-mlogloss:0.012605\teval-mlogloss:0.101491\n",
      "[1072]\ttrain-mlogloss:0.012585\teval-mlogloss:0.101475\n",
      "[1073]\ttrain-mlogloss:0.012562\teval-mlogloss:0.101468\n",
      "[1074]\ttrain-mlogloss:0.012542\teval-mlogloss:0.101466\n",
      "[1075]\ttrain-mlogloss:0.012521\teval-mlogloss:0.101448\n",
      "[1076]\ttrain-mlogloss:0.012503\teval-mlogloss:0.101425\n",
      "[1077]\ttrain-mlogloss:0.012479\teval-mlogloss:0.101396\n",
      "[1078]\ttrain-mlogloss:0.012455\teval-mlogloss:0.101384\n",
      "[1079]\ttrain-mlogloss:0.012436\teval-mlogloss:0.10137\n",
      "[1080]\ttrain-mlogloss:0.012417\teval-mlogloss:0.101357\n",
      "[1081]\ttrain-mlogloss:0.012395\teval-mlogloss:0.10136\n",
      "[1082]\ttrain-mlogloss:0.012375\teval-mlogloss:0.101335\n",
      "[1083]\ttrain-mlogloss:0.012354\teval-mlogloss:0.101317\n",
      "[1084]\ttrain-mlogloss:0.012333\teval-mlogloss:0.101299\n",
      "[1085]\ttrain-mlogloss:0.012314\teval-mlogloss:0.101285\n",
      "[1086]\ttrain-mlogloss:0.012294\teval-mlogloss:0.10127\n",
      "[1087]\ttrain-mlogloss:0.012273\teval-mlogloss:0.101242\n",
      "[1088]\ttrain-mlogloss:0.012253\teval-mlogloss:0.101227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1089]\ttrain-mlogloss:0.012234\teval-mlogloss:0.101214\n",
      "[1090]\ttrain-mlogloss:0.012213\teval-mlogloss:0.101217\n",
      "[1091]\ttrain-mlogloss:0.012195\teval-mlogloss:0.101218\n",
      "[1092]\ttrain-mlogloss:0.012174\teval-mlogloss:0.101196\n",
      "[1093]\ttrain-mlogloss:0.012154\teval-mlogloss:0.101183\n",
      "[1094]\ttrain-mlogloss:0.012135\teval-mlogloss:0.101165\n",
      "[1095]\ttrain-mlogloss:0.012115\teval-mlogloss:0.101145\n",
      "[1096]\ttrain-mlogloss:0.012096\teval-mlogloss:0.101118\n",
      "[1097]\ttrain-mlogloss:0.012077\teval-mlogloss:0.101098\n",
      "[1098]\ttrain-mlogloss:0.012056\teval-mlogloss:0.101081\n",
      "[1099]\ttrain-mlogloss:0.012037\teval-mlogloss:0.101052\n",
      "[1100]\ttrain-mlogloss:0.01202\teval-mlogloss:0.101034\n",
      "[1101]\ttrain-mlogloss:0.012001\teval-mlogloss:0.101011\n",
      "[1102]\ttrain-mlogloss:0.01198\teval-mlogloss:0.10102\n",
      "[1103]\ttrain-mlogloss:0.011961\teval-mlogloss:0.101005\n",
      "[1104]\ttrain-mlogloss:0.011941\teval-mlogloss:0.100991\n",
      "[1105]\ttrain-mlogloss:0.011922\teval-mlogloss:0.10097\n",
      "[1106]\ttrain-mlogloss:0.011904\teval-mlogloss:0.10094\n",
      "[1107]\ttrain-mlogloss:0.011887\teval-mlogloss:0.100931\n",
      "[1108]\ttrain-mlogloss:0.011866\teval-mlogloss:0.100911\n",
      "[1109]\ttrain-mlogloss:0.011849\teval-mlogloss:0.100895\n",
      "[1110]\ttrain-mlogloss:0.01183\teval-mlogloss:0.100899\n",
      "[1111]\ttrain-mlogloss:0.011812\teval-mlogloss:0.100892\n",
      "[1112]\ttrain-mlogloss:0.011794\teval-mlogloss:0.10088\n",
      "[1113]\ttrain-mlogloss:0.011775\teval-mlogloss:0.100864\n",
      "[1114]\ttrain-mlogloss:0.011757\teval-mlogloss:0.100852\n",
      "[1115]\ttrain-mlogloss:0.011739\teval-mlogloss:0.100818\n",
      "[1116]\ttrain-mlogloss:0.011719\teval-mlogloss:0.100824\n",
      "[1117]\ttrain-mlogloss:0.011701\teval-mlogloss:0.100817\n",
      "[1118]\ttrain-mlogloss:0.011683\teval-mlogloss:0.100794\n",
      "[1119]\ttrain-mlogloss:0.011664\teval-mlogloss:0.1008\n",
      "[1120]\ttrain-mlogloss:0.011645\teval-mlogloss:0.100782\n",
      "[1121]\ttrain-mlogloss:0.011628\teval-mlogloss:0.100777\n",
      "[1122]\ttrain-mlogloss:0.011609\teval-mlogloss:0.100754\n",
      "[1123]\ttrain-mlogloss:0.011591\teval-mlogloss:0.10073\n",
      "[1124]\ttrain-mlogloss:0.011575\teval-mlogloss:0.100725\n",
      "[1125]\ttrain-mlogloss:0.011558\teval-mlogloss:0.100709\n",
      "[1126]\ttrain-mlogloss:0.011541\teval-mlogloss:0.100692\n",
      "[1127]\ttrain-mlogloss:0.011523\teval-mlogloss:0.100702\n",
      "[1128]\ttrain-mlogloss:0.011507\teval-mlogloss:0.100676\n",
      "[1129]\ttrain-mlogloss:0.011487\teval-mlogloss:0.100655\n",
      "[1130]\ttrain-mlogloss:0.011471\teval-mlogloss:0.100651\n",
      "[1131]\ttrain-mlogloss:0.011453\teval-mlogloss:0.100636\n",
      "[1132]\ttrain-mlogloss:0.011437\teval-mlogloss:0.100622\n",
      "[1133]\ttrain-mlogloss:0.011419\teval-mlogloss:0.100604\n",
      "[1134]\ttrain-mlogloss:0.011401\teval-mlogloss:0.100586\n",
      "[1135]\ttrain-mlogloss:0.011385\teval-mlogloss:0.100567\n",
      "[1136]\ttrain-mlogloss:0.011368\teval-mlogloss:0.100552\n",
      "[1137]\ttrain-mlogloss:0.01135\teval-mlogloss:0.100541\n",
      "[1138]\ttrain-mlogloss:0.011333\teval-mlogloss:0.100538\n",
      "[1139]\ttrain-mlogloss:0.011315\teval-mlogloss:0.100523\n",
      "[1140]\ttrain-mlogloss:0.011298\teval-mlogloss:0.100498\n",
      "[1141]\ttrain-mlogloss:0.01128\teval-mlogloss:0.100498\n",
      "[1142]\ttrain-mlogloss:0.011265\teval-mlogloss:0.100493\n",
      "[1143]\ttrain-mlogloss:0.011247\teval-mlogloss:0.100482\n",
      "[1144]\ttrain-mlogloss:0.011232\teval-mlogloss:0.100456\n",
      "[1145]\ttrain-mlogloss:0.011215\teval-mlogloss:0.100427\n",
      "[1146]\ttrain-mlogloss:0.011198\teval-mlogloss:0.100412\n",
      "[1147]\ttrain-mlogloss:0.011182\teval-mlogloss:0.100397\n",
      "[1148]\ttrain-mlogloss:0.011165\teval-mlogloss:0.100385\n",
      "[1149]\ttrain-mlogloss:0.011147\teval-mlogloss:0.100389\n",
      "[1150]\ttrain-mlogloss:0.011131\teval-mlogloss:0.100386\n",
      "[1151]\ttrain-mlogloss:0.011114\teval-mlogloss:0.100369\n",
      "[1152]\ttrain-mlogloss:0.011099\teval-mlogloss:0.100332\n",
      "[1153]\ttrain-mlogloss:0.011081\teval-mlogloss:0.100325\n",
      "[1154]\ttrain-mlogloss:0.011064\teval-mlogloss:0.100314\n",
      "[1155]\ttrain-mlogloss:0.011049\teval-mlogloss:0.100294\n",
      "[1156]\ttrain-mlogloss:0.011034\teval-mlogloss:0.10028\n",
      "[1157]\ttrain-mlogloss:0.011017\teval-mlogloss:0.100277\n",
      "[1158]\ttrain-mlogloss:0.011001\teval-mlogloss:0.100261\n",
      "[1159]\ttrain-mlogloss:0.010986\teval-mlogloss:0.100232\n",
      "[1160]\ttrain-mlogloss:0.010968\teval-mlogloss:0.100241\n",
      "[1161]\ttrain-mlogloss:0.010954\teval-mlogloss:0.100225\n",
      "[1162]\ttrain-mlogloss:0.010937\teval-mlogloss:0.10019\n",
      "[1163]\ttrain-mlogloss:0.010921\teval-mlogloss:0.100181\n",
      "[1164]\ttrain-mlogloss:0.010905\teval-mlogloss:0.10017\n",
      "[1165]\ttrain-mlogloss:0.010888\teval-mlogloss:0.100163\n",
      "[1166]\ttrain-mlogloss:0.010871\teval-mlogloss:0.100143\n",
      "[1167]\ttrain-mlogloss:0.010857\teval-mlogloss:0.100128\n",
      "[1168]\ttrain-mlogloss:0.010841\teval-mlogloss:0.100112\n",
      "[1169]\ttrain-mlogloss:0.010826\teval-mlogloss:0.100104\n",
      "[1170]\ttrain-mlogloss:0.010811\teval-mlogloss:0.100092\n",
      "[1171]\ttrain-mlogloss:0.010795\teval-mlogloss:0.100074\n",
      "[1172]\ttrain-mlogloss:0.010778\teval-mlogloss:0.100072\n",
      "[1173]\ttrain-mlogloss:0.010763\teval-mlogloss:0.100037\n",
      "[1174]\ttrain-mlogloss:0.010747\teval-mlogloss:0.100037\n",
      "[1175]\ttrain-mlogloss:0.010732\teval-mlogloss:0.100029\n",
      "[1176]\ttrain-mlogloss:0.010718\teval-mlogloss:0.100001\n",
      "[1177]\ttrain-mlogloss:0.010701\teval-mlogloss:0.099992\n",
      "[1178]\ttrain-mlogloss:0.010685\teval-mlogloss:0.099971\n",
      "[1179]\ttrain-mlogloss:0.010671\teval-mlogloss:0.099962\n",
      "[1180]\ttrain-mlogloss:0.010655\teval-mlogloss:0.099961\n",
      "[1181]\ttrain-mlogloss:0.010641\teval-mlogloss:0.099934\n",
      "[1182]\ttrain-mlogloss:0.010624\teval-mlogloss:0.099923\n",
      "[1183]\ttrain-mlogloss:0.010609\teval-mlogloss:0.099908\n",
      "[1184]\ttrain-mlogloss:0.010595\teval-mlogloss:0.099888\n",
      "[1185]\ttrain-mlogloss:0.010578\teval-mlogloss:0.099879\n",
      "[1186]\ttrain-mlogloss:0.010563\teval-mlogloss:0.099854\n",
      "[1187]\ttrain-mlogloss:0.010549\teval-mlogloss:0.099825\n",
      "[1188]\ttrain-mlogloss:0.010533\teval-mlogloss:0.099799\n",
      "[1189]\ttrain-mlogloss:0.010519\teval-mlogloss:0.099768\n",
      "[1190]\ttrain-mlogloss:0.010505\teval-mlogloss:0.099757\n",
      "[1191]\ttrain-mlogloss:0.01049\teval-mlogloss:0.099749\n",
      "[1192]\ttrain-mlogloss:0.010475\teval-mlogloss:0.099748\n",
      "[1193]\ttrain-mlogloss:0.01046\teval-mlogloss:0.099737\n",
      "[1194]\ttrain-mlogloss:0.010445\teval-mlogloss:0.099743\n",
      "[1195]\ttrain-mlogloss:0.01043\teval-mlogloss:0.09973\n",
      "[1196]\ttrain-mlogloss:0.010416\teval-mlogloss:0.099729\n",
      "[1197]\ttrain-mlogloss:0.010401\teval-mlogloss:0.099717\n",
      "[1198]\ttrain-mlogloss:0.010386\teval-mlogloss:0.099692\n",
      "[1199]\ttrain-mlogloss:0.010371\teval-mlogloss:0.099681\n",
      "[1200]\ttrain-mlogloss:0.010356\teval-mlogloss:0.099666\n",
      "[1201]\ttrain-mlogloss:0.010341\teval-mlogloss:0.099643\n",
      "[1202]\ttrain-mlogloss:0.010326\teval-mlogloss:0.09964\n",
      "[1203]\ttrain-mlogloss:0.010311\teval-mlogloss:0.099634\n",
      "[1204]\ttrain-mlogloss:0.010297\teval-mlogloss:0.099628\n",
      "[1205]\ttrain-mlogloss:0.010283\teval-mlogloss:0.099616\n",
      "[1206]\ttrain-mlogloss:0.010268\teval-mlogloss:0.099618\n",
      "[1207]\ttrain-mlogloss:0.010253\teval-mlogloss:0.099592\n",
      "[1208]\ttrain-mlogloss:0.010238\teval-mlogloss:0.099576\n",
      "[1209]\ttrain-mlogloss:0.010224\teval-mlogloss:0.099572\n",
      "[1210]\ttrain-mlogloss:0.010209\teval-mlogloss:0.09957\n",
      "[1211]\ttrain-mlogloss:0.010194\teval-mlogloss:0.099549\n",
      "[1212]\ttrain-mlogloss:0.010181\teval-mlogloss:0.099542\n",
      "[1213]\ttrain-mlogloss:0.010166\teval-mlogloss:0.099528\n",
      "[1214]\ttrain-mlogloss:0.010151\teval-mlogloss:0.099505\n",
      "[1215]\ttrain-mlogloss:0.010136\teval-mlogloss:0.099494\n",
      "[1216]\ttrain-mlogloss:0.010123\teval-mlogloss:0.099484\n",
      "[1217]\ttrain-mlogloss:0.010108\teval-mlogloss:0.099477\n",
      "[1218]\ttrain-mlogloss:0.010093\teval-mlogloss:0.099476\n",
      "[1219]\ttrain-mlogloss:0.010079\teval-mlogloss:0.09945\n",
      "[1220]\ttrain-mlogloss:0.010065\teval-mlogloss:0.099441\n",
      "[1221]\ttrain-mlogloss:0.01005\teval-mlogloss:0.099436\n",
      "[1222]\ttrain-mlogloss:0.010036\teval-mlogloss:0.099432\n",
      "[1223]\ttrain-mlogloss:0.010023\teval-mlogloss:0.099426\n",
      "[1224]\ttrain-mlogloss:0.010008\teval-mlogloss:0.099424\n",
      "[1225]\ttrain-mlogloss:0.009996\teval-mlogloss:0.099405\n",
      "[1226]\ttrain-mlogloss:0.009981\teval-mlogloss:0.099387\n",
      "[1227]\ttrain-mlogloss:0.009967\teval-mlogloss:0.099398\n",
      "[1228]\ttrain-mlogloss:0.009953\teval-mlogloss:0.099399\n",
      "[1229]\ttrain-mlogloss:0.009941\teval-mlogloss:0.099386\n",
      "[1230]\ttrain-mlogloss:0.009927\teval-mlogloss:0.099373\n",
      "[1231]\ttrain-mlogloss:0.009913\teval-mlogloss:0.099363\n",
      "[1232]\ttrain-mlogloss:0.009899\teval-mlogloss:0.099343\n",
      "[1233]\ttrain-mlogloss:0.009886\teval-mlogloss:0.099332\n",
      "[1234]\ttrain-mlogloss:0.009873\teval-mlogloss:0.099329\n",
      "[1235]\ttrain-mlogloss:0.009859\teval-mlogloss:0.099337\n",
      "[1236]\ttrain-mlogloss:0.009846\teval-mlogloss:0.099316\n",
      "[1237]\ttrain-mlogloss:0.009833\teval-mlogloss:0.099298\n",
      "[1238]\ttrain-mlogloss:0.00982\teval-mlogloss:0.099289\n",
      "[1239]\ttrain-mlogloss:0.009806\teval-mlogloss:0.099286\n",
      "[1240]\ttrain-mlogloss:0.009794\teval-mlogloss:0.099271\n",
      "[1241]\ttrain-mlogloss:0.009781\teval-mlogloss:0.099244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1242]\ttrain-mlogloss:0.009767\teval-mlogloss:0.099241\n",
      "[1243]\ttrain-mlogloss:0.009754\teval-mlogloss:0.099223\n",
      "[1244]\ttrain-mlogloss:0.009741\teval-mlogloss:0.099221\n",
      "[1245]\ttrain-mlogloss:0.009728\teval-mlogloss:0.099203\n",
      "[1246]\ttrain-mlogloss:0.009716\teval-mlogloss:0.099189\n",
      "[1247]\ttrain-mlogloss:0.009703\teval-mlogloss:0.099174\n",
      "[1248]\ttrain-mlogloss:0.00969\teval-mlogloss:0.09917\n",
      "[1249]\ttrain-mlogloss:0.009677\teval-mlogloss:0.099155\n",
      "[1250]\ttrain-mlogloss:0.009664\teval-mlogloss:0.099148\n",
      "[1251]\ttrain-mlogloss:0.009651\teval-mlogloss:0.099141\n",
      "[1252]\ttrain-mlogloss:0.009639\teval-mlogloss:0.099122\n",
      "[1253]\ttrain-mlogloss:0.009626\teval-mlogloss:0.099114\n",
      "[1254]\ttrain-mlogloss:0.009615\teval-mlogloss:0.099103\n",
      "[1255]\ttrain-mlogloss:0.009603\teval-mlogloss:0.099084\n",
      "[1256]\ttrain-mlogloss:0.00959\teval-mlogloss:0.099079\n",
      "[1257]\ttrain-mlogloss:0.009579\teval-mlogloss:0.099066\n",
      "[1258]\ttrain-mlogloss:0.009565\teval-mlogloss:0.099057\n",
      "[1259]\ttrain-mlogloss:0.009552\teval-mlogloss:0.099032\n",
      "[1260]\ttrain-mlogloss:0.009539\teval-mlogloss:0.09902\n",
      "[1261]\ttrain-mlogloss:0.009528\teval-mlogloss:0.099012\n",
      "[1262]\ttrain-mlogloss:0.009516\teval-mlogloss:0.098997\n",
      "[1263]\ttrain-mlogloss:0.009503\teval-mlogloss:0.09898\n",
      "[1264]\ttrain-mlogloss:0.009491\teval-mlogloss:0.098976\n",
      "[1265]\ttrain-mlogloss:0.009479\teval-mlogloss:0.098965\n",
      "[1266]\ttrain-mlogloss:0.009466\teval-mlogloss:0.098958\n",
      "[1267]\ttrain-mlogloss:0.009453\teval-mlogloss:0.098941\n",
      "[1268]\ttrain-mlogloss:0.00944\teval-mlogloss:0.098938\n",
      "[1269]\ttrain-mlogloss:0.009428\teval-mlogloss:0.098924\n",
      "[1270]\ttrain-mlogloss:0.009416\teval-mlogloss:0.098917\n",
      "[1271]\ttrain-mlogloss:0.009403\teval-mlogloss:0.0989\n",
      "[1272]\ttrain-mlogloss:0.00939\teval-mlogloss:0.098893\n",
      "[1273]\ttrain-mlogloss:0.009379\teval-mlogloss:0.098884\n",
      "[1274]\ttrain-mlogloss:0.009367\teval-mlogloss:0.09887\n",
      "[1275]\ttrain-mlogloss:0.009355\teval-mlogloss:0.098861\n",
      "[1276]\ttrain-mlogloss:0.009343\teval-mlogloss:0.098861\n",
      "[1277]\ttrain-mlogloss:0.00933\teval-mlogloss:0.098866\n",
      "[1278]\ttrain-mlogloss:0.009318\teval-mlogloss:0.098862\n",
      "[1279]\ttrain-mlogloss:0.009308\teval-mlogloss:0.09885\n",
      "[1280]\ttrain-mlogloss:0.009296\teval-mlogloss:0.098849\n",
      "[1281]\ttrain-mlogloss:0.009283\teval-mlogloss:0.098823\n",
      "[1282]\ttrain-mlogloss:0.009272\teval-mlogloss:0.09882\n",
      "[1283]\ttrain-mlogloss:0.009261\teval-mlogloss:0.098822\n",
      "[1284]\ttrain-mlogloss:0.00925\teval-mlogloss:0.098806\n",
      "[1285]\ttrain-mlogloss:0.009237\teval-mlogloss:0.098803\n",
      "[1286]\ttrain-mlogloss:0.009225\teval-mlogloss:0.098787\n",
      "[1287]\ttrain-mlogloss:0.009214\teval-mlogloss:0.098768\n",
      "[1288]\ttrain-mlogloss:0.009203\teval-mlogloss:0.098765\n",
      "[1289]\ttrain-mlogloss:0.00919\teval-mlogloss:0.098754\n",
      "[1290]\ttrain-mlogloss:0.009179\teval-mlogloss:0.098737\n",
      "[1291]\ttrain-mlogloss:0.009167\teval-mlogloss:0.098738\n",
      "[1292]\ttrain-mlogloss:0.009155\teval-mlogloss:0.098748\n",
      "[1293]\ttrain-mlogloss:0.009145\teval-mlogloss:0.098747\n",
      "[1294]\ttrain-mlogloss:0.009134\teval-mlogloss:0.098745\n",
      "[1295]\ttrain-mlogloss:0.009124\teval-mlogloss:0.098726\n",
      "[1296]\ttrain-mlogloss:0.009112\teval-mlogloss:0.098712\n",
      "[1297]\ttrain-mlogloss:0.0091\teval-mlogloss:0.098684\n",
      "[1298]\ttrain-mlogloss:0.00909\teval-mlogloss:0.098688\n",
      "[1299]\ttrain-mlogloss:0.009077\teval-mlogloss:0.098687\n",
      "[1300]\ttrain-mlogloss:0.009068\teval-mlogloss:0.098684\n",
      "[1301]\ttrain-mlogloss:0.009056\teval-mlogloss:0.098677\n",
      "[1302]\ttrain-mlogloss:0.009044\teval-mlogloss:0.098679\n",
      "[1303]\ttrain-mlogloss:0.009034\teval-mlogloss:0.098673\n",
      "[1304]\ttrain-mlogloss:0.009023\teval-mlogloss:0.098675\n",
      "[1305]\ttrain-mlogloss:0.009012\teval-mlogloss:0.098662\n",
      "[1306]\ttrain-mlogloss:0.009001\teval-mlogloss:0.098661\n",
      "[1307]\ttrain-mlogloss:0.008989\teval-mlogloss:0.09866\n",
      "[1308]\ttrain-mlogloss:0.008979\teval-mlogloss:0.098652\n",
      "[1309]\ttrain-mlogloss:0.008967\teval-mlogloss:0.09865\n",
      "[1310]\ttrain-mlogloss:0.008956\teval-mlogloss:0.098624\n",
      "[1311]\ttrain-mlogloss:0.008945\teval-mlogloss:0.098606\n",
      "[1312]\ttrain-mlogloss:0.008933\teval-mlogloss:0.098596\n",
      "[1313]\ttrain-mlogloss:0.008923\teval-mlogloss:0.098589\n",
      "[1314]\ttrain-mlogloss:0.008913\teval-mlogloss:0.098591\n",
      "[1315]\ttrain-mlogloss:0.008902\teval-mlogloss:0.098594\n",
      "[1316]\ttrain-mlogloss:0.008892\teval-mlogloss:0.098589\n",
      "[1317]\ttrain-mlogloss:0.008881\teval-mlogloss:0.098578\n",
      "[1318]\ttrain-mlogloss:0.008871\teval-mlogloss:0.098563\n",
      "[1319]\ttrain-mlogloss:0.008862\teval-mlogloss:0.098558\n",
      "[1320]\ttrain-mlogloss:0.008852\teval-mlogloss:0.098556\n",
      "[1321]\ttrain-mlogloss:0.008841\teval-mlogloss:0.098534\n",
      "[1322]\ttrain-mlogloss:0.00883\teval-mlogloss:0.098529\n",
      "[1323]\ttrain-mlogloss:0.00882\teval-mlogloss:0.098529\n",
      "[1324]\ttrain-mlogloss:0.00881\teval-mlogloss:0.098526\n",
      "[1325]\ttrain-mlogloss:0.008799\teval-mlogloss:0.098527\n",
      "[1326]\ttrain-mlogloss:0.008788\teval-mlogloss:0.098525\n",
      "[1327]\ttrain-mlogloss:0.008779\teval-mlogloss:0.098533\n",
      "[1328]\ttrain-mlogloss:0.008769\teval-mlogloss:0.098533\n",
      "[1329]\ttrain-mlogloss:0.00876\teval-mlogloss:0.098527\n",
      "[1330]\ttrain-mlogloss:0.008749\teval-mlogloss:0.098507\n",
      "[1331]\ttrain-mlogloss:0.008738\teval-mlogloss:0.098492\n",
      "[1332]\ttrain-mlogloss:0.008727\teval-mlogloss:0.098491\n",
      "[1333]\ttrain-mlogloss:0.008717\teval-mlogloss:0.098481\n",
      "[1334]\ttrain-mlogloss:0.008706\teval-mlogloss:0.098485\n",
      "[1335]\ttrain-mlogloss:0.008695\teval-mlogloss:0.098495\n",
      "[1336]\ttrain-mlogloss:0.008684\teval-mlogloss:0.098478\n",
      "[1337]\ttrain-mlogloss:0.008676\teval-mlogloss:0.098482\n",
      "[1338]\ttrain-mlogloss:0.008667\teval-mlogloss:0.098462\n",
      "[1339]\ttrain-mlogloss:0.008656\teval-mlogloss:0.098464\n",
      "[1340]\ttrain-mlogloss:0.008646\teval-mlogloss:0.098454\n",
      "[1341]\ttrain-mlogloss:0.008637\teval-mlogloss:0.098442\n",
      "[1342]\ttrain-mlogloss:0.008628\teval-mlogloss:0.09844\n",
      "[1343]\ttrain-mlogloss:0.008618\teval-mlogloss:0.098441\n",
      "[1344]\ttrain-mlogloss:0.008608\teval-mlogloss:0.098443\n",
      "[1345]\ttrain-mlogloss:0.008598\teval-mlogloss:0.098434\n",
      "[1346]\ttrain-mlogloss:0.008588\teval-mlogloss:0.098409\n",
      "[1347]\ttrain-mlogloss:0.008577\teval-mlogloss:0.098392\n",
      "[1348]\ttrain-mlogloss:0.008569\teval-mlogloss:0.098392\n",
      "[1349]\ttrain-mlogloss:0.00856\teval-mlogloss:0.098387\n",
      "[1350]\ttrain-mlogloss:0.008552\teval-mlogloss:0.098382\n",
      "[1351]\ttrain-mlogloss:0.008542\teval-mlogloss:0.098364\n",
      "[1352]\ttrain-mlogloss:0.008532\teval-mlogloss:0.098363\n",
      "[1353]\ttrain-mlogloss:0.008522\teval-mlogloss:0.098353\n",
      "[1354]\ttrain-mlogloss:0.008513\teval-mlogloss:0.098348\n",
      "[1355]\ttrain-mlogloss:0.008503\teval-mlogloss:0.098357\n",
      "[1356]\ttrain-mlogloss:0.008494\teval-mlogloss:0.098341\n",
      "[1357]\ttrain-mlogloss:0.008485\teval-mlogloss:0.098348\n",
      "[1358]\ttrain-mlogloss:0.008474\teval-mlogloss:0.098341\n",
      "[1359]\ttrain-mlogloss:0.008463\teval-mlogloss:0.098347\n",
      "[1360]\ttrain-mlogloss:0.008455\teval-mlogloss:0.098353\n",
      "[1361]\ttrain-mlogloss:0.008446\teval-mlogloss:0.098351\n",
      "[1362]\ttrain-mlogloss:0.008437\teval-mlogloss:0.098341\n",
      "[1363]\ttrain-mlogloss:0.008429\teval-mlogloss:0.098333\n",
      "[1364]\ttrain-mlogloss:0.008418\teval-mlogloss:0.09833\n",
      "[1365]\ttrain-mlogloss:0.00841\teval-mlogloss:0.098316\n",
      "[1366]\ttrain-mlogloss:0.0084\teval-mlogloss:0.09828\n",
      "[1367]\ttrain-mlogloss:0.008391\teval-mlogloss:0.098282\n",
      "[1368]\ttrain-mlogloss:0.008383\teval-mlogloss:0.098273\n",
      "[1369]\ttrain-mlogloss:0.008373\teval-mlogloss:0.098262\n",
      "[1370]\ttrain-mlogloss:0.008365\teval-mlogloss:0.098253\n",
      "[1371]\ttrain-mlogloss:0.008355\teval-mlogloss:0.098259\n",
      "[1372]\ttrain-mlogloss:0.008347\teval-mlogloss:0.098255\n",
      "[1373]\ttrain-mlogloss:0.008338\teval-mlogloss:0.098262\n",
      "[1374]\ttrain-mlogloss:0.008327\teval-mlogloss:0.098244\n",
      "[1375]\ttrain-mlogloss:0.008318\teval-mlogloss:0.098241\n",
      "[1376]\ttrain-mlogloss:0.008309\teval-mlogloss:0.098226\n",
      "[1377]\ttrain-mlogloss:0.0083\teval-mlogloss:0.098228\n",
      "[1378]\ttrain-mlogloss:0.008291\teval-mlogloss:0.098223\n",
      "[1379]\ttrain-mlogloss:0.008281\teval-mlogloss:0.09823\n",
      "[1380]\ttrain-mlogloss:0.008274\teval-mlogloss:0.098234\n",
      "[1381]\ttrain-mlogloss:0.008263\teval-mlogloss:0.098216\n",
      "[1382]\ttrain-mlogloss:0.008255\teval-mlogloss:0.098194\n",
      "[1383]\ttrain-mlogloss:0.008246\teval-mlogloss:0.098204\n",
      "[1384]\ttrain-mlogloss:0.008237\teval-mlogloss:0.098195\n",
      "[1385]\ttrain-mlogloss:0.008228\teval-mlogloss:0.098194\n",
      "[1386]\ttrain-mlogloss:0.008219\teval-mlogloss:0.0982\n",
      "[1387]\ttrain-mlogloss:0.008211\teval-mlogloss:0.098204\n",
      "[1388]\ttrain-mlogloss:0.008201\teval-mlogloss:0.0982\n",
      "[1389]\ttrain-mlogloss:0.008194\teval-mlogloss:0.098192\n",
      "[1390]\ttrain-mlogloss:0.008184\teval-mlogloss:0.098183\n",
      "[1391]\ttrain-mlogloss:0.008175\teval-mlogloss:0.098189\n",
      "[1392]\ttrain-mlogloss:0.008166\teval-mlogloss:0.098192\n",
      "[1393]\ttrain-mlogloss:0.008158\teval-mlogloss:0.098175\n",
      "[1394]\ttrain-mlogloss:0.008148\teval-mlogloss:0.098171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1395]\ttrain-mlogloss:0.008141\teval-mlogloss:0.098173\n",
      "[1396]\ttrain-mlogloss:0.008132\teval-mlogloss:0.098183\n",
      "[1397]\ttrain-mlogloss:0.008122\teval-mlogloss:0.098174\n",
      "[1398]\ttrain-mlogloss:0.008113\teval-mlogloss:0.098165\n",
      "[1399]\ttrain-mlogloss:0.008105\teval-mlogloss:0.098162\n",
      "[1400]\ttrain-mlogloss:0.008097\teval-mlogloss:0.098146\n",
      "[1401]\ttrain-mlogloss:0.008087\teval-mlogloss:0.098153\n",
      "[1402]\ttrain-mlogloss:0.008079\teval-mlogloss:0.098154\n",
      "[1403]\ttrain-mlogloss:0.008072\teval-mlogloss:0.098159\n",
      "[1404]\ttrain-mlogloss:0.008062\teval-mlogloss:0.098155\n",
      "[1405]\ttrain-mlogloss:0.008054\teval-mlogloss:0.09815\n",
      "[1406]\ttrain-mlogloss:0.008046\teval-mlogloss:0.098156\n",
      "[1407]\ttrain-mlogloss:0.008037\teval-mlogloss:0.098159\n",
      "[1408]\ttrain-mlogloss:0.008028\teval-mlogloss:0.098144\n",
      "[1409]\ttrain-mlogloss:0.008021\teval-mlogloss:0.098127\n",
      "[1410]\ttrain-mlogloss:0.008012\teval-mlogloss:0.098121\n",
      "[1411]\ttrain-mlogloss:0.008005\teval-mlogloss:0.098124\n",
      "[1412]\ttrain-mlogloss:0.007996\teval-mlogloss:0.098106\n",
      "[1413]\ttrain-mlogloss:0.007988\teval-mlogloss:0.098115\n",
      "[1414]\ttrain-mlogloss:0.007979\teval-mlogloss:0.09811\n",
      "[1415]\ttrain-mlogloss:0.00797\teval-mlogloss:0.098109\n",
      "[1416]\ttrain-mlogloss:0.007962\teval-mlogloss:0.098108\n",
      "[1417]\ttrain-mlogloss:0.007953\teval-mlogloss:0.098101\n",
      "[1418]\ttrain-mlogloss:0.007946\teval-mlogloss:0.098097\n",
      "[1419]\ttrain-mlogloss:0.007936\teval-mlogloss:0.09809\n",
      "[1420]\ttrain-mlogloss:0.007929\teval-mlogloss:0.098097\n",
      "[1421]\ttrain-mlogloss:0.007921\teval-mlogloss:0.098084\n",
      "[1422]\ttrain-mlogloss:0.007912\teval-mlogloss:0.098085\n",
      "[1423]\ttrain-mlogloss:0.007903\teval-mlogloss:0.098077\n",
      "[1424]\ttrain-mlogloss:0.007893\teval-mlogloss:0.098075\n",
      "[1425]\ttrain-mlogloss:0.007885\teval-mlogloss:0.09807\n",
      "[1426]\ttrain-mlogloss:0.007876\teval-mlogloss:0.098071\n",
      "[1427]\ttrain-mlogloss:0.007869\teval-mlogloss:0.098055\n",
      "[1428]\ttrain-mlogloss:0.00786\teval-mlogloss:0.098051\n",
      "[1429]\ttrain-mlogloss:0.007852\teval-mlogloss:0.098049\n",
      "[1430]\ttrain-mlogloss:0.007844\teval-mlogloss:0.098047\n",
      "[1431]\ttrain-mlogloss:0.007837\teval-mlogloss:0.098046\n",
      "[1432]\ttrain-mlogloss:0.007828\teval-mlogloss:0.098035\n",
      "[1433]\ttrain-mlogloss:0.00782\teval-mlogloss:0.09804\n",
      "[1434]\ttrain-mlogloss:0.007811\teval-mlogloss:0.098053\n",
      "[1435]\ttrain-mlogloss:0.007802\teval-mlogloss:0.098058\n",
      "[1436]\ttrain-mlogloss:0.007794\teval-mlogloss:0.098056\n",
      "[1437]\ttrain-mlogloss:0.007787\teval-mlogloss:0.098042\n",
      "[1438]\ttrain-mlogloss:0.007778\teval-mlogloss:0.098036\n",
      "[1439]\ttrain-mlogloss:0.007769\teval-mlogloss:0.098033\n",
      "[1440]\ttrain-mlogloss:0.007762\teval-mlogloss:0.098032\n",
      "[1441]\ttrain-mlogloss:0.007754\teval-mlogloss:0.098033\n",
      "[1442]\ttrain-mlogloss:0.007745\teval-mlogloss:0.09803\n",
      "[1443]\ttrain-mlogloss:0.007738\teval-mlogloss:0.098033\n",
      "[1444]\ttrain-mlogloss:0.007729\teval-mlogloss:0.098026\n",
      "[1445]\ttrain-mlogloss:0.007722\teval-mlogloss:0.098024\n",
      "[1446]\ttrain-mlogloss:0.007714\teval-mlogloss:0.098034\n",
      "[1447]\ttrain-mlogloss:0.007705\teval-mlogloss:0.09803\n",
      "[1448]\ttrain-mlogloss:0.007698\teval-mlogloss:0.098018\n",
      "[1449]\ttrain-mlogloss:0.007688\teval-mlogloss:0.098024\n",
      "[1450]\ttrain-mlogloss:0.007681\teval-mlogloss:0.098018\n",
      "[1451]\ttrain-mlogloss:0.007673\teval-mlogloss:0.098023\n",
      "[1452]\ttrain-mlogloss:0.007665\teval-mlogloss:0.098027\n",
      "[1453]\ttrain-mlogloss:0.007656\teval-mlogloss:0.098021\n",
      "[1454]\ttrain-mlogloss:0.00765\teval-mlogloss:0.098015\n",
      "[1455]\ttrain-mlogloss:0.007642\teval-mlogloss:0.098005\n",
      "[1456]\ttrain-mlogloss:0.007633\teval-mlogloss:0.097989\n",
      "[1457]\ttrain-mlogloss:0.007626\teval-mlogloss:0.097978\n",
      "[1458]\ttrain-mlogloss:0.007618\teval-mlogloss:0.097984\n",
      "[1459]\ttrain-mlogloss:0.00761\teval-mlogloss:0.097988\n",
      "[1460]\ttrain-mlogloss:0.007603\teval-mlogloss:0.09798\n",
      "[1461]\ttrain-mlogloss:0.007594\teval-mlogloss:0.097982\n",
      "[1462]\ttrain-mlogloss:0.007586\teval-mlogloss:0.097973\n",
      "[1463]\ttrain-mlogloss:0.007577\teval-mlogloss:0.097961\n",
      "[1464]\ttrain-mlogloss:0.00757\teval-mlogloss:0.09796\n",
      "[1465]\ttrain-mlogloss:0.007563\teval-mlogloss:0.097958\n",
      "[1466]\ttrain-mlogloss:0.007556\teval-mlogloss:0.097954\n",
      "[1467]\ttrain-mlogloss:0.007547\teval-mlogloss:0.097963\n",
      "[1468]\ttrain-mlogloss:0.00754\teval-mlogloss:0.097937\n",
      "[1469]\ttrain-mlogloss:0.007532\teval-mlogloss:0.097955\n",
      "[1470]\ttrain-mlogloss:0.007524\teval-mlogloss:0.097964\n",
      "[1471]\ttrain-mlogloss:0.007517\teval-mlogloss:0.097949\n",
      "[1472]\ttrain-mlogloss:0.00751\teval-mlogloss:0.097949\n",
      "[1473]\ttrain-mlogloss:0.007502\teval-mlogloss:0.097958\n",
      "[1474]\ttrain-mlogloss:0.007494\teval-mlogloss:0.097941\n",
      "[1475]\ttrain-mlogloss:0.007487\teval-mlogloss:0.09794\n",
      "[1476]\ttrain-mlogloss:0.007478\teval-mlogloss:0.097923\n",
      "[1477]\ttrain-mlogloss:0.007471\teval-mlogloss:0.09794\n",
      "[1478]\ttrain-mlogloss:0.007462\teval-mlogloss:0.097921\n",
      "[1479]\ttrain-mlogloss:0.007455\teval-mlogloss:0.09791\n",
      "[1480]\ttrain-mlogloss:0.007448\teval-mlogloss:0.097915\n",
      "[1481]\ttrain-mlogloss:0.00744\teval-mlogloss:0.097909\n",
      "[1482]\ttrain-mlogloss:0.007433\teval-mlogloss:0.097894\n",
      "[1483]\ttrain-mlogloss:0.007425\teval-mlogloss:0.097891\n",
      "[1484]\ttrain-mlogloss:0.007418\teval-mlogloss:0.097894\n",
      "[1485]\ttrain-mlogloss:0.007409\teval-mlogloss:0.097885\n",
      "[1486]\ttrain-mlogloss:0.007403\teval-mlogloss:0.097873\n",
      "[1487]\ttrain-mlogloss:0.007395\teval-mlogloss:0.097877\n",
      "[1488]\ttrain-mlogloss:0.007388\teval-mlogloss:0.097881\n",
      "[1489]\ttrain-mlogloss:0.007381\teval-mlogloss:0.097866\n",
      "[1490]\ttrain-mlogloss:0.007373\teval-mlogloss:0.097862\n",
      "[1491]\ttrain-mlogloss:0.007365\teval-mlogloss:0.097863\n",
      "[1492]\ttrain-mlogloss:0.007358\teval-mlogloss:0.09786\n",
      "[1493]\ttrain-mlogloss:0.007351\teval-mlogloss:0.097843\n",
      "[1494]\ttrain-mlogloss:0.007344\teval-mlogloss:0.097842\n",
      "[1495]\ttrain-mlogloss:0.007337\teval-mlogloss:0.097839\n",
      "[1496]\ttrain-mlogloss:0.00733\teval-mlogloss:0.097831\n",
      "[1497]\ttrain-mlogloss:0.007323\teval-mlogloss:0.097815\n",
      "[1498]\ttrain-mlogloss:0.007315\teval-mlogloss:0.097801\n",
      "[1499]\ttrain-mlogloss:0.007308\teval-mlogloss:0.097796\n",
      "[1500]\ttrain-mlogloss:0.007301\teval-mlogloss:0.097791\n",
      "[1501]\ttrain-mlogloss:0.007293\teval-mlogloss:0.097769\n",
      "[1502]\ttrain-mlogloss:0.007286\teval-mlogloss:0.097771\n",
      "[1503]\ttrain-mlogloss:0.007279\teval-mlogloss:0.097765\n",
      "[1504]\ttrain-mlogloss:0.007272\teval-mlogloss:0.097752\n",
      "[1505]\ttrain-mlogloss:0.007266\teval-mlogloss:0.097749\n",
      "[1506]\ttrain-mlogloss:0.007258\teval-mlogloss:0.097728\n",
      "[1507]\ttrain-mlogloss:0.007252\teval-mlogloss:0.09772\n",
      "[1508]\ttrain-mlogloss:0.007245\teval-mlogloss:0.097719\n",
      "[1509]\ttrain-mlogloss:0.007237\teval-mlogloss:0.097709\n",
      "[1510]\ttrain-mlogloss:0.00723\teval-mlogloss:0.097714\n",
      "[1511]\ttrain-mlogloss:0.007222\teval-mlogloss:0.097683\n",
      "[1512]\ttrain-mlogloss:0.007216\teval-mlogloss:0.097661\n",
      "[1513]\ttrain-mlogloss:0.007209\teval-mlogloss:0.097673\n",
      "[1514]\ttrain-mlogloss:0.007201\teval-mlogloss:0.09766\n",
      "[1515]\ttrain-mlogloss:0.007195\teval-mlogloss:0.097662\n",
      "[1516]\ttrain-mlogloss:0.007189\teval-mlogloss:0.097638\n",
      "[1517]\ttrain-mlogloss:0.007181\teval-mlogloss:0.097604\n",
      "[1518]\ttrain-mlogloss:0.007174\teval-mlogloss:0.097618\n",
      "[1519]\ttrain-mlogloss:0.007166\teval-mlogloss:0.097602\n",
      "[1520]\ttrain-mlogloss:0.00716\teval-mlogloss:0.097578\n",
      "[1521]\ttrain-mlogloss:0.007153\teval-mlogloss:0.097567\n",
      "[1522]\ttrain-mlogloss:0.007146\teval-mlogloss:0.097558\n",
      "[1523]\ttrain-mlogloss:0.00714\teval-mlogloss:0.09755\n",
      "[1524]\ttrain-mlogloss:0.007133\teval-mlogloss:0.097551\n",
      "[1525]\ttrain-mlogloss:0.007126\teval-mlogloss:0.097553\n",
      "[1526]\ttrain-mlogloss:0.00712\teval-mlogloss:0.097542\n",
      "[1527]\ttrain-mlogloss:0.007113\teval-mlogloss:0.097528\n",
      "[1528]\ttrain-mlogloss:0.007106\teval-mlogloss:0.097547\n",
      "[1529]\ttrain-mlogloss:0.0071\teval-mlogloss:0.097522\n",
      "[1530]\ttrain-mlogloss:0.007093\teval-mlogloss:0.097531\n",
      "[1531]\ttrain-mlogloss:0.007087\teval-mlogloss:0.097518\n",
      "[1532]\ttrain-mlogloss:0.00708\teval-mlogloss:0.097501\n",
      "[1533]\ttrain-mlogloss:0.007073\teval-mlogloss:0.097512\n",
      "[1534]\ttrain-mlogloss:0.007067\teval-mlogloss:0.097484\n",
      "[1535]\ttrain-mlogloss:0.007061\teval-mlogloss:0.097478\n",
      "[1536]\ttrain-mlogloss:0.007054\teval-mlogloss:0.09746\n",
      "[1537]\ttrain-mlogloss:0.007048\teval-mlogloss:0.097441\n",
      "[1538]\ttrain-mlogloss:0.007042\teval-mlogloss:0.09744\n",
      "[1539]\ttrain-mlogloss:0.007036\teval-mlogloss:0.097417\n",
      "[1540]\ttrain-mlogloss:0.007029\teval-mlogloss:0.097425\n",
      "[1541]\ttrain-mlogloss:0.007023\teval-mlogloss:0.097419\n",
      "[1542]\ttrain-mlogloss:0.007017\teval-mlogloss:0.097422\n",
      "[1543]\ttrain-mlogloss:0.007011\teval-mlogloss:0.097423\n",
      "[1544]\ttrain-mlogloss:0.007005\teval-mlogloss:0.097417\n",
      "[1545]\ttrain-mlogloss:0.006999\teval-mlogloss:0.097394\n",
      "[1546]\ttrain-mlogloss:0.006992\teval-mlogloss:0.097388\n",
      "[1547]\ttrain-mlogloss:0.006986\teval-mlogloss:0.097378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1548]\ttrain-mlogloss:0.00698\teval-mlogloss:0.097375\n",
      "[1549]\ttrain-mlogloss:0.006973\teval-mlogloss:0.097389\n",
      "[1550]\ttrain-mlogloss:0.006967\teval-mlogloss:0.097372\n",
      "[1551]\ttrain-mlogloss:0.006961\teval-mlogloss:0.097369\n",
      "[1552]\ttrain-mlogloss:0.006956\teval-mlogloss:0.097358\n",
      "[1553]\ttrain-mlogloss:0.006949\teval-mlogloss:0.097359\n",
      "[1554]\ttrain-mlogloss:0.006943\teval-mlogloss:0.097345\n",
      "[1555]\ttrain-mlogloss:0.006937\teval-mlogloss:0.097345\n",
      "[1556]\ttrain-mlogloss:0.00693\teval-mlogloss:0.097341\n",
      "[1557]\ttrain-mlogloss:0.006924\teval-mlogloss:0.097341\n",
      "[1558]\ttrain-mlogloss:0.006917\teval-mlogloss:0.097341\n",
      "[1559]\ttrain-mlogloss:0.006911\teval-mlogloss:0.097323\n",
      "[1560]\ttrain-mlogloss:0.006905\teval-mlogloss:0.097318\n",
      "[1561]\ttrain-mlogloss:0.006898\teval-mlogloss:0.097304\n",
      "[1562]\ttrain-mlogloss:0.006891\teval-mlogloss:0.097302\n",
      "[1563]\ttrain-mlogloss:0.006884\teval-mlogloss:0.097297\n",
      "[1564]\ttrain-mlogloss:0.006878\teval-mlogloss:0.097299\n",
      "[1565]\ttrain-mlogloss:0.006871\teval-mlogloss:0.097299\n",
      "[1566]\ttrain-mlogloss:0.006865\teval-mlogloss:0.097303\n",
      "[1567]\ttrain-mlogloss:0.006859\teval-mlogloss:0.097291\n",
      "[1568]\ttrain-mlogloss:0.006853\teval-mlogloss:0.09729\n",
      "[1569]\ttrain-mlogloss:0.006846\teval-mlogloss:0.097283\n",
      "[1570]\ttrain-mlogloss:0.00684\teval-mlogloss:0.097272\n",
      "[1571]\ttrain-mlogloss:0.006834\teval-mlogloss:0.097264\n",
      "[1572]\ttrain-mlogloss:0.006827\teval-mlogloss:0.097242\n",
      "[1573]\ttrain-mlogloss:0.006821\teval-mlogloss:0.097252\n",
      "[1574]\ttrain-mlogloss:0.006815\teval-mlogloss:0.097226\n",
      "[1575]\ttrain-mlogloss:0.006809\teval-mlogloss:0.097219\n",
      "[1576]\ttrain-mlogloss:0.006803\teval-mlogloss:0.097219\n",
      "[1577]\ttrain-mlogloss:0.006797\teval-mlogloss:0.097207\n",
      "[1578]\ttrain-mlogloss:0.00679\teval-mlogloss:0.097203\n",
      "[1579]\ttrain-mlogloss:0.006784\teval-mlogloss:0.097203\n",
      "[1580]\ttrain-mlogloss:0.006778\teval-mlogloss:0.097187\n",
      "[1581]\ttrain-mlogloss:0.006772\teval-mlogloss:0.097178\n",
      "[1582]\ttrain-mlogloss:0.006767\teval-mlogloss:0.097178\n",
      "[1583]\ttrain-mlogloss:0.00676\teval-mlogloss:0.097159\n",
      "[1584]\ttrain-mlogloss:0.006754\teval-mlogloss:0.097165\n",
      "[1585]\ttrain-mlogloss:0.006748\teval-mlogloss:0.097144\n",
      "[1586]\ttrain-mlogloss:0.006742\teval-mlogloss:0.097127\n",
      "[1587]\ttrain-mlogloss:0.006736\teval-mlogloss:0.09713\n",
      "[1588]\ttrain-mlogloss:0.006729\teval-mlogloss:0.097109\n",
      "[1589]\ttrain-mlogloss:0.006723\teval-mlogloss:0.097122\n",
      "[1590]\ttrain-mlogloss:0.006718\teval-mlogloss:0.097102\n",
      "[1591]\ttrain-mlogloss:0.006712\teval-mlogloss:0.097114\n",
      "[1592]\ttrain-mlogloss:0.006707\teval-mlogloss:0.097098\n",
      "[1593]\ttrain-mlogloss:0.006702\teval-mlogloss:0.097101\n",
      "[1594]\ttrain-mlogloss:0.006696\teval-mlogloss:0.097081\n",
      "[1595]\ttrain-mlogloss:0.006691\teval-mlogloss:0.097086\n",
      "[1596]\ttrain-mlogloss:0.006684\teval-mlogloss:0.09707\n",
      "[1597]\ttrain-mlogloss:0.006678\teval-mlogloss:0.097056\n",
      "[1598]\ttrain-mlogloss:0.006672\teval-mlogloss:0.097063\n",
      "[1599]\ttrain-mlogloss:0.006666\teval-mlogloss:0.097046\n",
      "[1600]\ttrain-mlogloss:0.00666\teval-mlogloss:0.097039\n",
      "[1601]\ttrain-mlogloss:0.006655\teval-mlogloss:0.097017\n",
      "[1602]\ttrain-mlogloss:0.006649\teval-mlogloss:0.09703\n",
      "[1603]\ttrain-mlogloss:0.006643\teval-mlogloss:0.097019\n",
      "[1604]\ttrain-mlogloss:0.006637\teval-mlogloss:0.097026\n",
      "[1605]\ttrain-mlogloss:0.006632\teval-mlogloss:0.097\n",
      "[1606]\ttrain-mlogloss:0.006625\teval-mlogloss:0.096977\n",
      "[1607]\ttrain-mlogloss:0.006619\teval-mlogloss:0.096988\n",
      "[1608]\ttrain-mlogloss:0.006613\teval-mlogloss:0.096985\n",
      "[1609]\ttrain-mlogloss:0.006607\teval-mlogloss:0.096976\n",
      "[1610]\ttrain-mlogloss:0.006601\teval-mlogloss:0.096979\n",
      "[1611]\ttrain-mlogloss:0.006596\teval-mlogloss:0.096967\n",
      "[1612]\ttrain-mlogloss:0.00659\teval-mlogloss:0.096955\n",
      "[1613]\ttrain-mlogloss:0.006585\teval-mlogloss:0.096957\n",
      "[1614]\ttrain-mlogloss:0.006579\teval-mlogloss:0.096954\n",
      "[1615]\ttrain-mlogloss:0.006573\teval-mlogloss:0.096942\n",
      "[1616]\ttrain-mlogloss:0.006568\teval-mlogloss:0.096946\n",
      "[1617]\ttrain-mlogloss:0.006562\teval-mlogloss:0.096947\n",
      "[1618]\ttrain-mlogloss:0.006556\teval-mlogloss:0.096945\n",
      "[1619]\ttrain-mlogloss:0.006551\teval-mlogloss:0.096938\n",
      "[1620]\ttrain-mlogloss:0.006546\teval-mlogloss:0.096942\n",
      "[1621]\ttrain-mlogloss:0.006541\teval-mlogloss:0.096933\n",
      "[1622]\ttrain-mlogloss:0.006535\teval-mlogloss:0.096919\n",
      "[1623]\ttrain-mlogloss:0.006529\teval-mlogloss:0.096922\n",
      "[1624]\ttrain-mlogloss:0.006524\teval-mlogloss:0.096917\n",
      "[1625]\ttrain-mlogloss:0.006518\teval-mlogloss:0.096905\n",
      "[1626]\ttrain-mlogloss:0.006513\teval-mlogloss:0.096909\n",
      "[1627]\ttrain-mlogloss:0.006508\teval-mlogloss:0.0969\n",
      "[1628]\ttrain-mlogloss:0.006502\teval-mlogloss:0.096893\n",
      "[1629]\ttrain-mlogloss:0.006496\teval-mlogloss:0.096895\n",
      "[1630]\ttrain-mlogloss:0.006491\teval-mlogloss:0.096877\n",
      "[1631]\ttrain-mlogloss:0.006486\teval-mlogloss:0.096873\n",
      "[1632]\ttrain-mlogloss:0.00648\teval-mlogloss:0.096872\n",
      "[1633]\ttrain-mlogloss:0.006474\teval-mlogloss:0.096881\n",
      "[1634]\ttrain-mlogloss:0.00647\teval-mlogloss:0.096867\n",
      "[1635]\ttrain-mlogloss:0.006464\teval-mlogloss:0.096857\n",
      "[1636]\ttrain-mlogloss:0.006458\teval-mlogloss:0.096856\n",
      "[1637]\ttrain-mlogloss:0.006453\teval-mlogloss:0.096842\n",
      "[1638]\ttrain-mlogloss:0.006448\teval-mlogloss:0.096846\n",
      "[1639]\ttrain-mlogloss:0.006442\teval-mlogloss:0.096833\n",
      "[1640]\ttrain-mlogloss:0.006437\teval-mlogloss:0.096828\n",
      "[1641]\ttrain-mlogloss:0.006431\teval-mlogloss:0.096812\n",
      "[1642]\ttrain-mlogloss:0.006426\teval-mlogloss:0.096813\n",
      "[1643]\ttrain-mlogloss:0.006421\teval-mlogloss:0.096816\n",
      "[1644]\ttrain-mlogloss:0.006415\teval-mlogloss:0.096802\n",
      "[1645]\ttrain-mlogloss:0.00641\teval-mlogloss:0.096811\n",
      "[1646]\ttrain-mlogloss:0.006405\teval-mlogloss:0.096815\n",
      "[1647]\ttrain-mlogloss:0.006399\teval-mlogloss:0.096802\n",
      "[1648]\ttrain-mlogloss:0.006394\teval-mlogloss:0.09679\n",
      "[1649]\ttrain-mlogloss:0.006389\teval-mlogloss:0.096792\n",
      "[1650]\ttrain-mlogloss:0.006384\teval-mlogloss:0.096786\n",
      "[1651]\ttrain-mlogloss:0.006379\teval-mlogloss:0.096763\n",
      "[1652]\ttrain-mlogloss:0.006374\teval-mlogloss:0.09676\n",
      "[1653]\ttrain-mlogloss:0.006369\teval-mlogloss:0.096772\n",
      "[1654]\ttrain-mlogloss:0.006364\teval-mlogloss:0.096765\n",
      "[1655]\ttrain-mlogloss:0.006359\teval-mlogloss:0.096774\n",
      "[1656]\ttrain-mlogloss:0.006354\teval-mlogloss:0.096756\n",
      "[1657]\ttrain-mlogloss:0.006349\teval-mlogloss:0.096744\n",
      "[1658]\ttrain-mlogloss:0.006344\teval-mlogloss:0.096743\n",
      "[1659]\ttrain-mlogloss:0.006338\teval-mlogloss:0.096735\n",
      "[1660]\ttrain-mlogloss:0.006333\teval-mlogloss:0.096736\n",
      "[1661]\ttrain-mlogloss:0.006328\teval-mlogloss:0.096721\n",
      "[1662]\ttrain-mlogloss:0.006323\teval-mlogloss:0.096725\n",
      "[1663]\ttrain-mlogloss:0.006318\teval-mlogloss:0.096737\n",
      "[1664]\ttrain-mlogloss:0.006313\teval-mlogloss:0.096724\n",
      "[1665]\ttrain-mlogloss:0.006308\teval-mlogloss:0.096708\n",
      "[1666]\ttrain-mlogloss:0.006303\teval-mlogloss:0.096724\n",
      "[1667]\ttrain-mlogloss:0.006298\teval-mlogloss:0.096707\n",
      "[1668]\ttrain-mlogloss:0.006292\teval-mlogloss:0.096692\n",
      "[1669]\ttrain-mlogloss:0.006287\teval-mlogloss:0.096703\n",
      "[1670]\ttrain-mlogloss:0.006282\teval-mlogloss:0.096703\n",
      "[1671]\ttrain-mlogloss:0.006278\teval-mlogloss:0.096686\n",
      "[1672]\ttrain-mlogloss:0.006272\teval-mlogloss:0.096674\n",
      "[1673]\ttrain-mlogloss:0.006267\teval-mlogloss:0.096671\n",
      "[1674]\ttrain-mlogloss:0.006263\teval-mlogloss:0.096657\n",
      "[1675]\ttrain-mlogloss:0.006258\teval-mlogloss:0.096637\n",
      "[1676]\ttrain-mlogloss:0.006253\teval-mlogloss:0.096639\n",
      "[1677]\ttrain-mlogloss:0.006248\teval-mlogloss:0.09664\n",
      "[1678]\ttrain-mlogloss:0.006243\teval-mlogloss:0.096633\n",
      "[1679]\ttrain-mlogloss:0.006238\teval-mlogloss:0.096644\n",
      "[1680]\ttrain-mlogloss:0.006234\teval-mlogloss:0.096621\n",
      "[1681]\ttrain-mlogloss:0.006229\teval-mlogloss:0.096628\n",
      "[1682]\ttrain-mlogloss:0.006223\teval-mlogloss:0.09662\n",
      "[1683]\ttrain-mlogloss:0.006219\teval-mlogloss:0.096612\n",
      "[1684]\ttrain-mlogloss:0.006213\teval-mlogloss:0.096606\n",
      "[1685]\ttrain-mlogloss:0.006208\teval-mlogloss:0.096605\n",
      "[1686]\ttrain-mlogloss:0.006204\teval-mlogloss:0.096614\n",
      "[1687]\ttrain-mlogloss:0.006199\teval-mlogloss:0.096607\n",
      "[1688]\ttrain-mlogloss:0.006194\teval-mlogloss:0.096607\n",
      "[1689]\ttrain-mlogloss:0.00619\teval-mlogloss:0.096595\n",
      "[1690]\ttrain-mlogloss:0.006185\teval-mlogloss:0.096591\n",
      "[1691]\ttrain-mlogloss:0.00618\teval-mlogloss:0.096575\n",
      "[1692]\ttrain-mlogloss:0.006175\teval-mlogloss:0.096578\n",
      "[1693]\ttrain-mlogloss:0.006171\teval-mlogloss:0.096574\n",
      "[1694]\ttrain-mlogloss:0.006166\teval-mlogloss:0.096568\n",
      "[1695]\ttrain-mlogloss:0.00616\teval-mlogloss:0.096565\n",
      "[1696]\ttrain-mlogloss:0.006156\teval-mlogloss:0.096556\n",
      "[1697]\ttrain-mlogloss:0.006151\teval-mlogloss:0.096547\n",
      "[1698]\ttrain-mlogloss:0.006146\teval-mlogloss:0.096551\n",
      "[1699]\ttrain-mlogloss:0.006142\teval-mlogloss:0.09654\n",
      "[1700]\ttrain-mlogloss:0.006138\teval-mlogloss:0.096545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1701]\ttrain-mlogloss:0.006133\teval-mlogloss:0.096542\n",
      "[1702]\ttrain-mlogloss:0.006128\teval-mlogloss:0.096537\n",
      "[1703]\ttrain-mlogloss:0.006123\teval-mlogloss:0.096528\n",
      "[1704]\ttrain-mlogloss:0.006118\teval-mlogloss:0.096523\n",
      "[1705]\ttrain-mlogloss:0.006114\teval-mlogloss:0.096515\n",
      "[1706]\ttrain-mlogloss:0.006109\teval-mlogloss:0.096511\n",
      "[1707]\ttrain-mlogloss:0.006105\teval-mlogloss:0.09652\n",
      "[1708]\ttrain-mlogloss:0.006101\teval-mlogloss:0.096503\n",
      "[1709]\ttrain-mlogloss:0.006096\teval-mlogloss:0.096511\n",
      "[1710]\ttrain-mlogloss:0.006091\teval-mlogloss:0.096501\n",
      "[1711]\ttrain-mlogloss:0.006086\teval-mlogloss:0.096509\n",
      "[1712]\ttrain-mlogloss:0.006082\teval-mlogloss:0.0965\n",
      "[1713]\ttrain-mlogloss:0.006078\teval-mlogloss:0.096496\n",
      "[1714]\ttrain-mlogloss:0.006073\teval-mlogloss:0.096492\n",
      "[1715]\ttrain-mlogloss:0.006069\teval-mlogloss:0.096499\n",
      "[1716]\ttrain-mlogloss:0.006065\teval-mlogloss:0.096479\n",
      "[1717]\ttrain-mlogloss:0.00606\teval-mlogloss:0.096468\n",
      "[1718]\ttrain-mlogloss:0.006056\teval-mlogloss:0.09648\n",
      "[1719]\ttrain-mlogloss:0.006051\teval-mlogloss:0.096474\n",
      "[1720]\ttrain-mlogloss:0.006047\teval-mlogloss:0.096473\n",
      "[1721]\ttrain-mlogloss:0.006043\teval-mlogloss:0.09647\n",
      "[1722]\ttrain-mlogloss:0.006039\teval-mlogloss:0.096459\n",
      "[1723]\ttrain-mlogloss:0.006034\teval-mlogloss:0.096449\n",
      "[1724]\ttrain-mlogloss:0.006029\teval-mlogloss:0.096453\n",
      "[1725]\ttrain-mlogloss:0.006025\teval-mlogloss:0.096445\n",
      "[1726]\ttrain-mlogloss:0.00602\teval-mlogloss:0.096455\n",
      "[1727]\ttrain-mlogloss:0.006016\teval-mlogloss:0.09645\n",
      "[1728]\ttrain-mlogloss:0.006012\teval-mlogloss:0.096453\n",
      "[1729]\ttrain-mlogloss:0.006008\teval-mlogloss:0.096447\n",
      "[1730]\ttrain-mlogloss:0.006003\teval-mlogloss:0.096438\n",
      "[1731]\ttrain-mlogloss:0.005998\teval-mlogloss:0.096436\n",
      "[1732]\ttrain-mlogloss:0.005994\teval-mlogloss:0.096418\n",
      "[1733]\ttrain-mlogloss:0.00599\teval-mlogloss:0.096418\n",
      "[1734]\ttrain-mlogloss:0.005986\teval-mlogloss:0.096423\n",
      "[1735]\ttrain-mlogloss:0.005981\teval-mlogloss:0.096428\n",
      "[1736]\ttrain-mlogloss:0.005977\teval-mlogloss:0.096423\n",
      "[1737]\ttrain-mlogloss:0.005973\teval-mlogloss:0.096421\n",
      "[1738]\ttrain-mlogloss:0.005968\teval-mlogloss:0.09642\n",
      "[1739]\ttrain-mlogloss:0.005964\teval-mlogloss:0.096403\n",
      "[1740]\ttrain-mlogloss:0.00596\teval-mlogloss:0.096411\n",
      "[1741]\ttrain-mlogloss:0.005956\teval-mlogloss:0.096408\n",
      "[1742]\ttrain-mlogloss:0.005951\teval-mlogloss:0.096415\n",
      "[1743]\ttrain-mlogloss:0.005947\teval-mlogloss:0.096405\n",
      "[1744]\ttrain-mlogloss:0.005943\teval-mlogloss:0.096418\n",
      "[1745]\ttrain-mlogloss:0.005939\teval-mlogloss:0.096402\n",
      "[1746]\ttrain-mlogloss:0.005934\teval-mlogloss:0.096404\n",
      "[1747]\ttrain-mlogloss:0.005929\teval-mlogloss:0.09639\n",
      "[1748]\ttrain-mlogloss:0.005925\teval-mlogloss:0.096393\n",
      "[1749]\ttrain-mlogloss:0.005921\teval-mlogloss:0.096395\n",
      "[1750]\ttrain-mlogloss:0.005917\teval-mlogloss:0.096384\n",
      "[1751]\ttrain-mlogloss:0.005913\teval-mlogloss:0.096376\n",
      "[1752]\ttrain-mlogloss:0.005909\teval-mlogloss:0.096369\n",
      "[1753]\ttrain-mlogloss:0.005905\teval-mlogloss:0.096371\n",
      "[1754]\ttrain-mlogloss:0.0059\teval-mlogloss:0.09636\n",
      "[1755]\ttrain-mlogloss:0.005896\teval-mlogloss:0.096366\n",
      "[1756]\ttrain-mlogloss:0.005892\teval-mlogloss:0.096355\n",
      "[1757]\ttrain-mlogloss:0.005887\teval-mlogloss:0.096354\n",
      "[1758]\ttrain-mlogloss:0.005883\teval-mlogloss:0.096352\n",
      "[1759]\ttrain-mlogloss:0.005879\teval-mlogloss:0.096342\n",
      "[1760]\ttrain-mlogloss:0.005875\teval-mlogloss:0.09635\n",
      "[1761]\ttrain-mlogloss:0.005871\teval-mlogloss:0.096327\n",
      "[1762]\ttrain-mlogloss:0.005867\teval-mlogloss:0.096309\n",
      "[1763]\ttrain-mlogloss:0.005862\teval-mlogloss:0.096312\n",
      "[1764]\ttrain-mlogloss:0.005859\teval-mlogloss:0.096304\n",
      "[1765]\ttrain-mlogloss:0.005854\teval-mlogloss:0.0963\n",
      "[1766]\ttrain-mlogloss:0.00585\teval-mlogloss:0.096305\n",
      "[1767]\ttrain-mlogloss:0.005846\teval-mlogloss:0.096298\n",
      "[1768]\ttrain-mlogloss:0.005842\teval-mlogloss:0.096301\n",
      "[1769]\ttrain-mlogloss:0.005838\teval-mlogloss:0.096305\n",
      "[1770]\ttrain-mlogloss:0.005834\teval-mlogloss:0.096293\n",
      "[1771]\ttrain-mlogloss:0.00583\teval-mlogloss:0.096288\n",
      "[1772]\ttrain-mlogloss:0.005826\teval-mlogloss:0.096286\n",
      "[1773]\ttrain-mlogloss:0.005822\teval-mlogloss:0.096278\n",
      "[1774]\ttrain-mlogloss:0.005818\teval-mlogloss:0.096274\n",
      "[1775]\ttrain-mlogloss:0.005814\teval-mlogloss:0.096269\n",
      "[1776]\ttrain-mlogloss:0.005809\teval-mlogloss:0.09627\n",
      "[1777]\ttrain-mlogloss:0.005805\teval-mlogloss:0.096278\n",
      "[1778]\ttrain-mlogloss:0.005801\teval-mlogloss:0.096265\n",
      "[1779]\ttrain-mlogloss:0.005797\teval-mlogloss:0.096265\n",
      "[1780]\ttrain-mlogloss:0.005793\teval-mlogloss:0.096261\n",
      "[1781]\ttrain-mlogloss:0.005789\teval-mlogloss:0.096253\n",
      "[1782]\ttrain-mlogloss:0.005785\teval-mlogloss:0.096247\n",
      "[1783]\ttrain-mlogloss:0.005782\teval-mlogloss:0.096246\n",
      "[1784]\ttrain-mlogloss:0.005777\teval-mlogloss:0.096232\n",
      "[1785]\ttrain-mlogloss:0.005773\teval-mlogloss:0.09623\n",
      "[1786]\ttrain-mlogloss:0.005769\teval-mlogloss:0.09622\n",
      "[1787]\ttrain-mlogloss:0.005765\teval-mlogloss:0.09621\n",
      "[1788]\ttrain-mlogloss:0.005761\teval-mlogloss:0.096223\n",
      "[1789]\ttrain-mlogloss:0.005757\teval-mlogloss:0.096209\n",
      "[1790]\ttrain-mlogloss:0.005753\teval-mlogloss:0.096207\n",
      "[1791]\ttrain-mlogloss:0.005749\teval-mlogloss:0.096212\n",
      "[1792]\ttrain-mlogloss:0.005746\teval-mlogloss:0.096193\n",
      "[1793]\ttrain-mlogloss:0.005741\teval-mlogloss:0.096184\n",
      "[1794]\ttrain-mlogloss:0.005737\teval-mlogloss:0.096176\n",
      "[1795]\ttrain-mlogloss:0.005733\teval-mlogloss:0.096175\n",
      "[1796]\ttrain-mlogloss:0.00573\teval-mlogloss:0.096181\n",
      "[1797]\ttrain-mlogloss:0.005726\teval-mlogloss:0.096167\n",
      "[1798]\ttrain-mlogloss:0.005722\teval-mlogloss:0.096167\n",
      "[1799]\ttrain-mlogloss:0.005718\teval-mlogloss:0.096172\n",
      "[1800]\ttrain-mlogloss:0.005715\teval-mlogloss:0.096163\n",
      "[1801]\ttrain-mlogloss:0.00571\teval-mlogloss:0.096153\n",
      "[1802]\ttrain-mlogloss:0.005706\teval-mlogloss:0.096139\n",
      "[1803]\ttrain-mlogloss:0.005703\teval-mlogloss:0.096146\n",
      "[1804]\ttrain-mlogloss:0.005699\teval-mlogloss:0.096129\n",
      "[1805]\ttrain-mlogloss:0.005695\teval-mlogloss:0.096121\n",
      "[1806]\ttrain-mlogloss:0.005691\teval-mlogloss:0.096119\n",
      "[1807]\ttrain-mlogloss:0.005687\teval-mlogloss:0.096111\n",
      "[1808]\ttrain-mlogloss:0.005683\teval-mlogloss:0.096101\n",
      "[1809]\ttrain-mlogloss:0.005679\teval-mlogloss:0.096106\n",
      "[1810]\ttrain-mlogloss:0.005676\teval-mlogloss:0.096108\n",
      "[1811]\ttrain-mlogloss:0.005672\teval-mlogloss:0.096105\n",
      "[1812]\ttrain-mlogloss:0.005668\teval-mlogloss:0.096088\n",
      "[1813]\ttrain-mlogloss:0.005664\teval-mlogloss:0.096088\n",
      "[1814]\ttrain-mlogloss:0.00566\teval-mlogloss:0.096087\n",
      "[1815]\ttrain-mlogloss:0.005657\teval-mlogloss:0.096081\n",
      "[1816]\ttrain-mlogloss:0.005653\teval-mlogloss:0.096069\n",
      "[1817]\ttrain-mlogloss:0.005649\teval-mlogloss:0.096072\n",
      "[1818]\ttrain-mlogloss:0.005645\teval-mlogloss:0.096074\n",
      "[1819]\ttrain-mlogloss:0.005642\teval-mlogloss:0.096062\n",
      "[1820]\ttrain-mlogloss:0.005638\teval-mlogloss:0.096057\n",
      "[1821]\ttrain-mlogloss:0.005635\teval-mlogloss:0.096056\n",
      "[1822]\ttrain-mlogloss:0.005631\teval-mlogloss:0.096055\n",
      "[1823]\ttrain-mlogloss:0.005627\teval-mlogloss:0.096052\n",
      "[1824]\ttrain-mlogloss:0.005623\teval-mlogloss:0.096042\n",
      "[1825]\ttrain-mlogloss:0.00562\teval-mlogloss:0.096034\n",
      "[1826]\ttrain-mlogloss:0.005616\teval-mlogloss:0.096033\n",
      "[1827]\ttrain-mlogloss:0.005613\teval-mlogloss:0.096028\n",
      "[1828]\ttrain-mlogloss:0.005609\teval-mlogloss:0.096018\n",
      "[1829]\ttrain-mlogloss:0.005605\teval-mlogloss:0.096022\n",
      "[1830]\ttrain-mlogloss:0.005602\teval-mlogloss:0.096017\n",
      "[1831]\ttrain-mlogloss:0.005598\teval-mlogloss:0.096021\n",
      "[1832]\ttrain-mlogloss:0.005594\teval-mlogloss:0.095999\n",
      "[1833]\ttrain-mlogloss:0.005591\teval-mlogloss:0.096003\n",
      "[1834]\ttrain-mlogloss:0.005587\teval-mlogloss:0.095999\n",
      "[1835]\ttrain-mlogloss:0.005584\teval-mlogloss:0.095993\n",
      "[1836]\ttrain-mlogloss:0.00558\teval-mlogloss:0.095995\n",
      "[1837]\ttrain-mlogloss:0.005576\teval-mlogloss:0.095989\n",
      "[1838]\ttrain-mlogloss:0.005573\teval-mlogloss:0.095985\n",
      "[1839]\ttrain-mlogloss:0.005569\teval-mlogloss:0.095978\n",
      "[1840]\ttrain-mlogloss:0.005566\teval-mlogloss:0.095972\n",
      "[1841]\ttrain-mlogloss:0.005561\teval-mlogloss:0.095961\n",
      "[1842]\ttrain-mlogloss:0.005558\teval-mlogloss:0.095969\n",
      "[1843]\ttrain-mlogloss:0.005554\teval-mlogloss:0.095958\n",
      "[1844]\ttrain-mlogloss:0.005551\teval-mlogloss:0.09595\n",
      "[1845]\ttrain-mlogloss:0.005547\teval-mlogloss:0.095946\n",
      "[1846]\ttrain-mlogloss:0.005543\teval-mlogloss:0.095946\n",
      "[1847]\ttrain-mlogloss:0.00554\teval-mlogloss:0.095953\n",
      "[1848]\ttrain-mlogloss:0.005536\teval-mlogloss:0.095951\n",
      "[1849]\ttrain-mlogloss:0.005532\teval-mlogloss:0.095952\n",
      "[1850]\ttrain-mlogloss:0.005529\teval-mlogloss:0.095953\n",
      "[1851]\ttrain-mlogloss:0.005525\teval-mlogloss:0.095944\n",
      "[1852]\ttrain-mlogloss:0.005522\teval-mlogloss:0.095937\n",
      "[1853]\ttrain-mlogloss:0.005518\teval-mlogloss:0.09593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1854]\ttrain-mlogloss:0.005515\teval-mlogloss:0.09593\n",
      "[1855]\ttrain-mlogloss:0.005511\teval-mlogloss:0.095931\n",
      "[1856]\ttrain-mlogloss:0.005508\teval-mlogloss:0.095934\n",
      "[1857]\ttrain-mlogloss:0.005504\teval-mlogloss:0.095926\n",
      "[1858]\ttrain-mlogloss:0.005501\teval-mlogloss:0.095928\n",
      "[1859]\ttrain-mlogloss:0.005498\teval-mlogloss:0.095925\n",
      "[1860]\ttrain-mlogloss:0.005494\teval-mlogloss:0.095924\n",
      "[1861]\ttrain-mlogloss:0.005491\teval-mlogloss:0.095911\n",
      "[1862]\ttrain-mlogloss:0.005487\teval-mlogloss:0.095913\n",
      "[1863]\ttrain-mlogloss:0.005484\teval-mlogloss:0.095909\n",
      "[1864]\ttrain-mlogloss:0.005481\teval-mlogloss:0.095908\n",
      "[1865]\ttrain-mlogloss:0.005477\teval-mlogloss:0.095911\n",
      "[1866]\ttrain-mlogloss:0.005474\teval-mlogloss:0.095914\n",
      "[1867]\ttrain-mlogloss:0.00547\teval-mlogloss:0.095915\n",
      "[1868]\ttrain-mlogloss:0.005466\teval-mlogloss:0.095906\n",
      "[1869]\ttrain-mlogloss:0.005463\teval-mlogloss:0.095905\n",
      "[1870]\ttrain-mlogloss:0.00546\teval-mlogloss:0.095911\n",
      "[1871]\ttrain-mlogloss:0.005456\teval-mlogloss:0.095904\n",
      "[1872]\ttrain-mlogloss:0.005453\teval-mlogloss:0.095899\n",
      "[1873]\ttrain-mlogloss:0.00545\teval-mlogloss:0.095892\n",
      "[1874]\ttrain-mlogloss:0.005447\teval-mlogloss:0.0959\n",
      "[1875]\ttrain-mlogloss:0.005443\teval-mlogloss:0.095893\n",
      "[1876]\ttrain-mlogloss:0.00544\teval-mlogloss:0.09589\n",
      "[1877]\ttrain-mlogloss:0.005437\teval-mlogloss:0.095893\n",
      "[1878]\ttrain-mlogloss:0.005434\teval-mlogloss:0.095895\n",
      "[1879]\ttrain-mlogloss:0.00543\teval-mlogloss:0.095876\n",
      "[1880]\ttrain-mlogloss:0.005426\teval-mlogloss:0.095875\n",
      "[1881]\ttrain-mlogloss:0.005423\teval-mlogloss:0.09587\n",
      "[1882]\ttrain-mlogloss:0.00542\teval-mlogloss:0.095876\n",
      "[1883]\ttrain-mlogloss:0.005417\teval-mlogloss:0.09587\n",
      "[1884]\ttrain-mlogloss:0.005413\teval-mlogloss:0.095874\n",
      "[1885]\ttrain-mlogloss:0.00541\teval-mlogloss:0.095872\n",
      "[1886]\ttrain-mlogloss:0.005407\teval-mlogloss:0.095873\n",
      "[1887]\ttrain-mlogloss:0.005403\teval-mlogloss:0.095858\n",
      "[1888]\ttrain-mlogloss:0.0054\teval-mlogloss:0.095859\n",
      "[1889]\ttrain-mlogloss:0.005397\teval-mlogloss:0.095863\n",
      "[1890]\ttrain-mlogloss:0.005394\teval-mlogloss:0.095864\n",
      "[1891]\ttrain-mlogloss:0.005391\teval-mlogloss:0.095861\n",
      "[1892]\ttrain-mlogloss:0.005388\teval-mlogloss:0.095858\n",
      "[1893]\ttrain-mlogloss:0.005384\teval-mlogloss:0.095851\n",
      "[1894]\ttrain-mlogloss:0.005381\teval-mlogloss:0.095842\n",
      "[1895]\ttrain-mlogloss:0.005377\teval-mlogloss:0.095851\n",
      "[1896]\ttrain-mlogloss:0.005374\teval-mlogloss:0.09584\n",
      "[1897]\ttrain-mlogloss:0.005371\teval-mlogloss:0.095843\n",
      "[1898]\ttrain-mlogloss:0.005368\teval-mlogloss:0.095837\n",
      "[1899]\ttrain-mlogloss:0.005364\teval-mlogloss:0.09583\n",
      "[1900]\ttrain-mlogloss:0.005361\teval-mlogloss:0.095835\n",
      "[1901]\ttrain-mlogloss:0.005358\teval-mlogloss:0.095836\n",
      "[1902]\ttrain-mlogloss:0.005355\teval-mlogloss:0.095832\n",
      "[1903]\ttrain-mlogloss:0.005352\teval-mlogloss:0.095834\n",
      "[1904]\ttrain-mlogloss:0.005349\teval-mlogloss:0.09583\n",
      "[1905]\ttrain-mlogloss:0.005345\teval-mlogloss:0.095833\n",
      "[1906]\ttrain-mlogloss:0.005342\teval-mlogloss:0.095831\n",
      "[1907]\ttrain-mlogloss:0.005339\teval-mlogloss:0.095833\n",
      "[1908]\ttrain-mlogloss:0.005335\teval-mlogloss:0.095827\n",
      "[1909]\ttrain-mlogloss:0.005332\teval-mlogloss:0.095824\n",
      "[1910]\ttrain-mlogloss:0.005329\teval-mlogloss:0.095824\n",
      "[1911]\ttrain-mlogloss:0.005326\teval-mlogloss:0.09582\n",
      "[1912]\ttrain-mlogloss:0.005322\teval-mlogloss:0.095826\n",
      "[1913]\ttrain-mlogloss:0.005319\teval-mlogloss:0.095822\n",
      "[1914]\ttrain-mlogloss:0.005316\teval-mlogloss:0.095815\n",
      "[1915]\ttrain-mlogloss:0.005313\teval-mlogloss:0.095816\n",
      "[1916]\ttrain-mlogloss:0.00531\teval-mlogloss:0.095818\n",
      "[1917]\ttrain-mlogloss:0.005306\teval-mlogloss:0.095822\n",
      "[1918]\ttrain-mlogloss:0.005303\teval-mlogloss:0.09582\n",
      "[1919]\ttrain-mlogloss:0.0053\teval-mlogloss:0.095807\n",
      "[1920]\ttrain-mlogloss:0.005297\teval-mlogloss:0.095813\n",
      "[1921]\ttrain-mlogloss:0.005294\teval-mlogloss:0.095808\n",
      "[1922]\ttrain-mlogloss:0.005291\teval-mlogloss:0.095807\n",
      "[1923]\ttrain-mlogloss:0.005287\teval-mlogloss:0.095809\n",
      "[1924]\ttrain-mlogloss:0.005284\teval-mlogloss:0.095807\n",
      "[1925]\ttrain-mlogloss:0.005281\teval-mlogloss:0.095803\n",
      "[1926]\ttrain-mlogloss:0.005278\teval-mlogloss:0.095802\n",
      "[1927]\ttrain-mlogloss:0.005274\teval-mlogloss:0.095801\n",
      "[1928]\ttrain-mlogloss:0.005272\teval-mlogloss:0.095807\n",
      "[1929]\ttrain-mlogloss:0.005269\teval-mlogloss:0.095802\n",
      "[1930]\ttrain-mlogloss:0.005266\teval-mlogloss:0.095798\n",
      "[1931]\ttrain-mlogloss:0.005262\teval-mlogloss:0.095791\n",
      "[1932]\ttrain-mlogloss:0.005259\teval-mlogloss:0.095784\n",
      "[1933]\ttrain-mlogloss:0.005256\teval-mlogloss:0.095779\n",
      "[1934]\ttrain-mlogloss:0.005253\teval-mlogloss:0.095783\n",
      "[1935]\ttrain-mlogloss:0.005249\teval-mlogloss:0.095781\n",
      "[1936]\ttrain-mlogloss:0.005246\teval-mlogloss:0.095775\n",
      "[1937]\ttrain-mlogloss:0.005243\teval-mlogloss:0.095776\n",
      "[1938]\ttrain-mlogloss:0.00524\teval-mlogloss:0.095774\n",
      "[1939]\ttrain-mlogloss:0.005237\teval-mlogloss:0.095782\n",
      "[1940]\ttrain-mlogloss:0.005234\teval-mlogloss:0.095782\n",
      "[1941]\ttrain-mlogloss:0.005231\teval-mlogloss:0.095778\n",
      "[1942]\ttrain-mlogloss:0.005228\teval-mlogloss:0.095788\n",
      "[1943]\ttrain-mlogloss:0.005225\teval-mlogloss:0.095791\n",
      "[1944]\ttrain-mlogloss:0.005221\teval-mlogloss:0.095785\n",
      "[1945]\ttrain-mlogloss:0.005218\teval-mlogloss:0.095784\n",
      "[1946]\ttrain-mlogloss:0.005215\teval-mlogloss:0.095777\n",
      "[1947]\ttrain-mlogloss:0.005212\teval-mlogloss:0.095775\n",
      "[1948]\ttrain-mlogloss:0.005209\teval-mlogloss:0.095775\n",
      "Stopping. Best iteration:\n",
      "[1938]\ttrain-mlogloss:0.00524\teval-mlogloss:0.095774\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### XGBoost classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         EI       0.97      0.96      0.97       320\n",
      "         IE       0.95      0.97      0.96       327\n",
      "          N       0.99      0.98      0.98       629\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### XGBoost <span style='color: red'>randomly selected 1000 data</span> classification report : "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataPreprocess' object has no attribute 'vect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-80eb307c562d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-aa3a68bbdd5d>\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(self, num_boost_round, lr, max_delta_step)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint_classification_report_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mdisplay_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"##### XGBoost <span style='color: red'>randomly selected 1000 data</span> classification report : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mprint_classification_report_xgb_1000\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-aa3a68bbdd5d>\u001b[0m in \u001b[0;36mprint_classification_report_xgb_1000\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectorized_data_to_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tfidf_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mprint_classification_report_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-10c78b7a824e>\u001b[0m in \u001b[0;36mget_vectorized_data_to_eval\u001b[0;34m(self, x, is_tfidf_vect)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtmp_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_arr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtmp_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_inversed_target_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataPreprocess' object has no attribute 'vect'"
     ]
    }
   ],
   "source": [
    "new_model_train.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
